{"skillName": "DSRMP06", "skillText": "Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread. Everett Rogers, a professor of communication studies, popularized the theory in his book Diffusion of Innovations; the book was first published in 1962, and is now in its fifth edition (2003).[1] Rogers argues that diffusion is the process by which an innovation is communicated over time among the participants in a social system. The origins of the diffusion of innovations theory are varied and span multiple disciplines.  Rogers proposes that four main elements influence the spread of a new idea: the innovation itself, communication channels, time, and a social system. This process relies heavily on human capital. The innovation must be widely adopted in order to self-sustain. Within the rate of adoption, there is a point at which an innovation reaches critical mass.  The categories of adopters are innovators, early adopters, early majority, late majority, and laggards.[2] Diffusion manifests itself in different ways and is highly subject to the type of adopters and innovation-decision process. The criterion for the adopter categorization is innovativeness, defined as the degree to which an individual adopts a new idea.  Contents      1 History     2 Elements         2.1 Characteristics of innovations         2.2 Characteristics of individual adopters         2.3 Characteristics of organizations     3 Process     4 Decisions     5 Rate of adoption         5.1 Adoption strategies         5.2 Diffusion vs adoption     6 Adopter categories     7 Failed diffusion     8 Heterophily and communication channels     9 The role of social systems         9.1 Opinion leaders         9.2 Electronic communication social networks         9.3 Organizations     10 Extensions of the theory         10.1 Policy         10.2 Technology     11 Consequences of adoption         11.1 Public versus private         11.2 Benefits versus costs     12 Mathematical treatment         12.1 Complex Systems models     13 Criticism     14 See also     15 References         15.1 Notes     16 External links  History  The concept of diffusion was first studied by the French sociologist Gabriel Tarde in late 19th century[3] and by German and Austrian anthropologists and geographers such as Friedrich Ratzel and Leo Frobenius.[4] The study of diffusion of innovations took off in the subfield of rural sociology in the midwestern United States in the 1920s and 1930s. Agriculture technology was advancing rapidly, and researchers started to examine how independent farmers were adopting hybrid seeds, equipment, and techniques.[5] A study of the adoption of hybrid corn seed in Iowa by Ryan and Gross (1943) solidified the prior work on diffusion into a distinct paradigm that would be cited consistently in the future.[5][6] Since its start in rural sociology, Diffusion of Innovations has been applied to numerous contexts, including medical sociology, communications, marketing, development studies, health promotion, organizational studies, knowledge management, and complexity studies,[7] with a particularly large impact on the use of medicines, medical techniques, and health communications.[8] In organizational studies, its basic epidemiological or internal-influence form was formulated by H. Earl Pemberton,[9] who provided examples of institutional diffusion[10] such as postage stamps and standardized school ethics codes.  In 1962, Everett Rogers, a professor of rural sociology, published his seminal work: Diffusion of Innovations. Rogers synthesized research from over 508 diffusion studies across the fields that initially influenced the theory: anthropology, early sociology, rural sociology, education, industrial sociology and medical sociology. Using his synthesis, Rogers produced a theory of the adoption of innovations among individuals and organizations.[11] Diffusion of Innovations and Rogers' later books are among the most often cited in diffusion research. His methodologies are closely followed in recent diffusion research, even as the field has expanded into, and been influenced by, other methodological disciplines such as social network analysis and communication.[12][13] Elements  The key elements in diffusion research are: Element \tDefinition Innovation \tInnovations are a broad category, relative to the current knowledge of the analyzed unit. Any idea, practice, or object that is perceived as new by an individual or other unit of adoption could be considered an innovation available for study.[14] Adopters \tAdopters are the minimal unit of analysis. In most studies, adopters are individuals, but can also be organizations (businesses, schools, hospitals, etc.), clusters within social networks, or countries.[15] Communication channels \tDiffusion, by definition, takes place among people or organizations. Communication channels allow the transfer of information from one unit to the other.[16] Communication patterns or capabilities must be established between parties as a minimum for diffusion to occur.[17] Time \tThe passage of time is necessary for innovations to be adopted; they are rarely adopted instantaneously. In fact, in the Ryan and Gross (1943) study on hybrid corn adoption, adoption occurred over more than ten years, and most farmers only dedicated a fraction on their fields to the new corn in the first years after adoption.[6][18] Social system \tThe social system is the combination of external influences (mass media, organizational or governmental mandates) and internal influences (strong and weak social relationships, distance from opinion leaders).[19] There are many roles in a social system, and their combination represents the total influences on a potential adopter.[20] Characteristics of innovations  Studies have explored many characteristics of innovations. Meta-reviews have identified several characteristics that are common among most studies.[21] These are in line with the characteristics that Rogers initially cited in his reviews.[22]  Potential adopters evaluate an innovation on its relative advantage (the perceived efficiencies gained by the innovation relative to current tools or procedures), its compatibility with the pre-existing system, its complexity or difficulty to learn, its trialability or testability, its potential for reinvention (using the tool for initially unintended purposes), and its observed effects. These qualities interact and are judged as a whole. For example, an innovation might be extremely complex, reducing its likelihood to be adopted and diffused, but it might be very compatible with a large advantage relative to current tools. Even with this high learning curve, potential adopters might adopt the innovation anyway.[22]  Studies also identify other characteristics of innovations, but these are not as common as the ones that Rogers lists above.[23] The fuzziness of the boundaries of the innovation can impact its adoption. Specifically, innovations with a small core and large periphery are easier to adopt.[24] Innovations that are less risky are easier to adopt as the potential loss from failed integration is lower.[25] Innovations that are disruptive to routine tasks, even when they bring a large relative advantage, might not be adopted because of added instability. Likewise, innovations that make tasks easier are likely to be adopted.[26] Closely related to relative complexity, knowledge requirements are the ability barrier to use presented by the difficulty to use the innovation. Even when there are high knowledge requirements, support from prior adopters or other sources can increase the chances for adoption.[27] Characteristics of individual adopters  Like innovations, adopters have been determined to have traits that affect their likelihood to adopt an innovation. A bevy of individual personality traits have been explored for their impacts on adoption, but with little agreement.[28] Ability and motivation, which vary on situation unlike personality traits, have a large impact on a potential adopter's likelihood to adopt an innovation. Unsurprisingly, potential adopters who are motivated to adopt an innovation are likely to make the adjustments needed to adopt it.[29] Motivation can be impacted by the meaning that an innovation holds; innovations can have symbolic value that encourage (or discourage) adoption.[30] First proposed by Ryan and Gross (1943), the overall connectedness of a potential adopter to the broad community represented by a city.[6] Potential adopters who frequent metropolitan areas are more likely to adopt an innovation. Finally, potential adopters who have the power or agency to create change, particularly in organizations, are more likely to adopt an innovation than someone with less power over his choices.[31] Characteristics of organizations  Organizations face more complex adoption possibilities because organizations are both the aggregate of its individuals and its own system with a set of procedures and norms.[32] Three organizational characteristics match well with the individual characteristics above: tension for change (motivation and ability), innovation-system fit (compatibility), and assessment of implications (observability). Organizations can feel pressured by a tension for change. If the organization's situation is untenable, it will be motivated to adopt an innovation to change its fortunes. This tension often plays out among its individual members. Innovations that match the organization's pre-existing system require fewer coincidental changes and are easy to assess are more likely to be adopted.[33] The wider environment of the organization, often an industry, community, or economy, exerts pressures on the organization, too. Where an innovation is diffusing through the organization's environment for any reason, the organization is more likely to adopt it.[25] Innovations that are intentionally spread, including by political mandate or directive, are also likely to diffuse quickly.[34][35] Process  Diffusion occurs through a five–step decision-making process. It occurs through a series of communication channels over a period of time among the members of a similar social system. Ryan and Gross first identified adoption as a process in 1943.[36] Rogers' five stages (steps): awareness, interest, evaluation, trial, and adoption are integral to this theory. An individual might reject an innovation at any time during or after the adoption process. Abrahamson examined this process critically by posing questions such as: How do technically inefficient innovations diffuse and what impedes technically efficient innovations from catching on? Abrahamson makes suggestions for how organizational scientists can more comprehensively evaluate the spread of innovations.[37] In later editions of Diffusion of Innovation, Rogers changes his terminology of the five stages to: knowledge, persuasion, decision, implementation, and confirmation. However, the descriptions of the categories have remained similar throughout the editions. DoI Stages.jpg Five stages of the adoption process Stage \tDefinition Knowledge \tThe individual is first exposed to an innovation, but lacks information about the innovation. During this stage the individual has not yet been inspired to find out more information about the innovation. Persuasion \tThe individual is interested in the innovation and actively seeks related information/details. Decision \tThe individual takes the concept of the change and weighs the advantages/disadvantages of using the innovation and decides whether to adopt or reject the innovation. Due to the individualistic nature of this stage, Rogers notes that it is the most difficult stage on which to acquire empirical evidence.[11] Implementation \tThe individual employs the innovation to a varying degree depending on the situation. During this stage the individual also determines the usefulness of the innovation and may search for further information about it. Confirmation \tThe individual finalizes his/her decision to continue using the innovation. This stage is both intrapersonal (may cause cognitive dissonance) and interpersonal, confirmation the group has made the right decision. Decisions  Two factors determine what type a particular decision is:      Whether the decision is made freely and implemented voluntarily     Who makes the decision.  Based on these considerations, three types of innovation-decisions have been identified.[citation needed] Type \tDefinition Optional Innovation-Decision \tmade by an individual who is in some way distinguished from others. Collective Innovation-Decision \tmade collectively by all participants. Authority Innovation-Decision \tmade for the entire social system by individuals in positions of influence or power. Rate of adoption  The rate of adoption is defined as the relative speed at which participants adopt an innovation. Rate is usually measured by the length of time required for a certain percentage of the members of a social system to adopt an innovation.[38] The rates of adoption for innovations are determined by an individual’s adopter category. In general, individuals who first adopt an innovation require a shorter adoption period (adoption process) when compared to late adopters.  Within the adoption curve at some point the innovation reaches critical mass. This is when the number of individual adopters ensures that the innovation is self-sustaining. Adoption strategies  Rogers outlines several strategies in order to help an innovation reach this stage, including when an innovation adopted by a highly respected individual within a social network and creating an instinctive desire for a specific innovation. Another strategy includes injecting an innovation into a group of individuals who would readily use said technology, as well as providing positive reactions and benefits for early adopters. Diffusion vs adoption  Adoption is an individual process detailing the series of stages one undergoes from first hearing about a product to finally adopting it. Diffusion signifies a group phenomenon, which suggests how an innovation spreads. Adopter categories  Rogers defines an adopter category as a classification of individuals within a social system on the basis of innovativeness. In the book Diffusion of Innovations, Rogers suggests a total of five categories of adopters in order to standardize the usage of adopter categories in diffusion research. The adoption of an innovation follows an S curve when plotted over a length of time.[39] The categories of adopters are: innovators, early adopters, early majority, late majority and laggards[2] In addition to the gatekeepers and opinion leaders who exist within a given community, change agents may come from outside the community. Change agents bring innovations to new communities– ﬁrst through the gatekeepers, then through the opinion leaders, and so on through the community. Adopter category \tDefinition Innovators \tInnovators are willing to take risks, have the highest social status, have financial liquidity, are social and have closest contact to scientific sources and interaction with other innovators. Their risk tolerance allows them to adopt technologies that may ultimately fail. Financial resources help absorb these failures.[40] Early adopters \tThese individuals have the highest degree of opinion leadership among the adopter categories. Early adopters have a higher social status, financial liquidity, advanced education and are more socially forward than late adopters. They are more discreet in adoption choices than innovators. They use judicious choice of adoption to help them maintain a central communication position.[41] Early Majority \tThey adopt an innovation after a varying degree of time that is significantly longer than the innovators and early adopters. Early Majority have above average social status, contact with early adopters and seldom hold positions of opinion leadership in a system (Rogers 1962, p. 283) Late Majority \tThey adopt an innovation after the average participant. These individuals approach an innovation with a high degree of skepticism and after the majority of society has adopted the innovation. Late Majority are typically skeptical about an innovation, have below average social status, little financial liquidity, in contact with others in late majority and early majority and little opinion leadership. Laggards \tThey are the last to adopt an innovation. Unlike some of the previous categories, individuals in this category show little to no opinion leadership. These individuals typically have an aversion to change-agents. Laggards typically tend to be focused on \"traditions\", lowest social status, lowest financial liquidity, oldest among adopters, and in contact with only family and close friends. Failed diffusion  Failed diffusion does not mean that the technology was adopted by no one. Rather, failed diffusion often refers to diffusion that does not reach or approach 100% adoption due to its own weaknesses, competition from other innovations, or simply a lack of awareness. From a social networks perspective, a failed diffusion might be widely adopted within certain clusters but fail to make an impact on more distantly related people. Networks that are over-connected might suffer from a rigidity that prevents the changes an innovation might bring, as well.[42][43] Sometimes, some innovations also fail as a result of lack of local involvement and community participation.  For example, Rogers discussed a situation in Peru involving the implementation of boiling drinking water to improve health and wellness levels in the village of Los Molinas. The residents had no knowledge of the link between sanitation and illness. The campaign worked with the villagers to try to teach them to boil water, burn their garbage, install latrines and report cases of illness to local health agencies. In Los Molinas, a stigma was linked to boiled water as something that only the \"unwell\" consumed, and thus, the idea of healthy residents boiling water prior to consumption was frowned upon. The two-year educational campaign was considered to be largely unsuccessful. This failure exemplified the importance of the roles of the communication channels that are involved in such a campaign for social change. An examination of diffusion in El Salvador determined that there can be more than one social network at play as innovations are communicated. One network carries information and the other carries influence. While people might hear of an innovation's uses, in Rogers' Los Molinas sanitation case, a network of influence and status prevented adoption.[44][45] Heterophily and communication channels  Lazarsfeld and Merton first called attention to the principles of homophily and its opposite, heterophily. Using their definition, Rogers defines homophily as \"the degree to which pairs of individuals who interact are similar in certain attributes, such as beliefs, education, social status, and the like\".[46] When given the choice, individuals usually choose to interact with someone similar to themselves. Homophilous individuals engage in more effective communication because their similarities lead to greater knowledge gain as well as attitude or behavior change. As a result, homophilous people tend to promote diffusion among each other.[47] However, diffusion requires a certain degree of heterophily to introduce new ideas into a relationship; if two individuals are identical, no diffusion occurs because there is no new information to exchange. Therefore, an ideal situation would involve potential adopters who are homophilous in every way, except in knowledge of the innovation.[48]  Promotion of healthy behavior provides an example of the balance required of homophily and heterophily. People tend to be close to others of similar health status.[49] As a result, people with unhealthy behaviors like smoking and obesity are less likely to encounter information and behaviors that encourage good health. This presents a critical challenge for health communications, as ties between heterophilous people are relatively weaker, harder to create, and harder to maintain.[50] Developing heterophilous ties to unhealthy communities can increase the effectiveness of the diffusion of good health behaviors. Once one previously homophilous tie adopts the behavior or innovation, the other members of that group are more likely to adopt it, too.[51] The role of social systems Opinion leaders  Not all individuals exert an equal amount of influence over others. In this sense opinion leaders are influential in spreading either positive or negative information about an innovation. Rogers relies on the ideas of Katz & Lazarsfeld and the two-step flow theory in developing his ideas on the influence of opinion leaders.[52]  Opinion leaders have the most influence during the evaluation stage of the innovation-decision process and on late adopters.[53] In addition opinion leaders typically have greater exposure to the mass media, more cosmopolitan, greater contact with change agents, more social experience and exposure, higher socioeconomic status, and are more innovative than others.  Research was done in the early 1950s at the University of Chicago attempting to assess the cost-effectiveness of broadcast advertising on the diffusion of new products and services.[54] The findings were that opinion leadership tended to be organized into a hierarchy within a society, with each level in the hierarchy having most influence over other members in the same level, and on those in the next level below it. The lowest levels were generally larger in numbers and tended to coincide with various demographic attributes that might be targeted by mass advertising. However, it found that direct word of mouth and example were far more influential than broadcast messages, which were only effective if they reinforced the direct influences. This led to the conclusion that advertising was best targeted, if possible, on those next in line to adopt, and not on those not yet reached by the chain of influence.  Other research relating the concept to public choice theory finds that the hierarchy of influence for innovations need not, and likely does not, coincide with hierarchies of official, political, or economic status.[55] Elites are often not innovators, and innovations may have to be introduced by outsiders and propagated up a hierarchy to the top decision makers. Electronic communication social networks  Prior to the introduction of the Internet, it was argued that social networks had a crucial role in the diffusion of innovation particularly tacit knowledge in the book The IRG Solution – hierarchical incompetence and how to overcome it.[56] The book argued that the widespread adoption of computer networks of individuals would lead to much better diffusion of innovations, with greater understanding of their possible shortcomings and the identification of needed innovations that would not have otherwise occurred. The social model proposed by Ryan and Gross[36] is expanded by Valente who uses social networks as a basis for adopter categorization instead of solely relying on the system-level analysis used by Ryan and Gross. Valente also looks at an individual's personal network, which is a different application than the organizational perspective espoused by many other scholars.[57]  Recent research by Wear shows, that particularly in regional and rural areas, significantly more innovation takes place in communities which have stronger inter-personal networks.[58] Organizations  Innovations are often adopted by organizations through two types of innovation-decisions: collective innovation decisions and authority innovation decisions. The collective decision occurs when adoption is by consensus. The authority decision occurs by adoption among very few individuals with high positions of power within an organization.[59] Unlike the optional innovation decision process, these decision processes only occur within an organization or hierarchical group. Within an organization certain individuals are termed \"champions\" who stand behind an innovation and break through opposition. The champion plays a very similar role as the champion used within the efficiency business model Six Sigma. The process contains five stages that are slightly similar to the innovation-decision process that individuals undertake. These stages are: agenda-setting, matching, redefining/restructuring, clarifying and routinizing. Extensions of the theory Policy  Diffusion of Innovations has been applied beyond its original domains. In the case of political science and administration, policy diffusion focuses on how institutional innovations are adopted by other institutions, at the local, state, or country level. An alternative term is 'policy transfer' where the focus is more on the agents of diffusion and the diffusion of policy knowledge, such as in the work of Diane Stone.[60] Specifically, policy transfer can be defined as \"knowledge about how policies administrative arrangements, institutions, and ideas in one political setting (past or present) is used in the development of policies, administrative arrangements, institutions, and ideas in another political setting\".[61]  The first interests with regards to policy diffusion were focused in time variation or state lottery adoption,[62] but more recently interest has shifted towards mechanisms (emulation, learning and coercion)[63][64] or in channels of diffusion[65] where researchers find that regulatory agency creation is transmitted by country and sector channels. At the local level, examining popular city-level policies make it easy to find patterns in diffusion through measuring public awareness.[66] At the international level, economic policies have been thought to transfer among countries according to local politicians' learning of successes and failures elsewhere and outside mandates made by global financial organizations.[67] As a group of countries succeed with a set of policies, others follow, as exemplified by the deregulation and liberalization across the developing world after the successes of the Asian Tigers. The reintroduction of regulations in the early 2000s also shows this learning process, which would fit under the stages of knowledge and decision, can be seen as lessons learned by following China's successful growth.[68] Technology  Peres, Muller and Mahajan suggested that diffusion is \"the process of the market penetration of new products and services that is driven by social inﬂuences, which include all interdependencies among consumers that affect various market players with or without their explicit knowledge\".[69]  Eveland evaluated diffusion from a phenomenological view, stating, \"Technology is information, and exists only to the degree that people can put it into practice and use it to achieve values\".[70]  Diffusion of existing technologies has been measured using \"S curves\". These technologies include radio, television, VCR, cable, flush toilet, clothes washer, refrigerator, home ownership, air conditioning, dishwasher, electrified households, telephone, cordless phone, cellular phone, per capita airline miles, personal computer and the Internet. These data[71] can act as a predictor for future innovations.  Diffusion curves for infrastructure[72] reveal contrasts in the diffusion process of personal technologies versus infrastructure. Consequences of adoption  Both positive and negative outcomes are possible when an individual or organization chooses to adopt a particular innovation. Rogers states that this area needs further research because of the biased positive attitude that is associated with innovation.[73] Rogers lists three categories for consequences: desirable vs. undesirable, direct vs. indirect, and anticipated vs. unanticipated.  In contrast Wejnert details two categories: public vs. private and benefits vs. costs.[74] Public versus private  Public consequences comprise the impact of an innovation on those other than the actor, while private consequences refer to the impact on the actor. Public consequences usually involve collective actors, such as countries, states, organizations or social movements. The results are usually concerned with issues of societal well-being. Private consequences usually involve individuals or small collective entities, such as a community. The innovations are usually concerned with the improvement of quality of life or the reform of organizational or social structures.[75] Benefits versus costs  Benefits of an innovation obviously are the positive consequences, while the costs are the negative. Costs may be monetary or nonmonetary, direct or indirect. Direct costs are usually related to financial uncertainty and the economic state of the actor. Indirect costs are more difficult to identify. An example would be the need to buy a new kind of pesticide to use innovative seeds. Indirect costs may also be social, such as social conflict caused by innovation.[75] Marketers are particularly interested in the diffusion process as it determines the success or failure of a new product. It is quite important for a marketer to understand the diffusion process so as to ensure proper management of the spread of a new product or service. Mathematical treatment Main article: Logistic function  The diffusion of an innovation typically follows an S shaped curve which often resembles a logistic function. Mathematical programming models such as the S-D model apply the diffusion of innovations theory to real data problems.[76] Complex Systems models  Complex network models can also be used to investigate the spread of innovations among individuals connected to each other by a network of peer-to-peer influences, such as in a physical community or neighborhood.[77]  Such models represent a system of individuals as nodes in a network (or graph). The interactions that link these individuals are represented by the edges of the network and can be based on the probability or strength of social connections. In the dynamics of such models, each node is assigned a current state, indicating whether or not the individual has adopted the innovation, and model equations describe the evolution of these states over time.[78]  In threshold models,[79] the uptake of technologies is determined by the balance of two factors: the (perceived) usefulness (sometimes called utility) of the innovation to the individual as well as barriers to adoption, such as cost. The multiple parameters that influence decisions to adopt, both individual and socially motivated, can be represented by such models as a series of nodes and connections that represent real relationships. Borrowing from social network analysis, each node is an innovator, an adopter, or a potential adopter. Potential adopters have a threshold, which is a fraction of his neighbors who adopt the innovation that must be reached before he will adopt. Over time, each potential adopter views his neighbors and decides whether he should adopt based on the technologies they are using. When the effect of each individual node is analyzed along with its influence over the entire network, the expected level of adoption was seen to depend on the number of initial adopters and the network's structure and properties. Two factors emerge as important to successful spread of the innovation: the number of connections of nodes with their neighbors and the presence of a high degree of common connections in the network (quantified by the clustering coefficient). These models are particularly good at showing the impact of opinion leaders relative to others.[80] Computer models are often used to investigate this balance between the social aspects of diffusion and perceived intrinsic benefit to the individuals.[81] Criticism  Because there are more than four thousand articles across many disciplines published on Diffusion of Innovations, with a vast majority written after Rogers created a systematic theory, there have been few widely adopted changes to the theory.[7] Although each study applies the theory in slightly different ways, this lack of cohesion has left the theory stagnant and difficult to apply with consistency to new problems.[82][83]  Diffusion is difficult to quantify because humans and human networks are complex. It is extremely difficult, if not impossible, to measure what exactly causes adoption of an innovation.[84] This is important, particularly in healthcare. Those encouraging adoption of health behaviors or new medical technologies need to be aware of the many forces acting on an individual and his or her decision to adopt a new behavior or technology. Diffusion theories can never account for all variables, and therefore might miss critical predictors of adoption.[85] This variety of variables has also led to inconsistent results in research, reducing heuristic value.[86]  Rogers placed the contributions and criticisms of diffusion research into four categories: pro-innovation bias, individual-blame bias, recall problem, and issues of equality. The pro-innovation bias, in particular, implies that all innovation is positive and that all innovations should be adopted.[1] Cultural traditions and beliefs can be consumed by another culture's through diffusion, which can impose significant costs on a group of people.[86] The one-way information flow, from sender to receiver, is another weakness of this theory. The message sender has a goal to persuade the receiver, and there is little to no reverse flow. The person implementing the change controls the direction and outcome of the campaign. In some cases, this is the best approach, but other cases require a more participatory approach.[87] In complex environments where the adopter is receiving information from many sources and is returning feedback to the sender, a one-way model is insufficient and multiple communication flows need to be examined.[88] See also      Collaborative innovation network     Critical mass (sociodynamics)     Delphi technique     Hierarchical organization     Information Revolution     Lateral communication     Lateral diffusion     Lazy User Model     Memetics     Opinion leadership     Pro-innovation bias     Public Choice Theory     Sociological theory of diffusion     Tacit knowledge     Technological revolution     The Wisdom of Crowds   Ingenuity is the quality of being clever, original, and inventive, often in the process of applying ideas to solve problems or meet challenges. Ingenuity (Ingenium) is the root Latin word for engineering. For example, the process of figuring out how to cross a mountain stream using a fallen log, building an airplane model from a sheet of paper, or starting a new company in a foreign culture all involve the exercising of ingenuity. Human ingenuity has led to various technological developments through applied science, and can also be seen in the development of new social organizations, institutions, and relationships. Ingenuity involves the most complex human thought processes, bringing together our thinking and acting both individually and collectively to take advantage of opportunities and/or overcome problems.  One example of how ingenuity is used conceptually can be found in the analysis of Thomas Homer-Dixon, building on that of Paul Romer, to refer to what is usually called instructional capital. In the case of Homer-Dixon, his use of the phrase 'ingenuity gap' denotes the space between a challenge and a solution. His particular contribution is to explore the social dimensions of ingenuity. Typically we think of ingenuity being used to build faster computers or more advanced medical treatments. Homer-Dixon argues that as the complexity of the world increases, our ability to solve the problems we face is becoming critical. Human ingenuity is also included in many school systems, with most teachers encouraging students to be educated in human ingenuity.  These challenges require more than improvements arising from physics, chemistry and biology, as one will need to consider the highly complex interactions of individuals, institutions, cultures, and networks involving all of the human family around the globe. Organizing ourselves differently, communicating and making decisions in new ways, are examples of social ingenuity. If one's ability to generate adequate solutions to these problems is inadequate, the ingenuity gap will lead to a wide range of social problems. The full exploration of these ideas in meeting social challenges is featured in The Ingenuity Gap[1]  , one of Thomas Homer-Dixon's earliest books.  In another of Homer-Dixon's book, The Up Side of Down[2]  , he argues that increasingly expensive oil, driven by scarcity, will lead to great social instability. Walking across an empty room requires very little ingenuity. If the room is full of snakes, hungry bears, and land mines, the ingenuity requirement will have gone up considerably.  Ingenuity is often inherent in creative individuals, and thus is considered hard to separate from individual capital. It is not clear if Dixon or Romer considered it impossible to do so, or if they were simply not familiar with the prior analysis of \"applied ideas\", \"intellectual capital\", \"talent\", or \"innovation\" where instructional and individual contributions have been carefully separated, by economic theorists. See also \tLook up ingenuity in Wiktionary, the free dictionary.      Ingenuity Systems     Creativity techniques     Ingenuity Gap     instructional capital     International Innovation Index     Diffusion of innovations  The selection of the research method is crucial for what conclusions you can make about a phenomenon. It affects what you can say about the cause and factors influencing the phenomenon.  It is also important to choose a research method which is within the limits of what the researcher can do. Time, money, feasibility, ethics and availability to measure the phenomenon correctly are examples of issues constraining the research. Choosing the Measurement  Choosing the scientific measurements are also crucial for getting the correct conclusion. Some measurements might not reflect the real world, because they do not measure the phenomenon as it should. Results Significance Test  To test a hypothesis, quantitative research uses significance tests to determine which hypothesis is right.  The significance test can show whether the null hypothesis is more likely correct than the research hypothesis. Research methodology in a number of areas like social sciences depends heavily on significance tests.  A significance test may even drive the research process in a whole new direction, based on the findings.  The t-test (also called the Student's T-Test) is one of many statistical significance tests, which compares two supposedly equal sets of data to see if they really are alike or not. The t-test helps the researcher conclude whether a hypothesis is supported or not. Drawing Conclusions  Drawing a conclusion is based on several factors of the research process, not just because the researcher got the expected result. It has to be based on the validity and reliability of the measurement, how good the measurement was to reflect the real world and what more could have affected the results.  The observations are often referred to as 'empirical evidence' and the logic/thinking leads to the conclusions. Anyone should be able to check the observation and logic, to see if they also reach the same conclusions.  Errors of the observations may stem from measurement-problems, misinterpretations, unlikely random events etc.  A common error is to think that correlation implies a causal relationship. This is not necessarily true. Generalization  Generalization is to which extent the research and the conclusions of the research apply to the real world. It is not always so that good research will reflect the real world, since we can only measure a small portion of the population at a time.  Generalization in Research   Validity and Reliability  Validity refers to what degree the research reflects the given research problem, while Reliability refers to how consistent a set of measurements are.  Validity and Reliability  Types of validity:      External Validity     Population Validity     Ecological Validity     Internal Validity     Content Validity     Face Validity     Construct Validity     Convergent and Discriminant Validity     Test Validity     Criterion Validity     Concurrent Validity     Predictive Validity  A definition of reliability may be \"Yielding the same or compatible results in different clinical experiments or statistical trials\" (the free dictionary). Research methodology lacking reliability cannot be trusted. Replication studies are a way to test reliability.  Types of Reliability:      Test-Retest Reliability     Interrater Reliability     Internal Consistency Reliability     Instrument Reliability     Statistical Reliability     Reproducibility  Both validity and reliability are important aspects of the research methodology to get better explanations of the world. Errors in Research  Logically, there are two types of errors when drawing conclusions in research:  Type 1 error is when we accept the research hypothesis when the null hypothesis is in fact correct.  Type 2 error is when we reject the research hypothesis even if the null hypothesis is wrong. The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge.[2] To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.[3] The Oxford Dictionaries Online define the scientific method as \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses\".[4]  The scientific method is an ongoing process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.[1]  Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions.[5][6] A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.[7]  The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis.[8] Experiments can take place in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars, and so on. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles.[9] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order.[10] Some philosophers and scientists have argued that there is no scientific method. For example, Lee Smolin[11] and Paul Feyerabend (in his Against Method). Nola and Sankey remark that \"For some, the whole idea of a theory of scientific method is yester-year's debate\".[12] Contents      1 Overview         1.1 Process         1.2 DNA example         1.3 Other components     2 Scientific inquiry         2.1 Properties of scientific inquiry         2.2 Beliefs and biases     3 Elements of the scientific method         3.1 Characterizations         3.2 Hypothesis development         3.3 Predictions from the hypothesis         3.4 Experiments         3.5 Evaluation and improvement         3.6 Confirmation     4 Models of scientific inquiry         4.1 Classical model         4.2 Pragmatic model     5 Communication and community         5.1 Peer review evaluation         5.2 Documentation and replication         5.3 Dimensions of practice     6 Philosophy and sociology of science         6.1 Role of chance in discovery     7 History     8 Relationship with mathematics     9 Relationship with statistics     10 See also         10.1 Problems and issues         10.2 History, philosophy, sociology     11 Notes     12 References     13 Further reading     14 External links  Overview      The DNA example below is a synopsis of this method  Ibn al-Haytham (Alhazen), 965–1039 Iraq. A polymath, considered by some to be the father of modern scientific methodology, due to his emphasis on experimental data and reproducibility of its results.[13][14] Johannes Kepler (1571–1630). \"Kepler shows his keen logical sense in detailing the whole process by which he finally arrived at the true orbit. This is the greatest piece of Retroductive reasoning ever performed.\" – C. S. Peirce, c. 1896, on Kepler's reasoning through explanatory hypotheses[15] According to Morris Kline,[16] \"Modern science owes its present flourishing state to a new scientific method which was fashioned almost entirely by Galileo Galilei\" (1564−1642). Dudley Shapere[17] takes a more measured view of Galileo's contribution.  The scientific method is the process by which science is carried out.[18] As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.[19][20][21][22][23][24] This model can be seen to underlay the scientific revolution.[25] One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them,[26] an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences.[27] The current method is based on a hypothetico-deductive model[28] formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below). Process  The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct.[5] There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles.[29] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), \"invention, sagacity, [and] genius\"[10] are required at every step. Formulation of a question  The question can refer to the explanation of a specific observation, as in \"Why is the sky blue?\", but can also be open-ended, as in \"How can I design a drug to cure this particular disease?\" This stage frequently involves looking up and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.[30] Hypothesis  A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's \"DNA makes RNA makes protein\",[31] or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested. Prediction  This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).[32] Testing  This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk.[8] Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually. Analysis  This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question. DNA example DNA icon (25x25).png \tThe basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:      Question: Previous investigation of DNA had determined its chemical composition (the four nucleotides), the structure of each individual nucleotide, and other properties. It had been identified as the carrier of genetic information by the Avery–MacLeod–McCarty experiment in 1944,[33] but the mechanism of how genetic information was stored in DNA was unclear.     Hypothesis: Linus Pauling, Francis Crick and James D. Watson hypothesized that DNA had a helical structure.[34]     Prediction: If DNA had a helical structure, its X-ray diffraction pattern would be X-shaped.[35][36] This prediction was determined using the mathematics of the helix transform, which had been derived by Cochran, Crick and Vand[37] (and independently by Stokes). This prediction was a mathematical construct, completely independent from the biological problem at hand.     Experiment: Rosalind Franklin crystallized pure DNA and performed X-ray diffraction to produce photo 51. The results showed an X-shape.     Analysis: When Watson saw the detailed diffraction pattern, he immediately recognized it as a helix.[38][39] He and Crick then produced their model, using this information along with the previously known information about DNA's composition and about molecular interactions such as hydrogen bonds.[40]  The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article. Other components  The scientific method also includes other components required even when all the iterations of the steps above have been completed:[41] Replication  If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.[42] External review  The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.[43] Data recording and sharing  Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others.[44] Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.[45] Scientific inquiry  Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that can be used to predict the results of future experiments. This allows scientists to gain a better understanding of the topic being studied, and later be able to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it is to continue explaining a body of evidence better than its alternatives. The most successful explanations, which explain and make accurate predictions in a wide range of circumstances, are often called scientific theories.  Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding is typically the result of a gradual process of development over time, sometimes across different domains of science.[46] Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question is more powerful than its alternatives at explaining the evidence. Often the explanations are altered over time, or explanations are combined to produce new explanations. Properties of scientific inquiry  Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to be related to how long it has persisted without major alteration to its core principles.  Theories can also subject to subsumption by other theories. For example, thousands of years of scientific observations of the planets were explained almost perfectly by Newton's laws. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicting and explaining other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.[47]  Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors.[47] For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world;[48][49] its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology. Beliefs and biases Flying gallop falsified; see image below Muybridge's photographs of The Horse in Motion, 1878, were used to answer the question whether all four feet of a galloping horse are ever off the ground at the same time. This demonstrates a use of photography in science.  Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).  A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.[50] Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true.[2] In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic,[51] sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe.[52][53] Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.[54] Elements of the scientific method  There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.      Four essential elements[55][56][57] of the scientific method[58] are iterations,[59][60] recursions,[61] interleavings, or orderings of the following:          Characterizations (observations,[62] definitions, and measurements of the subject of inquiry)         Hypotheses[63][64] (theoretical, hypothetical explanations of observations and measurements of the subject)[65]         Predictions (reasoning including deductive reasoning[66] from the hypothesis or theory)         Experiments[67] (tests of all of the above)  Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as \"the scientific method\".[68]  The scientific method is not a single recipe: it requires intelligence, imagination, and creativity.[69] In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.  A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:[70]      Define a question     Gather information and resources (observe)     Form an explanatory hypothesis     Test the hypothesis by performing an experiment and collecting data in a reproducible manner     Analyze the data     Interpret the data and draw conclusions that serve as a starting point for new hypothesis     Publish results     Retest (frequently done by other scientists)  The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.  While this schema outlines a typical hypothesis/testing method,[71] it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced. Characterizations  The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.  The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.      I am not accustomed to saying anything with certainty after only one or two observations.     — Andreas Vesalius, (1546)[72]  Uncertainty  Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken. Definition  Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or \"idealized\" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of \"mass\" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.  The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.  New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, \"I do not define time, space, place and motion, as being well known to all.\" Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood.[73] In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them. DNA-characterizations DNA icon (25x25).png  The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle).[33] But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle.[74] ..2. DNA-hypotheses Another example: precession of Mercury Precession of the perihelion (exaggerated)  The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century. Hypothesis development Main article: Hypothesis formation  A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.  Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.  Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the \"irritation of doubt\" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a \"flash of inspiration\", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.  William Glen observes that      the success of a hypothesis, or its service to science, lies not simply in its perceived \"truth\", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.[75]  In general scientists tend to look for theories that are \"elegant\" or \"beautiful\". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses. DNA-hypotheses DNA icon (25x25).png  Linus Pauling proposed that DNA might be a triple helix.[76] This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong[77] and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions Predictions from the hypothesis Main article: Prediction in science  Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.  It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.  If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science. DNA-predictions DNA icon (25x25).png  James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'.[36][78] This prediction followed from the work of Cochran, Crick and Vand[37] (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.  In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, \"It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material\".[79] ..4. DNA-experiments Another example: general relativity Einstein's prediction (1907): Light bends in a gravitational field  Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.[80] Experiments Main article: Experiment  Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is.[81] Factor analysis is one technique for discovering the important factor in an effect.  Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.  Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).[82] DNA-experiments DNA icon (25x25).png  Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape  and was able to confirm the structure was helical.[38][39] This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations Evaluation and improvement  The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.  Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility. DNA-iterations DNA icon (25x25).png  After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts,[83][84][85] Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it.[40][86] They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example Confirmation  Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.[87]  To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center. Models of scientific inquiry Main article: Models of scientific inquiry Classical model  The classical model of scientific inquiry derives from Aristotle,[88] who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy. Pragmatic model See also: Pragmatic theory of truth  In 1877,[19] Charles Sanders Peirce (/ˈpɜːrs/ like \"purse\"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless.[89] He outlined four methods of settling opinion, ordered from least to most successful:      The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.[90]     The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.     The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of \"what is agreeable to reason.\" Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.     The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.  Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research,[91] which in turn should not be trammeled by the other methods and practical ends; reason's \"first rule\" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry.[92] The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.[19][22]  For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points.[93] In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to \"Do the science.\" Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge.[94] As inference, \"logic is rooted in the social principle\" since it depends on a standpoint that is, in a sense, unlimited.[95]  Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in \"A Neglected Argument\"[5] except as otherwise noted):      Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the \"facile and natural\", as by Galileo's natural light of reason and as distinct from \"logical simplicity\". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths.[96] Coordinative method leads from abducing a plausible hypothesis to judging it for its testability[97] and for how its trial would economize inquiry itself.[98] Peirce calls his pragmatism \"the logic of abduction\".[99] His pragmatic maxim is: \"Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object\".[93] His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity.[100] One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.[98]     Deduction. Two stages:         Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.         Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, Theorematic.     Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general[93]) that the real is only the object of the final opinion to which adequate investigation would lead;[101] anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:         Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.         Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters;[102] if quantitative, then dependent on measurements, or on statistics, or on countings.         Sentential Induction. \"...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result\".  Communication and community See also: Scientific community and Scholarly communication  Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment. Peer review evaluation  Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of \"groupthink\" can interfere with open and fair deliberation of some new research.[103] Documentation and replication Main article: Reproducibility  Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results. Archiving  Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery. Data sharing  When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research. Limitations  Since it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'. Dimensions of practice Further information: Rhetoric of science  The primary constraints on contemporary science are:      Publication, i.e. Peer review     Resources (mostly funding)  It has not always been like this: in the old days of the \"gentleman scientist\" funding (and to a lesser extent publication) were far weaker constraints.  Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to \"good scientific practice\" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines  for Nature. Philosophy and sociology of science See also: Philosophy of science and Sociology of science  Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world.[104] These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.[105]  Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.  Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the \"theory laden\" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description.[106] He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a \"different\" sun rise despite the same physiological phenomenon. Kuhn[107] and Feyerabend[108] acknowledge the pioneering significance of his work.  Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the \"route from theory to measurement can almost never be traveled backward\". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that \"once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed\".[109]  Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'.[110] Criticisms such as his led to the strong programme, a radical approach to the sociology of science.  The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.[111] Role of chance in discovery Main article: Role of chance in scientific discoveries  Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky.[112] Louis Pasteur is credited with the famous saying that \"Luck favours the prepared mind\", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected.[112][113] This is what Nassim Nicholas Taleb calls \"Anti-fragility\"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.[23]  Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.[112][113] History Main article: History of scientific method See also: Timeline of the history of scientific method Accuracy dispute \tThis article appears to contradict the article History of scientific method. Please see discussion on the linked talk page. (June 2015) (Learn how and when to remove this template message) \tThis section may contain an excessive amount of intricate detail that may only interest a specific audience. Please help by spinning off or relocating any relevant information, and removing excessive detail that may be against Wikipedia's inclusion policy. (June 2015) (Learn how and when to remove this template message) Aristotle, 384 BCE – 322 BCE. \"As regards his method, Aristotle is recognized as the inventor of scientific method because of his refined analysis of logical implications contained in demonstrative discourse, which goes well beyond natural logic and does not owe anything to the ones who philosophized before him.\" – Riccardo Pozzo[114]  The development of the scientific method emerges in the history of science itself. Ancient Egyptian documents describe empirical methods in astronomy,[115] mathematics,[116] and medicine.[117] The Greeks made contributions to the scientific method, most notably through Aristotle in his six works of logic collected as the Organon. Aristotle's inductive-deductive method used inductions from observations to infer general principles, deductions from those principles to check against further observations, and more cycles of induction and deduction to continue the advance of knowledge[118]  According to Karl Popper, Parmenides (fl. 5th century BCE) had conceived an axiomatic-deductive method.[119] According to David Lindberg, Aristotle (4th century BCE) wrote about the scientific method even if he and his followers did not actually follow what he said.[67] Lindberg also notes that Ptolemy (2nd century CE) and Ibn al-Haytham (11th century CE) are among the early examples of people who carried out scientific experiments.[120] Also, John Losee writes that \"the Physics and the Metaphysics contain discussions of certain aspects of scientific method\", of which, he says \"Aristotle viewed scientific inquiry as a progression from observations to general principles and back to observations.\"[121]  Early Christian leaders such as Clement of Alexandria (150–215) and Basil of Caesarea (330–379) encouraged future generations to view the Greek wisdom as \"handmaidens to theology\" and science was considered a means to more accurate understanding of the Bible and of God.[122]:pp.4–5 Augustine of Hippo (354–430) who contributed great philosophical wealth to the Latin Middle Ages, advocated the study of science and was wary of philosophies that disagreed with the Bible, such as astrology and the Greek belief that the world had no beginning.[122]:p.5 This Christian accommodation with Greek science \"laid a foundation for the later widespread, intensive study of natural philosophy during the Late Middle Ages.\"[122]:pp.8,9 However, the division of Latin-speaking Western Europe from the Greek-speaking East,[122]:p.18 followed by barbarian invasions, the Plague of Justinian, and the Islamic conquests,[123] resulted in the West largely losing access to Greek wisdom.  By the 8th century Islam had conquered the Christian lands[124] of Syria, Iraq, Iran and Egypt.[125] This swift conquest further severed Western Europe from many of the great works of Aristotle, Plato, Euclid and others, many of which were housed in the great library of Alexandria. Having come upon such a wealth of knowledge, the Arabs, who viewed non-Arab languages as inferior, even as a source of pollution,[126] employed conquered Christians and Jews to translate these works from the native Greek and Syriac into Arabic.[127]  Thus equipped, Arab philosopher Alhazen (Ibn al-Haytham) performed optical and physiological experiments, reported in his manifold works, the most famous being Book of Optics (1021).[128] He was thus a forerunner of scientific method, having understood that a controlled environment involving experimentation and measurement is required in order to draw educated conclusions. Other Arab polymaths of the same era produced copious works on mathematics, philosophy, astronomy and alchemy. Most stuck closely to Aristotle, being hesitant to admit that some of Aristotle's thinking was errant,[129] while others strongly criticized him.  During these years, occasionally a paraphrased translation from the Arabic, which itself had been translated from Greek and Syriac, might make its way to the West for scholarly study. It was not until 1204, during which the Latins conquered and took Constantinople from the Byzantines in the name of the fourth Crusade, that a renewed scholarly interest in the original Greek manuscripts began to grow. Due to the new easier access to the libraries of Constantinople by Western scholars, a certain revival in the study and analysis of the original Greek texts by Western scholars began.[130] From that point a functional scientific method that would launch modern science was on the horizon.  Grosseteste (1175–1253), an English statesman, scientist and Christian theologian, was \"the principal figure\" in bringing about \"a more adequate method of scientific inquiry\" by which \"medieval scientists were able eventually to outstrip their ancient European and Muslim teachers\" (Dales 1973, p. 62). ... His thinking influenced Roger Bacon, who spread Grosseteste's ideas from Oxford to the University of Paris during a visit there in the 1240s. From the prestigious universities in Oxford and Paris, the new experimental science spread rapidly throughout the medieval universities: \"And so it went to Galileo, William Gilbert, Francis Bacon, William Harvey, Descartes, Robert Hooke, Newton, Leibniz, and the world of the seventeenth century\" (Crombie 1953, p. 15). \"So it went to us as well \" (Gauch 2003, pp. 52–53). Roger Bacon (c. 1214 – c. 1292) is sometimes credited as one of the earliest European advocates of the modern scientific method inspired by the works of Aristotle.[131]  Roger Bacon (c. 1214 – c. 1292), an English thinker and experimenter, is recognized by many to be the father of modern scientific method. His view that mathematics was essential to a correct understanding of natural philosophy was considered to be 400 years ahead of its time.[132]:2 He was viewed as \"a lone genius proclaiming the truth about time,\" having correctly calculated the calendar[132]:3 His work in optics provided the platform on which Newton, Descartes, Huygens and others later transformed the science of light. Bacon's groundbreaking advances were due largely to his discovery that experimental science must be based on mathematics. (186–187) His works Opus Majus and De Speculis Comburentibus contain many \"carefully drawn diagrams showing Bacon's meticulous investigations into the behavior of light.\"[132]:66 He gives detailed descriptions of systematic studies using prisms and measurements by which he shows how a rainbow functions.[132]:200  Others who advanced scientific method during this era included Albertus Magnus (c. 1193 – 1280), Theodoric of Freiberg, (c. 1250 – c. 1310), William of Ockham (c. 1285 – c. 1350), and Jean Buridan (c. 1300 – c. 1358). These were not only scientists but leaders of the church – Christian archbishops, friars and priests.  By the late 15th century, the physician-scholar Niccolò Leoniceno was finding errors in Pliny's Natural History. As a physician, Leoniceno was concerned about these botanical errors propagating to the materia medica on which medicines were based.[133] To counter this, a botanical garden was established at Orto botanico di Padova, University of Padua (in use for teaching by 1546), in order that medical students might have empirical access to the plants of a pharmacopia. The philosopher and physician Francisco Sanches was led by his medical training at Rome, 1571–73, and by the philosophical skepticism recently placed in the European mainstream by the publication of Sextus Empiricus' \"Outlines of Pyrrhonism\", to search for a true method of knowing (modus sciendi), as nothing clear can be known by the methods of Aristotle and his followers[134] – for example, syllogism fails upon circular reasoning. Following the physician Galen's method of medicine, Sanches lists the methods of judgement and experience, which are faulty in the wrong hands,[135] and we are left with the bleak statement That Nothing is Known (1581). This challenge was taken up by René Descartes in the next generation (1637), but at the least, Sanches warns us that we ought to refrain from the methods, summaries, and commentaries on Aristotle, if we seek scientific knowledge. In this, he is echoed by Francis Bacon, also influenced by skepticism; Sanches cites the humanist Juan Luis Vives who sought a better educational system, as well as a statement of human rights as a pathway for improvement of the lot of the poor.  The modern scientific method crystallized no later than in the 17th and 18th centuries. In his work Novum Organum (1620) – a reference to Aristotle's Organon – Francis Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism.[136] Then, in 1637, René Descartes established the framework for scientific method's guiding principles in his treatise, Discourse on Method. The writings of Alhazen, Bacon and Descartes are considered critical in the historical development of the modern scientific method, as are those of John Stuart Mill.[137]  In the late 19th century, Charles Sanders Peirce proposed a schema that would turn out to have considerable influence in the development of current scientific methodology generally. Peirce accelerated the progress on several fronts. Firstly, speaking in broader context in \"How to Make Our Ideas Clear\" (1878)  , Peirce outlined an objectively verifiable method to test the truth of putative knowledge on a way that goes beyond mere foundational alternatives, focusing upon both deduction and induction. He thus placed induction and deduction in a complementary rather than competitive context (the latter of which had been the primary trend at least since David Hume, who wrote in the mid-to-late 18th century). Secondly, and of more direct importance to modern method, Peirce put forth the basic schema for hypothesis/testing that continues to prevail today. Extracting the theory of inquiry from its raw materials in classical logic, he refined it in parallel with the early development of symbolic logic to address the then-current problems in scientific reasoning. Peirce examined and articulated the three fundamental modes of reasoning that, as discussed above in this article, play a role in inquiry today, the processes that are currently known as abductive, deductive, and inductive inference. Thirdly, he played a major role in the progress of symbolic logic itself – indeed this was his primary specialty.  Beginning in the 1930s, Karl Popper argued that there is no such thing as inductive reasoning.[138] All inferences ever made, including in science, are purely[139] deductive according to this view. Accordingly, he claimed that the empirical character of science has nothing to do with induction – but with the deductive property of falsifiability that scientific hypotheses have. Contrasting his views with inductivism and positivism, he even denied the existence of the scientific method: \"(1) There is no method of discovering a scientific theory (2) There is no method for ascertaining the truth of a scientific hypothesis, i.e., no method of verification; (3) There is no method for ascertaining whether a hypothesis is 'probable', or probably true\".[140] Instead, he held that there is only one universal method, a method not particular to science: The negative method of criticism, or colloquially termed trial and error. It covers not only all products of the human mind, including science, mathematics, philosophy, art and so on, but also the evolution of life. Following Peirce and others, Popper argued that science is fallible and has no authority.[140] In contrast to empiricist-inductivist views, he welcomed metaphysics and philosophical discussion and even gave qualified support to myths[141] and pseudosciences.[142] Popper's view has become known as critical rationalism.  Although science in a broad sense existed before the modern era, and in many historical civilizations (as described above), modern science is so distinct in its approach and successful in its results that it now defines what science is in the strictest sense of the term.[143] Relationship with mathematics  Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.[144]  Mathematical work and scientific work can inspire each other.[145] For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).  Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.  George Pólya's work on problem solving,[146] the construction of mathematical proofs, and heuristic[147][148] show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps. \tMathematical method \tScientific method 1 \tUnderstanding \tCharacterization from experience and observation 2 \tAnalysis \tHypothesis: a proposed explanation 3 \tSynthesis \tDeduction: prediction from the hypothesis 4 \tReview/Extend \tTest and experiment  In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus,[149] involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details[150] of the proof; review involves reconsidering and re-examining the result and the path taken to it.  Gauss, when asked how he came about his theorems, once replied \"durch planmässiges Tattonieren\" (through systematic palpable experimentation).[151]  Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work.[152] In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)  Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.[153] Relationship with statistics  The scientific method has been extremely successful in bringing the world out of medieval times, especially once it was combined with industrial processes.[154] However, when the scientific method employs statistics as part of its arsenal, there are a number of both mathematical and practical issues that can have a deleterious effect on the reliability of the output of the scientific methods. This is outlined in detail in the most downloaded 2005 scientific paper \"Why Most Published Research Findings Are False\"[155] ever by John Ioannidis.  The particular points raised are statistical (\"The smaller the studies conducted in a scientific field, the less likely the research findings are to be true\" and \"The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\") and economical (\"The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true\" and \"The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\") Hence: \"Most research findings are false for most research designs and for most fields\" and \"As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings.\" However: \"Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds,\" which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries. See also      Armchair theorizing     Confirmability     Contingency     Empirical limits in science     Evidence-based medicine     Fuzzy logic     Inquiry     Information theory     Logic     Methodology         Historical         Philosophical         Phronetic         Scholarly     Operationalization     Quantitative research     Replication crisis     Social research     Statistical hypothesis testing     Strong inference     Testability  Problems and issues      Demarcation problem     Holistic science     Inductive reasoning     Junk science     List of cognitive biases     Normative science     Occam's razor     Pseudoscience     Poverty of the stimulus     Problem of induction     Reference class problem     Scientific misconduct     Skeptical hypotheses     Underdetermination  History, philosophy, sociology      Epistemology     Epistemic truth     History of scientific method     Mertonian norms     Normal science     Post-normal science     Science studies     Sociology of scientific knowledge     Timeline of the history of scientific method   Engaging learners in the excitement of science, helping them discover the value of evidence-based reasoning and higher-order cognitive skills, and teaching them to become creative problem solvers have long been goals of science education reformers. But the means to achieve these goals, especially methods to promote creative thinking in scientific problem solving, have not become widely known or used. In this essay, I review the evidence that creativity is not a single hard-to-measure property. The creative process can be explained by reference to increasingly well-understood cognitive skills such as cognitive flexibility and inhibitory control that are widely distributed in the population. I explore the relationship between creativity and the higher-order cognitive skills, review assessment methods, and describe several instructional strategies for enhancing creative problem solving in the college classroom. Evidence suggests that instruction to support the development of creativity requires inquiry-based teaching that includes explicit strategies to promote cognitive flexibility. Students need to be repeatedly reminded and shown how to be creative, to integrate material across subject areas, to question their own assumptions, and to imagine other viewpoints and possibilities. Further research is required to determine whether college students' learning will be enhanced by these measures. INTRODUCTION  Dr. Dunne paces in front of his section of first-year college students, today not as their Bio 110 teacher but in the role of facilitator in their monthly “invention session.” For this meeting, the topic is stem cell therapy in heart disease. Members of each team of four students have primed themselves on the topic by reading selected articles from accessible sources such as Science, Nature, and Scientific American, and searching the World Wide Web, triangulating for up-to-date, accurate, background information. Each team knows that their first goal is to define a set of problems or limitations to overcome within the topic and to begin to think of possible solutions. Dr. Dunne starts the conversation by reminding the group of the few ground rules: one speaker at a time, listen carefully and have respect for others' ideas, question your own and others' assumptions, focus on alternative paths or solutions, maintain an atmosphere of collaboration and mutual support. He then sparks the discussion by asking one of the teams to describe a problem in need of solution.  Science in the United States is widely credited as a major source of discovery and economic development. According to the 2005 TAP Report produced by a prominent group of corporate leaders, “To maintain our country's competitiveness in the twenty-first century, we must cultivate the skilled scientists and engineers needed to create tomorrow's innovations.” (www.tap2015.org/about/TAP_report2.pdf). A panel of scientists, engineers, educators, and policy makers convened by the National Research Council (NRC) concurred with this view, reporting that the vitality of the nation “is derived in large part from the productivity of well-trained people and the steady stream of scientific and technical innovations they produce” (NRC, 2007 blue right-pointing triangle).  For many decades, science education reformers have promoted the idea that learners should be engaged in the excitement of science; they should be helped to discover the value of evidence-based reasoning and higher-order cognitive skills, and be taught to become innovative problem solvers (for reviews, see DeHaan, 2005 blue right-pointing triangle; Hake, 2005 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle; Perkins and Wieman, 2008 blue right-pointing triangle). But the means to achieve these goals, especially methods to promote creative thinking in scientific problem solving, are not widely known or used. An invention session such as that led by the fictional Dr. Dunne, described above, may seem fanciful as a means of teaching students to think about science as something more than a body of facts and terms to memorize. In recent years, however, models for promoting creative problem solving were developed for classroom use, as detailed by Treffinger and Isaksen (2005) blue right-pointing triangle, and such techniques are often used in the real world of high technology. To promote imaginative thinking, the advertising executive Alex F. Osborn invented brainstorming (Osborn, 1948 blue right-pointing triangle, 1979 blue right-pointing triangle), a technique that has since been successful in stimulating inventiveness among engineers and scientists. Could such strategies be transferred to a class for college students? Could they serve as a supplement to a high-quality, scientific teaching curriculum that helps students learn the facts and conceptual frameworks of science and make progress along the novice–expert continuum? Could brainstorming or other instructional strategies that are specifically designed to promote creativity teach students to be more adaptive in their growing expertise, more innovative in their problem-solving abilities? To begin to answer those questions, we first need to understand what is meant by “creativity.” OVERVIEW What Is Creativity? Big-C versus Mini-C Creativity  How to define creativity is an age-old question. Justice Potter Stewart's famous dictum regarding obscenity “I know it when I see it” has also long been an accepted test of creativity. But this is not an adequate criterion for developing an instructional approach. A scientist colleague of mine recently noted that “Many of us [in the scientific community] rarely give the creative process a second thought, imagining one either ‘has it’ or doesn't.” We often think of inventiveness or creativity in scientific fields as the kind of gift associated with a Michelangelo or Einstein. This is what Kaufman and Beghetto (2008) blue right-pointing triangle call big-C creativity, borrowing the term that earlier workers applied to the talents of experts in various fields who were identified as particularly creative by their expert colleagues (MacKinnon, 1978 blue right-pointing triangle). In this sense, creativity is seen as the ability of individuals to generate new ideas that contribute substantially to an intellectual domain. Howard Gardner defined such a creative person as one who “regularly solves problems, fashions products, or defines new questions in a domain in a way that is initially considered novel but that ultimately comes to be accepted in a particular cultural setting” (Gardner, 1993 blue right-pointing triangle, p. 35).  But there is another level of inventiveness termed by various authors as “little-c” (Craft, 2000 blue right-pointing triangle) or “mini-c” (Kaufman and Beghetto, 2008 blue right-pointing triangle) creativity that is widespread among all populations. This would be consistent with the workplace definition of creativity offered by Amabile and her coworkers: “coming up with fresh ideas for changing products, services and processes so as to better achieve the organization's goals” (Amabile et al., 2005 blue right-pointing triangle). Mini-c creativity is based on what Craft calls “possibility thinking” (Craft, 2000 blue right-pointing triangle, pp. 3–4), as experienced when a worker suddenly has the insight to visualize a new, improved way to accomplish a task; it is represented by the “aha” moment when a student first sees two previously disparate concepts or facts in a new relationship, an example of what Arthur Koestler identified as bisociation: “perceiving a situation or event in two habitually incompatible associative contexts” (Koestler, 1964 blue right-pointing triangle, p. 95).  In this essay, I maintain that mini-c creativity is not a mysterious, innate endowment of rare individuals. Instead, I argue that creative thinking is a multicomponent process, mediated through social interactions, that can be explained by reference to increasingly well-understood mental abilities such as cognitive flexibility and cognitive control that are widely distributed in the population. Moreover, I explore some of the recent research evidence (though with no effort at a comprehensive literature review) showing that these mental abilities are teachable; like other higher-order cognitive skills (HOCS), they can be enhanced by explicit instruction. Creativity Is a Multicomponent Process  Efforts to define creativity in psychological terms go back to J. P. Guilford (Guilford, 1950 blue right-pointing triangle) and E. P. Torrance (Torrance, 1974 blue right-pointing triangle), both of whom recognized that underlying the construct were other cognitive variables such as ideational fluency, originality of ideas, and sensitivity to missing elements. Many authors since then have extended the argument that a creative act is not a singular event but a process, an interplay among several interactive cognitive and affective elements. In this view, the creative act has two phases, a generative and an exploratory or evaluative phase (Finke et al., 1996 blue right-pointing triangle). During the generative process, the creative mind pictures a set of novel mental models as potential solutions to a problem. In the exploratory phase, we evaluate the multiple options and select the best one. Early scholars of creativity, such as J. P. Guilford, characterized the two phases as divergent thinking and convergent thinking (Guilford, 1950 blue right-pointing triangle). Guilford defined divergent thinking as the ability to produce a broad range of associations to a given stimulus or to arrive at many solutions to a problem (for overviews of the field from different perspectives, see Amabile, 1996 blue right-pointing triangle; Banaji et al., 2006 blue right-pointing triangle; Sawyer, 2006 blue right-pointing triangle). In neurocognitive terms, divergent thinking is referred to as associative richness (Gabora, 2002 blue right-pointing triangle; Simonton, 2004 blue right-pointing triangle), which is often measured experimentally by comparing the number of words that an individual generates from memory in response to stimulus words on a word association test. In contrast, convergent thinking refers to the capacity to quickly focus on the one best solution to a problem.  The idea that there are two stages to the creative process is consistent with results from cognition research indicating that there are two distinct modes of thought, associative and analytical (Neisser, 1963 blue right-pointing triangle; Sloman, 1996 blue right-pointing triangle). In the associative mode, thinking is defocused, suggestive, and intuitive, revealing remote or subtle connections between items that may be correlated, or may not, and are usually not causally related (Burton, 2008 blue right-pointing triangle). In the analytical mode, thought is focused and evaluative, more conducive to analyzing relationships of cause and effect (for a review of other cognitive aspects of creativity, see Runco, 2004 blue right-pointing triangle). Science educators associate the analytical mode with the upper levels (analysis, synthesis, and evaluation) of Bloom's taxonomy (e.g., Crowe et al., 2008 blue right-pointing triangle), or with “critical thinking,” the process that underlies the “purposeful, self-regulatory judgment that drives problem-solving and decision-making” (Quitadamo et al., 2008 blue right-pointing triangle, p. 328). These modes of thinking are under cognitive control through the executive functions of the brain. The core executive functions, which are thought to underlie all planning, problem solving, and reasoning, are defined (Blair and Razza, 2007 blue right-pointing triangle) as working memory control (mentally holding and retrieving information), cognitive flexibility (considering multiple ideas and seeing different perspectives), and inhibitory control (resisting several thoughts or actions to focus on one). Readers wishing to delve further into the neuroscience of the creative process can refer to the cerebrocerebellar theory of creativity (Vandervert et al., 2007 blue right-pointing triangle) in which these mental activities are described neurophysiologically as arising through interactions among different parts of the brain.  The main point from all of these works is that creativity is not some single hard-to-measure property or act. There is ample evidence that the creative process requires both divergent and convergent thinking and that it can be explained by reference to increasingly well-understood underlying mental abilities (Haring-Smith, 2006 blue right-pointing triangle; Kim, 2006 blue right-pointing triangle; Sawyer, 2006 blue right-pointing triangle; Kaufman and Sternberg, 2007 blue right-pointing triangle) and cognitive processes (Simonton, 2004 blue right-pointing triangle; Diamond et al., 2007 blue right-pointing triangle; Vandervert et al., 2007 blue right-pointing triangle). Creativity Is Widely Distributed and Occurs in a Social Context  Although it is understandable to speak of an aha moment as a creative act by the person who experiences it, authorities in the field have long recognized (e.g., Simonton, 1975 blue right-pointing triangle) that creative thinking is not so much an individual trait but rather a social phenomenon involving interactions among people within their specific group or cultural settings. “Creativity isn't just a property of individuals, it is also a property of social groups” (Sawyer, 2006 blue right-pointing triangle, p. 305). Indeed, Osborn introduced his brainstorming method because he was convinced that group creativity is always superior to individual creativity. He drew evidence for this conclusion from activities that demand collaborative output, for example, the improvisations of a jazz ensemble. Although each musician is individually creative during a performance, the novelty and inventiveness of each performer's playing is clearly influenced, and often enhanced, by “social and interactional processes” among the musicians (Sawyer, 2006 blue right-pointing triangle, p. 120). Recently, Brophy (2006) blue right-pointing triangle offered evidence that for problem solving, the situation may be more nuanced. He confirmed that groups of interacting individuals were better at solving complex, multipart problems than single individuals. However, when dealing with certain kinds of single-issue problems, individual problem solvers produced a greater number of solutions than interacting groups, and those solutions were judged to be more original and useful.  Consistent with the findings of Brophy (2006) blue right-pointing triangle, many scholars acknowledge that creative discoveries in the real world such as solving the problems of cutting-edge science—which are usually complex and multipart—are influenced or even stimulated by social interaction among experts. The common image of the lone scientist in the laboratory experiencing a flash of creative inspiration is probably a myth from earlier days. As a case in point, the science historian Mara Beller analyzed the social processes that underlay some of the major discoveries of early twentieth-century quantum physics. Close examination of successive drafts of publications by members of the Copenhagen group revealed a remarkable degree of influence and collaboration among 10 or more colleagues, although many of these papers were published under the name of a single author (Beller, 1999 blue right-pointing triangle). Sociologists Bruno Latour and Steve Woolgar's study (Latour and Woolgar, 1986 blue right-pointing triangle) of a neuroendocrinology laboratory at the Salk Institute for Biological Studies make the related point that social interactions among the participating scientists determined to a remarkable degree what discoveries were made and how they were interpreted. In the laboratory, researchers studied the chemical structure of substances released by the brain. By analysis of the Salk scientists' verbalizations of concepts, theories, formulas, and results of their investigations, Latour and Woolgar showed that the structures and interpretations that were agreed upon, that is, the discoveries announced by the laboratory, were mediated by social interactions and power relationships among members of the laboratory group. By studying the discovery process in other fields of the natural sciences, sociologists and anthropologists have provided more cases that further illustrate how social and cultural dimensions affect scientific insights (for a thoughtful review, see Knorr Cetina, 1995 blue right-pointing triangle).  In sum, when an individual experiences an aha moment that feels like a singular creative act, it may rather have resulted from a multicomponent process, under the influence of group interactions and social context. The process that led up to what may be sensed as a sudden insight will probably have included at least three diverse, but testable elements: 1) divergent thinking, including ideational fluency or cognitive flexibility, which is the cognitive executive function that underlies the ability to visualize and accept many ideas related to a problem; 2) convergent thinking or the application of inhibitory control to focus and mentally evaluate ideas; and 3) analogical thinking, the ability to understand a novel idea in terms of one that is already familiar. LITERATURE REVIEW What Do We Know about How to Teach Creativity?  The possibility of teaching for creative problem solving gained credence in the 1960s with the studies of Jerome Bruner, who argued that children should be encouraged to “treat a task as a problem for which one invents an answer, rather than finding one out there in a book or on the blackboard” (Bruner, 1965 blue right-pointing triangle, pp. 1013–1014). Since that time, educators and psychologists have devised programs of instruction designed to promote creativity and inventiveness in virtually every student population: pre–K, elementary, high school, and college, as well as in disadvantaged students, athletes, and students in a variety of specific disciplines (for review, see Scott et al., 2004 blue right-pointing triangle). Smith (1998) blue right-pointing triangle identified 172 instructional approaches that have been applied at one time or another to develop divergent thinking skills.  Some of the most convincing evidence that elements of creativity can be enhanced by instruction comes from work with young children. Bodrova and Leong (2001) blue right-pointing triangle developed the Tools of the Mind (Tools) curriculum to improve all of the three core mental executive functions involved in creative problem solving: cognitive flexibility, working memory, and inhibitory control. In a year-long randomized study of 5-yr-olds from low-income families in 21 preschool classrooms, half of the teachers applied the districts' balanced literacy curriculum (literacy), whereas the experimenters trained the other half to teach the same academic content by using the Tools curriculum (Diamond et al., 2007 blue right-pointing triangle). At the end of the year, when the children were tested with a battery of neurocognitive tests including a test for cognitive flexibility (Durston et al., 2003 blue right-pointing triangle; Davidson et al., 2006 blue right-pointing triangle), those exposed to the Tools curriculum outperformed the literacy children by as much as 25% (Diamond et al., 2007 blue right-pointing triangle). Although the Tools curriculum and literacy program were similar in academic content and in many other ways, they differed primarily in that Tools teachers spent 80% of their time explicitly reminding the children to think of alternative ways to solve a problem and building their executive function skills.  Teaching older students to be innovative also demands instruction that explicitly promotes creativity but is rigorously content-rich as well. A large body of research on the differences between novice and expert cognition indicates that creative thinking requires at least a minimal level of expertise and fluency within a knowledge domain (Bransford et al., 2000 blue right-pointing triangle; Crawford and Brophy, 2006 blue right-pointing triangle). What distinguishes experts from novices, in addition to their deeper knowledge of the subject, is their recognition of patterns in information, their ability to see relationships among disparate facts and concepts, and their capacity for organizing content into conceptual frameworks or schemata (Bransford et al., 2000 blue right-pointing triangle; Sawyer, 2005 blue right-pointing triangle).  Such expertise is often lacking in the traditional classroom. For students attempting to grapple with new subject matter, many kinds of problems that are presented in high school or college courses or that arise in the real world can be solved merely by applying newly learned algorithms or procedural knowledge. With practice, problem solving of this kind can become routine and is often considered to represent mastery of a subject, producing what Sternberg refers to as “pseudoexperts” (Sternberg, 2003 blue right-pointing triangle). But beyond such routine use of content knowledge the instructor's goal must be to produce students who have gained the HOCS needed to apply, analyze, synthesize, and evaluate knowledge (Crowe et al., 2008 blue right-pointing triangle). The aim is to produce students who know enough about a field to grasp meaningful patterns of information, who can readily retrieve relevant knowledge from memory, and who can apply such knowledge effectively to novel problems. This condition is referred to as adaptive expertise (Hatano and Ouro, 2003 blue right-pointing triangle; Schwartz et al., 2005 blue right-pointing triangle). Instead of applying already mastered procedures, adaptive experts are able to draw on their knowledge to invent or adapt strategies for solving unique or novel problems within a knowledge domain. They are also able, ideally, to transfer conceptual frameworks and schemata from one domain to another (e.g., Schwartz et al., 2005 blue right-pointing triangle). Such flexible, innovative application of knowledge is what results in inventive or creative solutions to problems (Crawford and Brophy, 2006 blue right-pointing triangle; Crawford, 2007 blue right-pointing triangle). Promoting Creative Problem Solving in the College Classroom  In most college courses, instructors teach science primarily through lectures and textbooks that are dominated by facts and algorithmic processing rather than by concepts, principles, and evidence-based ways of thinking. This is despite ample evidence that many students gain little new knowledge from traditional lectures (Hrepic et al., 2007 blue right-pointing triangle). Moreover, it is well documented that these methods engender passive learning rather than active engagement, boredom instead of intellectual excitement, and linear thinking rather than cognitive flexibility (e.g., Halpern and Hakel, 2003 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle; Perkins and Wieman, 2008 blue right-pointing triangle). Cognitive flexibility, as noted, is one of the three core mental executive functions involved in creative problem solving (Ausubel, 1963 blue right-pointing triangle, 2000 blue right-pointing triangle). The capacity to apply ideas creatively in new contexts, referred to as the ability to “transfer” knowledge (see Mestre, 2005 blue right-pointing triangle), requires that learners have opportunities to actively develop their own representations of information to convert it to a usable form. Especially when a knowledge domain is complex and fraught with ill-structured information, as in a typical introductory college biology course, instruction that emphasizes active-learning strategies is demonstrably more effective than traditional linear teaching in reducing failure rates and in promoting learning and transfer (e.g., Freeman et al., 2007 blue right-pointing triangle). Furthermore, there is already some evidence that inclusion of creativity training as part of a college curriculum can have positive effects. Hunsaker (2005) blue right-pointing triangle has reviewed a number of such studies. He cites work by McGregor (2001) blue right-pointing triangle, for example, showing that various creativity training programs including brainstorming and creative problem solving increase student scores on tests of creative-thinking abilities.  What explicit instructional strategies are available to promote creative problem solving? In addition to brainstorming, McFadzean (2002) blue right-pointing triangle discusses several “paradigm-stretching” techniques that can encourage creative ideas. One method, known as heuristic ideation, encourages participants to force together two unrelated concepts to discover novel relationships, a modern version of Koestler's bisociation (Koestler, 1964 blue right-pointing triangle). On the website of the Center for Development and Learning, Robert Sternberg and Wendy M. Williams offer 24 “tips” for teachers wishing to promote creativity in their students (Sternberg and Williams, 1998 blue right-pointing triangle). Among them, the following techniques might apply to a science classroom:      Model creativity—students develop creativity when instructors model creative thinking and inventiveness.     Repeatedly encourage idea generation—students need to be reminded to generate their own ideas and solutions in an environment free of criticism.     Cross-fertilize ideas—where possible, avoid teaching in subject-area boxes: a math box, a social studies box, etc; students' creative ideas and insights often result from learning to integrate material across subject areas.     Build self-efficacy—all students have the capacity to create and to experience the joy of having new ideas, but they must be helped to believe in their own capacity to be creative.     Constantly question assumptions—make questioning a part of the daily classroom exchange; it is more important for students to learn what questions to ask and how to ask them than to learn the answers.     Imagine other viewpoints—students broaden their perspectives by learning to reflect upon ideas and concepts from different points of view.  Although these strategies are all consistent with the knowledge about creativity that I have reviewed above, evidence from well-designed investigations to warrant the claim that they can enhance measurable indicators of creativity in college students is only recently beginning to materialize. If creativity most often occurs in “a mental state where attention is defocused, thought is associative, and a large number of mental representations are simultaneously activated” (Martindale, 1999 blue right-pointing triangle, p. 149), the question arises whether instructional strategies designed to enhance the HOCS also foster such a mental state? Do valid tests exist to show that creative problem solving can be enhanced by such instruction? How Is Creativity Related to Critical Thinking and the Higher-Order Cognitive Skills?  It is not uncommon to associate creativity and ingenuity with scientific reasoning (Sawyer, 2005 blue right-pointing triangle; 2006 blue right-pointing triangle). When instructors apply scientific teaching strategies (Handelsman et al., 2004 blue right-pointing triangle; DeHaan, 2005 blue right-pointing triangle; Wood, 2009 blue right-pointing triangle) by using instructional methods based on learning research, according to Ebert-May and Hodder (2008 blue right-pointing triangle), “we see students actively engaged in the thinking, creativity, rigor, and experimentation we associate with the practice of science—in much the same way we see students learn in the field and in laboratories” (p. 2). Perkins and Wieman (2008) blue right-pointing triangle note that “To be successful innovators in science and engineering, students must develop a deep conceptual understanding of the underlying science ideas, an ability to apply these ideas and concepts broadly in different contexts, and a vision to see their relevance and usefulness in real-world applications … An innovator is able to perceive and realize potential connections and opportunities better than others” (pp. 181–182). The results of Scott et al. (2004) blue right-pointing triangle suggest that nontraditional courses in science that are based on constructivist principles and that use strategies of scientific teaching to promote the HOCS and enhance content mastery and dexterity in scientific thinking (Handelsman et al., 2007 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle) also should be effective in promoting creativity and cognitive flexibility if students are explicitly guided to learn these skills.  Creativity is an essential element of problem solving (Mumford et al., 1991 blue right-pointing triangle; Runco, 2004 blue right-pointing triangle) and of critical thinking (Abrami et al., 2008 blue right-pointing triangle). As such, it is common to think of applications of creativity such as inventiveness and ingenuity among the HOCS as defined in Bloom's taxonomy (Crowe et al., 2008 blue right-pointing triangle). Thus, it should come as no surprise that creativity, like other elements of the HOCS, can be taught most effectively through inquiry-based instruction, informed by constructivist theory (Ausubel, 1963 blue right-pointing triangle, 2000 blue right-pointing triangle; Duch et al., 2001 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle). In a survey of 103 instructors who taught college courses that included creativity instruction, Bull et al. (1995) blue right-pointing triangle asked respondents to rate the importance of various course characteristics for enhancing student creativity. Items ranking high on the list were: providing a social climate in which students feels safe, an open classroom environment that promotes tolerance for ambiguity and independence, the use of humor, metaphorical thinking, and problem defining. Many of the responses emphasized the same strategies as those advanced to promote creative problem solving (e.g., Mumford et al., 1991 blue right-pointing triangle; McFadzean, 2002 blue right-pointing triangle; Treffinger and Isaksen, 2005 blue right-pointing triangle) and critical thinking (Abrami et al., 2008 blue right-pointing triangle).  In a careful meta-analysis, Scott et al. (2004) blue right-pointing triangle examined 70 instructional interventions designed to enhance and measure creative performance. The results were striking. Courses that stressed techniques such as critical thinking, convergent thinking, and constraint identification produced the largest positive effect sizes. More open techniques that provided less guidance in strategic approaches had less impact on the instructional outcomes. A striking finding was the effectiveness of being explicit; approaches that clearly informed students about the nature of creativity and offered clear strategies for creative thinking were most effective. Approaches such as social modeling, cooperative learning, and case-based (project-based) techniques that required the application of newly acquired knowledge were found to be positively correlated to high effect sizes. The most clear-cut result to emerge from the Scott et al. (2004) blue right-pointing triangle study was simply to confirm that creativity instruction can be highly successful in enhancing divergent thinking, problem solving, and imaginative performance. Most importantly, of the various cognitive processes examined, those linked to the generation of new ideas such as problem finding, conceptual combination, and idea generation showed the greatest improvement. The success of creativity instruction, the authors concluded, can be attributed to “developing and providing guidance concerning the application of requisite cognitive capacities … [and] a set of heuristics or strategies for working with already available knowledge” (p. 382).  Many of the scientific teaching practices that have been shown by research to foster content mastery and HOCS, and that are coming more widely into use, also would be consistent with promoting creativity. Wood (2009) blue right-pointing triangle has recently reviewed examples of such practices and how to apply them. These include relatively small modifications of the traditional lecture to engender more active learning, such as the use of concept tests and peer instruction (Mazur, 1996 blue right-pointing triangle), Just-in-Time-Teaching techniques (Novak et al., 1999 blue right-pointing triangle), and student response systems known as “clickers” (Knight and Wood, 2005 blue right-pointing triangle; Crossgrove and Curran, 2008 blue right-pointing triangle), all designed to allow the instructor to frequently and effortlessly elicit and respond to student thinking. Other strategies can transform the lecture hall into a workshop or studio classroom (Gaffney et al., 2008 blue right-pointing triangle) where the teaching curriculum may emphasize problem-based (also known as project-based or case-based) learning strategies (Duch et al., 2001 blue right-pointing triangle; Ebert-May and Hodder, 2008 blue right-pointing triangle) or “community-based inquiry” in which students engage in research that enhances their critical-thinking skills (Quitadamo et al., 2008 blue right-pointing triangle).  Another important approach that could readily subserve explicit creativity instruction is the use of computer-based interactive simulations, or “sims” (Perkins and Wieman, 2008 blue right-pointing triangle) to facilitate inquiry learning and effective, easy self-assessment. An example in the biological sciences would be Neurons in Action (http://neuronsinaction.com/home/main). In such educational environments, students gain conceptual understanding of scientific ideas through interactive engagement with materials (real or virtual), with each other, and with instructors. Following the tenets of scientific teaching, students are encouraged to pose and answer their own questions, to make sense of the materials, and to construct their own understanding. The question I pose here is whether an additional focus—guiding students to meet these challenges in a context that explicitly promotes creativity—would enhance learning and advance students' progress toward adaptive expertise? Assessment of Creativity  To teach creativity, there must be measurable indicators to judge how much students have gained from instruction. Educational programs intended to teach creativity became popular after the Torrance Tests of Creative Thinking (TTCT) was introduced in the 1960s (Torrance, 1974 blue right-pointing triangle). But it soon became apparent that there were major problems in devising tests for creativity, both because of the difficulty of defining the construct and because of the number and complexity of elements that underlie it. Tests of intelligence and other personality characteristics on creative individuals revealed a host of related traits such as verbal fluency, metaphorical thinking, flexible decision making, tolerance of ambiguity, willingness to take risks, autonomy, divergent thinking, self-confidence, problem finding, ideational fluency, and belief in oneself as being “creative” (Barron and Harrington, 1981 blue right-pointing triangle; Tardif and Sternberg, 1988 blue right-pointing triangle; Runco and Nemiro, 1994 blue right-pointing triangle; Snyder et al., 2004 blue right-pointing triangle). Many of these traits have been the focus of extensive research of recent decades, but, as noted above, creativity is not defined by any one trait; there is now reason to believe that it is the interplay among the cognitive and affective processes that underlie inventiveness and the ability to find novel solutions to a problem.  Although the early creativity researchers recognized that assessing divergent thinking as a measure of creativity required tests for other underlying capacities (Guilford, 1950 blue right-pointing triangle; Torrance, 1974 blue right-pointing triangle), these workers and their colleagues nonetheless believed that a high score for divergent thinking alone would correlate with real creative output. Unfortunately, no such correlation was shown (Barron and Harrington, 1981 blue right-pointing triangle). Results produced by many of the instruments initially designed to measure various aspects of creative thinking proved to be highly dependent on the test itself. A review of several hundred early studies showed that an individual's creativity score could be affected by simple test variables, for example, how the verbal pretest instructions were worded (Barron and Harrington, 1981 blue right-pointing triangle, pp. 442–443). Most scholars now agree that divergent thinking, as originally defined, was not an adequate measure of creativity. The process of creative thinking requires a complex combination of elements that include cognitive flexibility, memory control, inhibitory control, and analogical thinking, enabling the mind to free-range and analogize, as well as to focus and test.  More recently, numerous psychometric measures have been developed and empirically tested (see Plucker and Renzulli, 1999 blue right-pointing triangle) that allow more reliable and valid assessment of specific aspects of creativity. For example, the creativity quotient devised by Snyder et al. (2004) blue right-pointing triangle tests the ability of individuals to link different ideas and different categories of ideas into a novel synthesis. The Wallach–Kogan creativity test (Wallach and Kogan, 1965 blue right-pointing triangle) explores the uniqueness of ideas associated with a stimulus. For a more complete list and discussion, see the Creativity Tests website (www.indiana.edu/∼bobweb/Handout/cretv_6.html).  The most widely used measure of creativity is the TTCT, which has been modified four times since its original version in 1966 to take into account subsequent research. The TTCT-Verbal and the TTCT-Figural are two versions (Torrance, 1998 blue right-pointing triangle; see http://ststesting.com/2005giftttct.html). The TTCT-Verbal consists of five tasks; the “stimulus” for each task is a picture to which the test-taker responds briefly in writing. A sample task that can be viewed from the TTCT Demonstrator website asks, “Suppose that people could transport themselves from place to place with just a wink of the eye or a twitch of the nose. What might be some things that would happen as a result? You have 3 min.” (www.indiana.edu/∼bobweb/Handout/d3.ttct.htm).  In the TTCT-Figural, participants are asked to construct a picture from a stimulus in the form of a partial line drawing given on the test sheet (see example below; Figure 1). Specific instructions are to “Add lines to the incomplete figures below to make pictures out of them. Try to tell complete stories with your pictures. Give your pictures titles. You have 3 min.” In the introductory materials, test-takers are urged to “… think of a picture or object that no one else will think of. Try to make it tell as complete and as interesting a story as you can …” (Torrance et al., 2008 blue right-pointing triangle, p. 2). Figure 1. Figure 1. Sample figural test item from the TTCT Demonstrator website (www.indiana.edu/∼bobweb/Handout/d3.ttct.htm).  How would an instructor in a biology course judge the creativity of students' responses to such an item? To assist in this task, the TTCT has scoring and norming guides (Torrance, 1998 blue right-pointing triangle; Torrance et al., 2008 blue right-pointing triangle) with numerous samples and responses representing different levels of creativity. The guides show sample evaluations based upon specific indicators such as fluency, originality, elaboration (or complexity), unusual visualization, extending or breaking boundaries, humor, and imagery. These examples are easy to use and provide a high degree of validity and generalizability to the tests. The TTCT has been more intensively researched and analyzed than any other creativity instrument, and the norming samples have longitudinal validations and high predictive validity over a wide age range. In addition to global creativity scores, the TTCT is designed to provide outcome measures in various domains and thematic areas to allow for more insightful analysis (Kaufman and Baer, 2006 blue right-pointing triangle). Kim (2006) blue right-pointing triangle has examined the characteristics of the TTCT, including norms, reliability, and validity, and concludes that the test is an accurate measure of creativity. When properly used, it has been shown to be fair in terms of gender, race, community status, and language background. According to Kim (2006) blue right-pointing triangle and other authorities in the field (McIntyre et al., 2003 blue right-pointing triangle; Scott et al., 2004 blue right-pointing triangle), Torrance's research and the development of the TTCT have provided groundwork for the idea that creative levels can be measured and then increased through instruction and practice. SCIENTIFIC TEACHING TO PROMOTE CREATIVITY How Could Creativity Instruction Be Integrated into Scientific Teaching?  Guidelines for designing specific course units that emphasize HOCS by using strategies of scientific teaching are now available from the current literature. As an example, Karen Cloud-Hansen and colleagues (Cloud-Hansen et al., 2008 blue right-pointing triangle) describe a course titled, “Ciprofloxacin Resistance in Neisseria gonorrhoeae.” They developed this undergraduate seminar to introduce college freshmen to important concepts in biology within a real-world context and to increase their content knowledge and critical-thinking skills. The centerpiece of the unit is a case study in which teams of students are challenged to take the role of a director of a local public health clinic. One of the county commissioners overseeing the clinic is an epidemiologist who wants to know “how you plan to address the emergence of ciprofloxacin resistance in Neisseria gonorrhoeae” (p. 304). State budget cuts limit availability of expensive antibiotics and some laboratory tests to patients. Student teams are challenged to 1) develop a plan to address the medical, economic, and political questions such a clinic director would face in dealing with ciprofloxacin-resistant N. gonorrhoeae; 2) provide scientific data to support their conclusions; and 3) describe their clinic plan in a one- to two-page referenced written report.  Throughout the 3-wk unit, in accordance with the principles of problem-based instruction (Duch et al., 2001 blue right-pointing triangle), course instructors encourage students to seek, interpret, and synthesize their own information to the extent possible. Students have access to a variety of instructional formats, and active-learning experiences are incorporated throughout the unit. These activities are interspersed among minilectures and give the students opportunities to apply new information to their existing base of knowledge. The active-learning activities emphasize the key concepts of the minilectures and directly confront common misconceptions about antibiotic resistance, gene expression, and evolution. Weekly classes include question/answer/discussion sessions to address student misconceptions and 20-min minilectures on such topics as antibiotic resistance, evolution, and the central dogma of molecular biology. Students gather information about antibiotic resistance in N. gonorrhoeae, epidemiology of gonorrhea, and treatment options for the disease, and each team is expected to formulate a plan to address ciprofloxacin resistance in N. gonorrhoeae.  In this project, the authors assessed student gains in terms of content knowledge regarding topics covered such as the role of evolution in antibiotic resistance, mechanisms of gene expression, and the role of oncogenes in human disease. They also measured HOCS as gains in problem solving, according to a rubric that assessed self-reported abilities to communicate ideas logically, solve difficult problems about microbiology, propose hypotheses, analyze data, and draw conclusions. Comparing the pre- and posttests, students reported significant learning of scientific content. Among the thinking skill categories, students demonstrated measurable gains in their ability to solve problems about microbiology but the unit seemed to have little impact on their more general perceived problem-solving skills (Cloud-Hansen et al., 2008 blue right-pointing triangle).  What would such a class look like with the addition of explicit creativity-promoting approaches? Would the gains in problem-solving abilities have been greater if during the minilectures and other activities, students had been introduced explicitly to elements of creative thinking from the Sternberg and Williams (1998) blue right-pointing triangle list described above? Would the students have reported greater gains if their instructors had encouraged idea generation with weekly brainstorming sessions; if they had reminded students to cross-fertilize ideas by integrating material across subject areas; built self-efficacy by helping students believe in their own capacity to be creative; helped students question their own assumptions; and encouraged students to imagine other viewpoints and possibilities? Of most relevance, could the authors have been more explicit in assessing the originality of the student plans? In an experiment that required college students to develop plans of a different, but comparable, type, Osborn and Mumford (2006) blue right-pointing triangle created an originality rubric (Figure 2) that could apply equally to assist instructors in judging student plans in any course. With such modifications, would student gains in problem-solving abilities or other HOCS have been greater? Would their plans have been measurably more imaginative? Figure 2. Figure 2. Originality rubric (adapted from Osburn and Mumford, 2006 blue right-pointing triangle, p. 183).  Answers to these questions can only be obtained when a course like that described by Cloud-Hansen et al. (2008) blue right-pointing triangle is taught with explicit instruction in creativity of the type I described above. But, such answers could be based upon more than subjective impressions of the course instructors. For example, students could be pretested with items from the TTCT-Verbal or TTCT-Figural like those shown. If, during minilectures and at every contact with instructors, students were repeatedly reminded and shown how to be as creative as possible, to integrate material across subject areas, to question their own assumptions and imagine other viewpoints and possibilities, would their scores on TTCT posttest items improve? Would the plans they formulated to address ciprofloxacin resistance become more imaginative?  Recall that in their meta-analysis, Scott et al. (2004) blue right-pointing triangle found that explicitly informing students about the nature of creativity and offering strategies for creative thinking were the most effective components of instruction. From their careful examination of 70 experimental studies, they concluded that approaches such as social modeling, cooperative learning, and case-based (project-based) techniques that required the application of newly acquired knowledge were positively correlated with high effect sizes. The study was clear in confirming that explicit creativity instruction can be successful in enhancing divergent thinking and problem solving. Would the same strategies work for courses in ecology and environmental biology, as detailed by Ebert-May and Hodder (2008) blue right-pointing triangle, or for a unit elaborated by Knight and Wood (2005) blue right-pointing triangle that applies classroom response clickers?  Finally, I return to my opening question with the fictional Dr. Dunne. Could a weekly brainstorming “invention session” included in a course like those described here serve as the site where students are introduced to concepts and strategies of creative problem solving? As frequently applied in schools of engineering (Paulus and Nijstad, 2003 blue right-pointing triangle), brainstorming provides an opportunity for the instructor to pose a problem and to ask the students to suggest as many solutions as possible in a brief period, thus enhancing ideational fluency. Here, students can be encouraged explicitly to build on the ideas of others and to think flexibly. Would brainstorming enhance students' divergent thinking or creative abilities as measured by TTCT items or an originality rubric? Many studies have demonstrated that group interactions such as brainstorming, under the right conditions, can indeed enhance creativity (Paulus and Nijstad, 2003 blue right-pointing triangle; Scott et al., 2004 blue right-pointing triangle), but there is little information from an undergraduate science classroom setting. Intellectual Ventures, a firm founded by Nathan Myhrvold, the creator of Microsoft's Research Division, has gathered groups of engineers and scientists around a table for day-long sessions to brainstorm about a prearranged topic. Here, the method seems to work. Since it was founded in 2000, Intellectual Ventures has filed hundreds of patent applications in more than 30 technology areas, applying the “invention session” strategy (Gladwell, 2008 blue right-pointing triangle). Currently, the company ranks among the top 50 worldwide in number of patent applications filed annually. Whether such a technique could be applied successfully in a college science course will only be revealed by future research.", "category": "Edison", "id": 112}
{"skillName": "DSRMP02", "skillText": "Systematic Study is a special way to learn school subjects. It is both easy and successful. If you study systematically your grades will improve and you might actually spend less time studying. We think that Systematic Study is smart study. Q: Why should you use Systematic Study?  You should study systematically so you can make good choices about how and when you study. Most people just study without thinking about how they will learn. This may mean they read their textbooks or go over notes they took in class. These students often just close their books or notes when they have finished reading or reviewing.      This is usually a poor way to study for several reasons:          Reading or going over notes may not be the best way to study the material.         No specific study activity was chosen. The students just did the first activity (reading or going over notes) that came to their minds.         No attempt was made to see what they learned. These students did not test themselves.         No plan to be successful was made.  With Systematic Study you will learn to choose and plan how you study. This will make learning easier and faster. Q: Why do you need to learn to study?  We can't say that you do. But we do know that most students can learn to study better. Learning in school can be both difficult and time consuming. No one is born knowing how to do her or his best in school. We learn to study by listening to teachers and our parents and watching others. But, study is private; it happens in our minds. So we can't see exactly what others do. Most of us do what seems to be best and get into the habit of always studying one way.  Psychologists who study school learning have discovered that most students could learn more in less time if they changed how they study. This means using some new study actions and different attitudes during study. The purpose of STUDY SMART is to help you learn these study actions and attitudes.  You can think of learning to study better as similar to a carpenter with a tool box or a cook with cooking utensils. If they had just one tool - a screwdriver or a mixing spoon - their chances of success would be limited. However, if both had a complete set of tools or utensils, they would be more likely to succeed.  The same is true for studying. If students have only one way to study, they will have less chance to succeed than if they have many. In STUDY SMART you will learn many ways to study so, like the good carpenter, you will have a full box of study tools.     Q: What does it take to improve study?  Improving any skill or ability takes some work and desire. The same is true with study. While we can show you more effective study skills, no one can study and learn for you. As you know, a mixing bowl does a cook no good unless it is used. The same is true of these skills. You will have to use them in your study.  Sometimes using new study skills may not be easy and you may find some study ideas don't work right away. If this happens, do not give up. New ideas and actions take time to learn and get used to. All the ideas, skills, activities, and attitudes presented in STUDY SMART do work.  You must also be willing to change what you do when you study. This will take some effort and some time. But, you will get big rewards in the future when you learn more in less time and remember it better. So, practice the skills and ideas you learn in STUDY SMART. You will find study will be easier and more successful. Q: What will you learn about study?  First, you will learn a Systematic Study approach. This will help you think about study differently. You will use this approach to choose study skills and study times and to make study plans. We call this study approach PAT. PAT stands for Prepare to study, Act to learn, and Test yourself.  Second, you will learn how to use your abilities more effectively. We think all students can do well in school if they use their abilities well. We will help you read faster and remember more, take good notes, listen better in class so you can hear test questions, improve your memory for facts and ideas, organize your time, and write better papers.  STUDY SMART has 24 lessons. You should do one or two lessons each week. After you work through a lesson, you should apply the ideas to your study for at least one week using the practice suggestions in each lesson.  In this way you will continue to add new ways to study but also master each new skill without feeling overwhelmed. If you have problems or questions, talk to your teacher or counselor. Most people do have questions, so don't feel like you are having special problems if all these ideas don't work right away. Q: Where do these STUDY SMART ideas come from?  We have developed these ideas from talking to and working with successful students. These are people like you who make very good grades in school.  They may be different from you because they appear to learn very quickly and effortlessly. Some people believe these students do well because they are especially smart or have very high intelligence scores. But, this usually is not the reason students are successful in school.  The main difference between successful and less successful students is how and why they study. If you study like more successful students, you can be just as successful. STUDY SMART will show you how to study like the most successful students. Q: How do you get started?  Begin by changing how you think about study. This will not be easy; but, it is a very important part of changing and improving your study.  First, we want you to learn PAT, the components of Systematic Study.  Prepare: This is an important part of study just as it is for athletes, travelers, and people in all occupations.  Athletes Prepare by stretching and warming up their muscles before they compete. They also make a game plan or strategy to use their skills. The same goes for travelers who choose a route to follow, buy tickets, make reservations, etc.  The best students Prepare when they study. They think about what they need to learn, they choose how to study, they select where they will study, and they decide how to determine if they were successful. Preparing is the first part of study; you will learn how to Prepare for class and home study.  Think about how you usually begin your study. What do you do? Open a book and begin reading? Grumble to yourself about having to study? Turn the radio to your favorite station? Find your favorite program on TV? Lay down on your bed?    Act: This part of study involves using the best skills to achieve the results you desire. When you are finished with STUDY SMART, you will have many skills or study tools to choose from. Developing many skills is important. Most good students use many skills because they choose the best skills for the study task which they have been assigned.  Now, think about how you choose to study. What do you do? The first thing you think of? The only thing you know? Or, do you consider several different ways to learn, memorize and understand?  Test: This is the part of study where you Test yourself to see if you learned successfully. It is interesting that good students rarely rely only on teachers to Test them. They Test themselves every time they finish studying.  Now, think about how you finish studying. What do you do? Close the book and heave a sigh of relief? Be grateful you don't have to read another chapter? Fall asleep? Listen to a favorite song? Turn on the TV? Call a friend on the phone?  Write a description of how you typically end your study. Q: How do you begin to improve your study?  Being a better student requires change. As you work through STUDY SMART you will learn many ways to study differently. You may already have some ideas of how you can change your study to be better. The best place to begin is to look carefully at what you are doing now. For the next week, keep a diary using Form 1.1 of what you do while you study.      Include:          When you start and stop (time and date).         What you study.         What you did to Prepare.         What Actions you took to learn.         What you did to Test yourself.  After about a week, identify the best things you did and the things you think you should change while you study. These should include skills such as reading and note taking, attitudes such as \"disorganized\" and \"unsystematic\", mental approaches such as \"bored\" and \"not interested\", and physical states such as \"tired\" and \"sore from athletic practice.\"  A systematic review is a type of literature review that collects and critically analyzes multiple research studies or papers. A review of existing studies is often quicker and cheaper than embarking on a new study. Researchers use methods that are selected before one or more research questions are formulated, and then they aim to find and analyze studies that relate to and answer those questions.[1] Systematic reviews of randomized controlled trials are key in the practice of evidence-based medicine.[2]  An understanding of systematic reviews, and how to implement them in practice, is highly recommended for professionals involved in the delivery of health care. Besides health interventions, systematic reviews may examine clinical tests, public health interventions, environmental interventions,[3] social interventions, adverse effects, and economic evaluations.[4][5] Systematic reviews are not limited to medicine and are quite common in all other sciences where data are collected, published in the literature, and an assessment of methodological quality for a precisely defined subject would be helpful.[6]  Contents      1 Characteristics     2 Stages of a systematic review     3 Cochrane Collaboration     4 Strengths and weaknesses     5 See also     6 References     7 External links  Characteristics  A systematic review aims to provide a complete, exhaustive summary of current literature relevant to a research question. The first step in conducting a systematic review is to perform a thorough search of the literature for relevant papers. The Methodology section of a systematic review will list all of the databases and citation indexes that were searched such as Web of Science, Embase, and PubMed and any individual journals that were searched. The titles and abstracts of identified articles are checked against pre-determined criteria for eligibility and relevance to form an inclusion set. This set will relate back to the research problem. Each included study may be assigned an objective assessment of methodological quality preferably by using methods conforming to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement (the current guideline)[7] or the high quality standards of Cochrane collaboration.[8]  Systematic reviews often, but not always, use statistical techniques (meta-analysis) to combine results of eligible studies, or at least use scoring of the levels of evidence depending on the methodology used. An additional rater may be consulted to resolve any scoring differences between raters.[6] Systematic review is often applied in the biomedical or healthcare context, but it can be applied in any field of research. Groups like the Campbell Collaboration are promoting the use of systematic reviews in policy-making beyond just healthcare.  A systematic review uses an objective and transparent approach for research synthesis, with the aim of minimizing bias. While many systematic reviews are based on an explicit quantitative meta-analysis of available data, there are also qualitative reviews which adhere to standards for gathering, analyzing and reporting evidence.[9] The EPPI-Centre has been influential in developing methods for combining both qualitative and quantitative research in systematic reviews.[10] The PRISMA statement[11] suggests a standardized way to ensure a transparent and complete reporting of systematic reviews, and is now required for this kind of research by more than 170 medical journals worldwide.[12]  Recent developments in systematic reviews include realist reviews,[13] and the meta-narrative approach.[14][15] These approaches try to overcome the problems of methodological and epistemological heterogeneity in the diverse literatures existing on some subjects. Stages of a systematic review  The main stages of a systematic review are: A visualisation of data being 'extracted' and 'combined' in a Cochrane systematic review.[16]      Defining a question and agreeing an objective method.[16]     A search for relevant data from research that matches certain criteria. For example, only selecting research that is good quality and answers the defined question.[16]     'Extraction' of relevant data. This can include how the research was done (often called the method or 'intervention'), who participated in the research (including how many people), how it was paid for (for example funding sources) and what happened (the outcomes).[16]     Assess the quality of the data by judging it against criteria identified at the first stage.[16]     Analyse and combine the data (using complex statistical methods) which give an overall result from all of the data. This combination of data can be visualised using a blobbogram (also called a forest plot).[16] The diamond in the blobbogram represents the combined results of all the data included. Because this combined result uses data from more sources than just one data set, it’s considered more reliable and better evidence, as the more data there is, the more confident we can be of conclusions.[16]  Once these stages are complete, the review may be published, disseminated and translated into practice after being adopted as evidence. Cochrane Collaboration  The Cochrane Collaboration is a group of over 31,000 specialists in healthcare who systematically review randomised trials of the effects of prevention, treatments and rehabilitation as well as health systems interventions. When appropriate, they also include the results of other types of research. Cochrane Reviews are published in The Cochrane Database of Systematic Reviews section of the Cochrane Library. The 2015 impact factor for The Cochrane Database of Systematic Reviews was 6.103, and it was ranked 12th in the “Medicine, General & Internal” category.[17] There are six types of Cochrane Review:[18][19][20][21]      Intervention reviews assess the benefits and harms of interventions used in healthcare and health policy.     Diagnostic test accuracy reviews assess how well a diagnostic test performs in diagnosing and detecting a particular disease.     Methodology reviews address issues relevant to how systematic reviews and clinical trials are conducted and reported.     Qualitative reviews synthesize qualitative and quantitative evidence to address questions on aspects other than effectiveness.[9]     Prognosis reviews address the probable course or future outcome(s) of people with a health problem.     Overviews of Systematic Reviews (OoRs) are a new type of study in order to compile multiple evidence from systematic reviews into a single document that is accessible and useful to serve as a friendly front end for the Cochrane Collaboration with regard to healthcare decision-making.  The Cochrane Collaboration provides a handbook for systematic reviewers of interventions which \"provides guidance to authors for the preparation of Cochrane Intervention reviews.\"[8] The Cochrane Handbook outlines eight general steps for preparing a systematic review:[8]      Defining the review question(s) and developing criteria for including studies     Searching for studies     Selecting studies and collecting data     Assessing risk of bias in included studies     Analysing data and undertaking meta-analyses     Addressing reporting biases     Presenting results and \"summary of findings\" tables     Interpreting results and drawing conclusions  The Cochrane Handbook forms the basis of two sets of standards for the conduct and reporting of Cochrane Intervention Reviews (MECIR - Methodological Expectations of Cochrane Intervention Reviews)[22]  The Cochrane Collaboration logo visually represents how results from some systematic reviews can be explained.[23] The lines within illustrate the summary results from an iconic systematic review showing the benefit of corticosteroids, which 'has probably saved thousands of premature babies'.[24] Strengths and weaknesses  While systematic reviews are regarded as the strongest form of medical evidence, a review of 300 studies found that not all systematic reviews were equally reliable, and that their reporting can be improved by a universally agreed upon set of standards and guidelines.[25] A further study by the same group found that of 100 systematic reviews monitored, 7% needed updating at the time of publication, another 4% within a year, and another 11% within 2 years; this figure was higher in rapidly changing fields of medicine, especially cardiovascular medicine.[26] A 2003 study suggested that extending searches beyond major databases, perhaps into grey literature, would increase the effectiveness of reviews.[27]  Roberts and colleagues highlighted the problems with systematic reviews, particularly those conducted by the Cochrane Collaboration, noting that published reviews are often biased, out of date and excessively long.[28] They criticized Cochrane reviews as not being sufficiently critical in the selection of trials and including too many of low quality. They proposed several solutions, including limiting studies in meta-analyses and reviews to registered clinical trials, requiring that original data be made available for statistical checking, paying greater attention to sample size estimates, and eliminating dependence on only published data.  Some of these difficulties were noted early on as described by Altman: \"much poor research arises because researchers feel compelled for career reasons to carry out research that they are ill equipped to perform, and nobody stops them.\"[29] Methodological limitations of meta-analysis have also been noted.[30] Another concern is that the methods used to conduct a systematic review are sometimes changed once researchers see the available trials they are going to include.[31] Bloggers have described retractions of systematic reviews and published reports of studies included in published systematic reviews.[32][33][34]  Systematic reviews are increasingly prevalent in other fields, such as international development research.[35] Subsequently, a number of donors – most notably the UK Department for International Development (DFID) and AusAid – are focusing more attention and resources on testing the appropriateness of systematic reviews in assessing the impacts of development and humanitarian interventions.[35] See also      Critical appraisal     Literature review     Peer review     Review journal", "category": "Edison", "id": 113}
{"skillName": "DSDM06", "skillText": "Maximising Contribution: Archiving data   Archiving data for secondary analysis   Seen as ethical practice   Ability to validate and refine or refute published  findings using publically available data (Albright  & Lyle, 2010).   May ensure greater transparency of data  ( Arrison  et al., 2009;  Carusi  &  Jirotka , 2009).    Reduces burden of repeat data collection on  particpants  ( Corti  et al., 2000).   Ongoing debate regarding archiving  qualitative data ...   Less debate in the literature regarding  quantitative archiving   “Preparing and preserving all data for sharing is  perhaps even more wasteful of scientific and social  resources as destroying all data after their first use.  The ethical dilemma, then, is to discover the most  intelligent course through a thicket of ever-shifting  circumstances surrounding the preparation, storage,  and ultimate secondary usefulness of data” ( Sieber ,  2005, p.165).  Respecting Privacy   Quantitative data assumed to be less identifiable   Albright and Lyle (2010)    Concerns regarding impact of technology on the  potential to identify participants in quantitative  research    Masking of potentially identifiable data though   Data swapping,    Microaggregation,    Adding random observations Awareness   Are researchers aware of the benefits and  challenges of archiving data?   Relatively recent phenomenon for many disciplines   Good vs best practice?   Changing legislative framework  Issues ...   When is an archive not an archive?   Distinction between formal managed archives  and researchers holding data for personal use.   Ethical issues in personal archives (who  manages them)   Must ensure data quality   “Each set of data an investigator obtains is a  slice of reality that may or may not be valid or  reliable” ( Sieber , 2005, p.166)  What are our ethical obligations when sharing research data?  Ethics dictionary entryWhen sharing or archiving confidential data, you have a duty:‌      of confidentiality towards participants.     to protect participants from harm by not disclosing personal and sensitive information.     to treat participants as intelligent beings, able to make their own decisions on how the information they provide is used, shared or made.     to inform participants how information and data will be used and shared.     to the wider society to make available resources produced by researchers.  When collecting personal or sensitive data, you should consider:      Do you really need to collect them or are these being collected for administrative purposes only?     That you should inform your participants about the use of personal data.     Not all research obtained from participants constitute personal data.  Data protection and research data     If you handle personal information about individuals, you have a number of legal obligations to protect that information under the Data Protection Act 1998.  The Data Protection Act (1998) gives individuals certain rights, and imposes obligations on those who record and use personal information to be open about how information is used.  There are eight data protection principles concerning how personal data should be managed:      processed fairly and lawfully     obtained for specified and lawful purposes     adequate, relevant and not excessive     accurate and, where necessary, kept up-to-date     not kept for longer than necessary     processed in accordance with the subject's rights     kept secure     not transferred abroad without adequate protection  More Information      The Data Protection Act (1998)     University Data Protection and Data Protection policy     Data Protection: UK Data Archive     Data protection, rights and access: MANTRA (University of Edinburgh)  ... and our legal obligations?  There are a number of different laws and guidelines surrounding research data such as the Data Protection Act, Freedom of Information Act, etc. and these must be adhered to when managing or sharing personal or sensitive research data. When collecting personal or sensitive data, you should consider:      Do you really need to collect them or are these being collected for administrative purposes only?     That you should inform your participants about the use of personal data.     Not all research obtained from participants constitute personal data.     Will you be able to supply information requested under Freedom of Information (FoI) legislation within 20 days (unless there is an exemption that allows the University not to disclose it (see below)?  Freedom of Information and research data     The Freedom of Information Act 2000 (FOI) gives the public a general right of access to information a public authority (in this case the University) holds and places an obligation on the authority to provide the information which has been requested, subject to any valid exemptions. Your research data may be covered by the terms of the Act. Most requests you receive are part of routine business as usual.  Researchers share or trade data all the time, so if you normally agree to share data, you should continue to do so; paying due care to normal research considerations; e.g. ethics, privacy and confidentiality. However, in circumstances where you don't want to supply the data, or you think there are legal or ethical reasons why you shouldn't supply it, or the request is specifically identified as a FoI request, you should consult the University Freedom of Information Officer.  The legislation requires the University to supply information, or a refusal notice within 20 working days from receipt of the request.  NB: requests may come to any member of the University; not just, say, the FoI officer.  More information      Freedom of Information Act (2000)     Freedom of Information (Scotland) Act 2002     University of St Andrews Freedom of Information (FoI) guidance     FoI and Research Data Q&A: JISC     Freedom of Information: UK Data Archive     FoI - specialist guidance for datasets: Information Commissioner's Office (ICO)  Environmental information     You may additionally receive requests for data under Environmental Information Regulations where applicable, which exist to provide public access to environmental information held by public authorities. Rules for requests of environmental data are similar to regular FOI requests; however, in compliance with European requirements, publicly-funded bodies have greater responsibility to provide requested environmental data, so it is harder to meet the criteria for an exception (i.e. harder to avoid providing data).  Both FoI and EIR include a number of exemptions and exceptions to protect information such as confidentiality or sensitive data or financially valuable information. If you are concerned, consult the University's FoI Officer. Research data and copyright     Copyright is an intellectual property right, assigned automatically to the creator, that prevents unauthorised copying and publishing of an original work. Copyright applies to research data and plays a role when creating, sharing and reusing data. More information on copyright and how it applies to research data can be found at the UK Data Service website. Does my data have any unrealised commercial value?  The University's Knowledge Transfer Centre (KTC) can help you determine the value or your research, in this case data, and identify any patent potentiality. The KTC plays a substantial role in assisting you as researchers exploit opportunities for the transfer of know-how, data, results and IP to business and industry and in the University, achieving economic impact from its research activities. The management of confidential, sensitive and/or personal data has ethical as well as legal implications. The core principle underpinning the University's Code of practice and principles for good ethical governance is that of avoidance of harm. This includes harm to the welfare and interests of human participants (whether participating actively or through observation/use of their data) and harm to the welfare and interests of the wider community. Two key considerations when managing your research data are:      To ensure the security of confidential, sensitive and/or personal data, including access control. For more information see our Storing your data securely web page.     To ensure that appropriate consent is in place for the use or reuse of human data.  Further detail can be found in the Code of practice and principles for good ethical governance.      For further guidance on the ethics of research data management, speak to a member of your departmental/subject level ethics committee in the first instance.  All research undertaken in the University's name or on its behalf which falls within the University's framework of ethical principles, regardless of funding source, should undergo review by a University ethics sub-committee, and have been signed off by the sub-committee before the research commences, even where ethical scrutiny is also undertaken by an outside body.  The review process will include addressing the above considerations where they apply. Details of the University's procedures for conducting ethical review are set out in the Code.  Your funder and/or your professional body may also have requirements and/or guidance relating to the ethical management of research data, which you will need to take into account when addressing the above considerations.  Many research funders now require the researchers they fund to share and preserve their research data. Writing a Data Management Plan at the start of your research project will help you to consider these issues and how you might produce a version of your data that you can share. University guidance on Data Protection, FOI and IPR  Click on the one of the options below to read the University's guidance on Data Protection, Freedom of Information or Intellectual Property Rights. Data Protection  The Data Protection Act 1998 establishes key principles that govern the collection, use and handling of personal information and provides individuals with important rights.      Data Protection information on data protection provided by University Records Manager.      The Data Protection Act: University policy, procedures and guidelines summarises the implications of the Data Protection Act for the University, sets out the University's general policy on adherence to the Act (e.g. procuring, storing and the disposing of personal data), and offers specific guidance relating to research involving data from human participants.      Records management guides a series of guides, with tips and examples, to help support the maintenance and good management of records. Data Security (PDF , 255kb) offers guidance on data acquisition, storage, data retention and the disposal of personal and sensitive data to meet ethical and legal requirements.  For help with data protection compliance, contact dataprotection@york.ac.uk      Data Protection: Online a short online training session covering data protection, freedom of information and records management. For the University to meet its legal obligations all members of staff should complete this module.  For help with data protection compliance, contact dataprotection@york.ac.uk Freedom of Information  The Freedom of Information Act and Environmental Information Regulations provide members of the public with a general right of access to the recorded information held by the University. The legislation works to promote openness across the public sector. So you could be required to release information about your research based on FOI and EIR requests.      Freedom of Information information on FOI provided by the University Records Manager. It includes the University’s Freedom of Information Policy and guidance to staff on enquiry handling.      Freedom of information and research guidance to staff relating to research data and publications. Included in this information is a link to JISC's FOI and research data: questions and answers, a useful FAQs on how researchers should respond if faced with FOI or EIR requests.      Data Protection: Online a short online training session covering freedom of information, data protection and records management. In order for the University to meet its legal obligations all members of staff should complete this module.  For FOI advice, contact records-manager@york.ac.uk Intellectual Property Rights  Intellectual Property Rights (IPR), for example copyright or patents, affect and may limit the way you and others can use the outputs of your research. It's therefore important to clarify copyright and intellectual property ownership of any data that you will create or use before your research begins.  If you agree or purchase licences to reuse third party data, be aware of any restrictions this places on what the data can be used for in the future. You should read carefully the terms and conditions associated with the use of third party data as its probable that copyright and/or licensing issues are associated.  Research funders may expect you to clarify IPR ownership in your Data Management Plan.      Intellectual property guidance provided by the University Intellectual Property Manager.      University policy on intellectual property outlines the ownership of IP (including copyright) for research materials and procedures for commercialisation.      Copyright guidance that includes information on copyright exceptions and how to seek the permission of the copyright holder before reusing their work in your research. For further information and guidance about copyright compliance, contact lib-copyright-advice@york.ac.uk.  For advice on IPR, contact the Intellectual Property Manager sue.final@york.ac.uk Recommended ethical and legal guidance from other organisations      RCUK Guidance on best practice in the management of research data [PDf]     RCUK recognises that there are legal, ethical and commercial constraints on release of research data. This document provides useful guidance on RCUK's expectations in relation to legal, ethical and commercial constraints.      UK Data Archive Copyright     Information on the implications of copyright ownership when creating, sharing and reusing data. The pages include example cases illustrating where issues may arise.      UK Data Archive Consent and ethics     Outlines strategies that you can utilise to allow you to share your personal and sensitive data, and meet your funder expectations, whilst protecting data integrity. Consult with your departmental/subject level ethics committee if you are unsure whether your data can be shared or published.      Australian National Data Service Publishing and sharing sensitive data     Step-by-step advice on what you need to know and do before publishing and sharing your sensitive data, in the Australian context.     PrePARe Project  Is it yours? Dealing with copyright & other intellectual property rights     A checklist to help you think about the steps to take to protect your own IPR and to respect the copyright in material created by someone else when you publish, share and deposit research material.      DCC How to license research data     A guide to help you decide how to apply a licence to your research data, and which licence would be most suitable.      JISC Legal     Offers guidance on legal issues that affect your data.      Medical Research Council Tool kits     The Data and tissues tool kit suggests practical ways to meet legal and ethical requirements relating to the use of personal information (and human tissue samples) in healthcare research.      MRC Regulatory Support Centre Research data and confidentiality     This e-learning module explores the concepts of confidentiality and data protection. It is aimed at all researchers working with data.      University of Edinburgh, Research Data MANTRA Data protection, rights and access     A training module from the MANTRA research data management online course. This module covers ethical obligations, data confidentiality and disclosure, data protection and anonymisation.  Intellectual property rights (IPR) management is an important part of any data management program. A builder of a database or other data resource will have an interest in who owns that resource and how others may use it. Someone who may populate that resource with data provided in part by others will want to make sure that all legal, ethical, and professional obligations that one may have to the provider of the data are met. Since the benefits of data sharing are so well known and documented, a researcher may wish to share their database and/or content with others. Others can only fully utilize external data if they know the terms of use (if any) for that data. This fact sheet provides a brief overview of some of the issues associated with managing IPR in data projects.      Data versus database     Data licensing     Data ownership at Cornell     Related information  Data versus database  In any data project, there are likely to be two components. The first is the data collected, assembled, or generated. Think of it as the raw content in the system. It could be hourly temperature readings from a sensor, the age of individuals in a survey, recordings of individual voices, or photographs of plant specimens. The second component is the data system in which the data is stored and managed.  We usually do not think of data content separate from the system in which it is stored, but the distinction is important in terms of intellectual property rights. The question is what, if anything, is protected by copyright. Data that is factual has no copyright protection under U.S. law; it is not possible to copyright facts. Not all data is in the public domain. A project might, for example, use copyrighted photographs; the photographs are part of the project’s “data.” In many cases, the data in a data management system as well as the metadata describing that data will be factual, and hence not protected by copyright.  A database, on the other hand, can have a thin layer of copyright protection. Deciding what data needs to be included in a database, how to organize the data, and how to relate different data elements are all creative decisions that may receive copyright protection.  Because of the different copyright status of databases and data content, different mechanisms are required to manage each. Copyright can govern the use of databases and some data content (that which is itself original), but contract law, trademarks, and other mechanisms are required to regulate factual data. Data licensing  In order to facilitate the reuse of data, it is imperative that others know the terms of use for the database and the data content. Fortunately, the Open Data Commons group (http://opendatacommons.org) has been developing legally binding tools to govern the use of data sets. Using a combination of copyright and contractual standards, they have created three standard licenses that one can use in conjunction with data projects. In addition, it is possible to articulate a set of “community norms” that complement the use of formal licenses. While not having the force of law, norms can express the shared beliefs of a community vis-à-vis data sharing and reuse.  The three ODC licenses are:      Public Domain Dedication and License (PDDL): This dedicates the database and its content to the public domain, free for everyone to use as they see fit.     Attribution License (ODC-By): Users are free to use the database and its content in new and different ways, provided they provide attribution to the source of the data and/or the database.     Open Database License (ODC-ODbL): ODbL stipulates that any subsequent use of the database must provide attribution, an unrestricted version of the new product must always be accessible, and any new products made using ODbL material must be distributed using the same terms. It is the most restrictive of all ODC licenses.  Creative Commons (http://www.creativecommons.org/) also has a library of standardized licenses, and some of them apply to data and databases. The ODC-By license, for example, is the equivalent of a Creative Commons Attribution license (CC BY). CC BY licenses, however, require copyright ownership of the underlying work, whereas the ODC-By license applies to works not protected by copyright (such as factual data)  The two CC licenses that are of greatest relevance to data management are:      CC0 (i.e., \"CC Zero\"): When an owner wishes to waive her copyright and/or database rights, she can use the CC0 mark. It effectively places the database and data into the public domain. It is the functional equivalent of an ODC PDDL license.     Public Domain mark (PDM): It is used to mark works that are in the public domain, and for which there are no known copyright or database restrictions. It is possible to flag factual data as PDM in a database, for example, in order to make it clear it is free to use.  Selecting a data license  There is no single right answer as to which license to assign to a database or content. Note, however, that anything other than an ODC PDDL or CC0 license may cause serious problems for subsequent scientists and other users. This is because of the problem of attribution stacking. It may be possible to extract data from a data set, use it in a research project, and still maintain information as to the source of that data. It is possible to create a data set derived from hundreds of sources with each source requiring acknowledgement. Furthermore, the data in the other databases may not have originated with it, but instead sourced from other databases that also demand attribution. Rather than legally require that everyone provide attribution to the data, it might be enough to have a community norm that says “if you make extensive use of data from this data set, please credit the authors.” Data ownership at Cornell  The ownership of works produced by Cornell faculty, students, and non-academic staff is governed by the Cornell University Policy 1.5, Inventions and Related Property Rights, and especially the Cornell Copyright Policy. The precise answer will depend on whether the project was created as part of sponsored research; the employment status of the creator; whether the work was conducted “pursuant to a specific direction or assigned duty…from the University”; and, if deemed to be an “encoded work,” whether substantial university resources were used in the creation of the encoded work.", "category": "Edison", "id": 114}
{"skillName": "DSRMP03", "skillText": "For the purposes of various statutory returns (such as research income figures returned under the Research Activity Survey and published by the Higher Education Statistics Agency), research is defined by the conventions set out in the Frascati Manual.  The Frascati Manual is the internationally recognised methodology for collecting and using R&D statistics. It defines research as follows: Research and experimental development (R&D) comprise creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this stock of knowledge to devise new applications.  The term R&D covers three activities: basic research, applied research and experimental development.      Basic research is experimental or theoretical work undertaken primarily to acquire new knowledge of the underlying foundation of phenomena and observable facts, without any particular application or use in view.     Applied research is also original investigation undertaken in order to acquire new knowledge. It is, however, directed primarily towards a specific practical aim or objective.     Experimental development is systematic work, drawing on existing knowledge gained from research and/or practical experience, which is directed to producing new materials, products or devices, to installing new processes, systems and services, or to improving substantially those already produced or installed. R&D covers both formal R&D in R&D units and informal or occasional R&D in other units.  The Frascati Manual lists situations where certain activities are to be excluded from R&D except when carried out solely or primarily for the purposes of an R&D project. These include: routine testing and analysis of materials, components, products, processes, etc; feasibility studies; routine software development; general purpose data collection. The later stages of some clinical drug trials may be more akin to routine testing, particularly in cases where the original research has been done by a drug company or other contractor.  The Frascati Manual contains the following examples of the type of work included under the three components of R&D:      The determination of the amino acid sequence of an antibody molecule would be basic research. Investigations undertaken in an effort to distinguish between antibodies for various diseases would be applied research. Experimental development would then consist of devising a method for synthesising the antibody for a particular disease on the basis of knowledge of its structure and clinically testing the effectiveness of the synthesised antibody on patients who have agreed to accept experimental advanced treatment.      Theoretical investigation of the factors determining regional variations in economic growth is basic research; however, such investigation performed for the purpose of developing government policy would be applied research. The development of operational models, based upon laws revealed through research and aimed at modifying regional disparities, would be experimental development.  The Collins Paperback English Dictionary defines research as a “systematic investigation to establish facts or collect information on a subject [14].” Vaishnavi and Kuechler [23] de- fine research as “an activity that contributes to the understanding of a phenomenon.” By these definitions, reading a first-year textbook would, hopefully, be research for a student. Vaishnavi and Kuechler [23] go on to define a phenomenon as “a set of behaviours of some entity(ies) that is found interesting by the researcher or by a group,” and understanding as “knowledge that allows prediction of the behaviour of some aspect of the phenomenon.” Academic research usually includes the idea that research adds something that is, in some sense, new to the body of knowledge. The Office of Research and Innovation of Edith Cowan University state that research “comprises creative work undertaken on a system- atic basis in order to increase the stock of knowledge, including knowledge of man (sic), culture and society, and the use of this stock of knowledge to devise new applications” [17]. Other concepts that are associated with academic research include rigour, relevance and a systematic approach. These are the kinds of things that journal reviewers look for when trying to evaluate the quality of research. Research is often divided into Basic and Applied research. Wikipedia [26] states that “Basic research (also called fundamental or pure research) has as its primary objective the advancement of knowledge and the theoretical understanding of the relations among variables. It is exploratory and often driven by the researcher’s curiosity, interest, or hunch. It is conducted without any practical end in mind, although it may have unexpected results pointing to practical applications.” On the other hand, “Applied research is done to solve specific, practical questions; its primary aim is not to gain knowledge for its own sake” [26]. 2.1 Why do we do research? We do research to gain understanding. There are three main reasons why we might wish to gain understanding. Firstly, because humans are more curious than cats. We would do research to satisfy our curiosity, even if there were no practical application for the understanding obtained. Secondly, as mentioned above, we do research in order to predict the behaviour of an entity. An example of this would be seismic research that helps us predict the occurrence of earthquakes. This has practical value in that it may allow us to save lives, even though we cannot actually change the phenomenon. Thirdly, we do research in order to change the behaviour of an entity. If we can predict an entity’s behaviour, then we can also predict the effect on that behaviour if we change the entity in some way. A good example of this is medical research, where we try to understand the processes that cause a disease, so that we can prevent or cure it. 2.2 What do we do research on? What kinds of phenomena do we do research on? Simon [20] divides the universe into the natural and the artificial. Natural phenomena are those that occur ‘naturally’ in the world, such as earthquakes, diseases and human behaviour. Artificial phenomena are those that are created by man, for the purpose of satisfying man’s desires and achieving his goals. Natural Science, or natural research, is concerned with understanding and explaining natural phenomena. The Science of the Artificial [20], also known as Design Science [23], is concerned with man-made, artificial phenomena. 2.3 Types of research If one reads almost any book on how to do research, one will immediately be presented with one of two ways to do research: Quantitative or Qualitative [16, 22]. However, these are just methodologies, or prescriptions of how to do a part of research. Much of the literature on research divides the types of research into Positivist and Interpretivist viewpoints [23], although some authors disagree with this distinction [25]. The viewpoint that is assumed by a particular researcher will affect all of their research, including the methodologies they choose to use. These viewpoints, and their associated methodologies will be discussed in more detail in §5.3. 2.4 How do we do research? The Natural Science research process is composed of two activities, discovery and justi- fication [9]. Discovery is the process of generating and proposing scientific claims. This process is not well understood, and is inherently creative. Justification is the process of testing the claims for validity. This is usually done by trying to prove the claim false, as a single negative instance can do so, while innumerable positive instances cannot prove a claim true [12]. Most research methodologies are prescriptions of how to gather data and test claims; that is, they are prescriptions for the justification process and say nothing about the discovery process. 2.5 How do we measure the quality of research? The way in which the quality of a particular piece of research is measured depends on the viewpoint that was adopted by the researchers conducting the research. In Natural Science, the measure of claims and theories is their explanatory power. “Good” claims are consistent with observed facts, and provide deep, encompassing and accurate predictions of future observations [12]. 3 What is operations research? As with many other applied fields, there are two components to the discipline of OR. There is the practice of the discipline, and there is research into the tools and methods of the discipline. An analogy of this is the practice of medicine. The General Practitioner or Specialist uses their knowledge, experience, tools and methods to diagnose and treat patients. This is not research as they are not adding anything new to the body of medical knowledge, although they may be providing new understanding to the patient. At the same time there are the medical researchers who are doing research to increase their understanding of the human body, and to use that understanding to develop better tools  and methods. Sometimes an individual will combine practice and research at the same time, but they are still distinct actives. The difference between medicine and OR is that medicine does not claim that the practice is also research in its name! 3.1 Research into operations research Research into OR seeks to advance the practice of OR by various means, including, but not limited to: • Developing new, or improved, models of various systems. • Developing new, or improved, algorithms for solving models. • Developing new, or improved, methodologies. • Developing new, or improved, tools such as software. • Increasing the understanding of phenomena that affect the implementation or adop- tion of OR models and methodologies. Research that seeks to improve the practice of OR is generally applied research, but it may be considered basic research if the practical application is only potential. 3.2 Practice of operations research The Institute for Operations Research and the Management Sciences (INFORMS) de- scribes OR as “the discipline of applying advanced analytical methods to help make better decisions” [5]. They go on to expand this explanation as follows: “By using techniques such as mathematical modelling to analyse complex situations, operations research gives executives the power to make more effective decisions and build more productive systems” [5]. When a practitioner applies a standard tool, such as linear programming, to solve a problem from a class that is well understood, such as resource allocation, is he conducting research? He is definitely providing new knowledge to the organisation for whom he is solving the problem, but he is not adding anything new to the body of OR knowledge. The author would argue that in the general sense of research as “systematic investigation to establish facts or collect information on a subject [14]”, then this is research. However, is it research in the sense of research as “creative work undertaken on a systematic basis in order to increase the stock of knowledge” [17]? A brief review of 20 abstracts from Volume 52 (2006) of Management Science revealed that 50% of the published articles were Research into OR. A further 30% of articles were not OR, but rather articles on Management. Only 20% of the articles reviewed described projects that could be considered Practice of OR. However, all of these projects approached the research from a Natural Science perspective. They were projects that analysed a particular system in order to gain greater understanding. There appears to be no attempt to build a model, or to ‘solve the problem’. This would seem to indicate that, in the eyes of Management Science at least, the practice of OR is not seen as research.", "category": "Edison", "id": 115}
{"skillName": "DSDM04", "skillText": "Data archaeology refers to the art and science of recovering computer data encoded and/or encrypted in now obsolete media or formats. Data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters.  The term originally appeared in 1993 as part of the Global Oceanographic Data Archaeology and Rescue Project (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of climate change. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the Nimbus 2 satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.[1]  NASA also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape, as exemplified by the Lunar Orbiter Image Recovery Project (LOIRP).[2]  Contents      1 Recovery         1.1 Disaster Recovery         1.2 Recovery Techniques     2 Prevention     3 See also     4 References  Recovery  It is also important to make the distinction in data archaeology between data recovery, and data intelligibility. You may be able to recover the data, but not understand it. For data archeology to be effective the data must be intelligible. [3] Disaster Recovery  Data archaeologists can also use data recovery after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during hurricane Marilyn the National Media Lab assisted the National Archives and Records Administration in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within. [4] Recovery Techniques  When deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of magnetic media, which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage. [5]  Humidity can cause tapes to become unusable as they begin to deteriorate and become sticky. In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape. However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable. [6]  Lubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation. As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles. This can cause damage to the tape. Loss of lubrication can be addressed by re-lubricating the tapes. This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data. [7]  Water exposure will damage tapes over time. This often occurs in a disaster situation. If the media is in salty or dirty water, it should be rinsed in fresh water. The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage. Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage. [8] Prevention  To prevent the need of data archeology, creators and holders of digital documents should take care to employ digital preservation. See also      Digital dark age     Knowledge discovery     Bit rot  Data governance is a control that ensures that the data entry by an operations team member or by an automated process meets precisely standards, such as a Business rule, a data definition and data integrity constraints in the data model. The data governor uses data quality monitoring against production data to communicate errors in data back to operational team members, or to the technical support team, for corrective action. Data governance is used by organizations to exercise control over processes and methods used by their data stewards and data custodians in order to improve data quality.  Data governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise. Data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality. It is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient. Data governance also describes an evolutionary process for a company, altering the company’s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization. It’s about using technology when necessary in many forms to help aid the process. When companies desire, or are required, to gain control of their data, they empower their people, set up processes and get help from technology to do it.[1]  According to one vendor, data governance is a quality control discipline for assessing, managing, using, improving, monitoring, maintaining, and protecting organizational information. It is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.[2]  Contents      1 Overview     2 Data governance drivers     3 Data governance initiatives     4 Implementation     5 Data governance tools     6 Data governance organizations     7 Data governance conferences     8 See also     9 References  Overview  Data governance encompasses the people, processes, and information technology required to create a consistent and proper handling of an organization's data across the business enterprise. Goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them. Some goals includes      Increasing consistency and confidence in decision making     Decreasing the risk of regulatory fines     Improving data security, also defining and verifying the requirements for data distribution policies[3]     Maximizing the income generation potential of data     Designating accountability for information quality     Enable better planning by supervisory staff     Minimizing or eliminating re-work     Optimize staff effectiveness     Establish process performance baselines to enable improvement efforts     Acknowledge and hold all gain  These goals are realized by the implementation of Data governance programs, or initiatives using Change Management techniques Data governance drivers  While data governance initiatives can be driven by a desire to improve data quality, they are more often driven by C-Level leaders responding to external regulations. Examples of these regulations include Sarbanes-Oxley, Basel I, Basel II, HIPAA, and a number of data privacy regulations. To achieve compliance with these regulations, business processes and controls require formal management processes to govern the data subject to these regulations.[4] Successful programs identify drivers meaningful to both supervisory and executive leadership.  Common themes among the external regulations center on the need to manage risk. The risks can be financial misstatement, inadvertent release of sensitive data, or poor data quality for key decisions. Methods to manage these risks vary from industry to industry. Examples of commonly referenced best practices and guidelines include COBIT, ISO/IEC 38500, and others. The proliferation of regulations and standards creates challenges for data governance professionals, particularly when multiple regulations overlap the data being managed. Organizations often launch data governance initiatives to address these challenges. Data governance initiatives  Data governance initiatives improve data quality by assigning a team responsible for data's accuracy, accessibility, consistency, and completeness, among other metrics. This team usually consists of executive leadership, project management, line-of-business managers, and data stewards. The team usually employs some form of methodology for tracking and improving enterprise data, such as Six Sigma, and tools for data mapping, profiling, cleansing, and monitoring data.  Data governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers (such as supply chain management), compliance with regulatory law, improving operations after rapid company growth or corporate mergers, or to aid the efficiency of enterprise knowledge workers by reducing confusion and error and increasing their scope of knowledge. Many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level, leading to incongruent and redundant data quality processes. Most large companies have many applications and databases that can't easily share information. Therefore, knowledge workers within large organizations often don't have access to the information they need to best do their jobs. When they do have access to the data, the data quality may be poor. By setting up a data governance practice or Corporate Data Authority, these problems can be mitigated.  The structure of a data governance initiative will vary not only with the size of the organization, but with the desired objectives or the 'focus areas' [5] of the effort. Implementation  Implementation of a Data Governance initiative may vary in scope as well as origin. Sometimes, an executive mandate will arise to initiate an enterprise wide effort, sometimes the mandate will be to create a pilot project or projects, limited in scope and objectives, aimed at either resolving existing issues or demonstrating value. Sometimes an initiative will originate lower down in the organization’s hierarchy, and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization. The initial scope of an implementation can vary greatly as well, from review of a one-off IT system, to a cross-organization initiative. Data governance tools  Leaders of successful data governance programs declared in December 2006 at the Data Governance Conference in Orlando, Fl, that data governance is between 80 and 95 percent communication.\"[6] That stated, it is a given that many of the objectives of a Data Governance program must be accomplished with appropriate tools. Many vendors are now positioning their products as Data Governance tools; due to the different focus areas of various data governance initiatives, any given tool may or may not be appropriate, in addition, many tools that are not marketed as governance tools address governance needs.[7] Data governance organizations  DAMA International[8]     DAMA (the Data Management Association) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and data resource management (DRM).  Data Governance Professionals Organization (DGPO)[9]     The Data Governance Professionals Organization (DGPO) is a non-profit, vendor neutral, association of business, IT and data professionals dedicated to advancing the discipline of data governance. The objective of the DGPO is to provide a forum that fosters discussion and networking for members and to encourage, develop and advance the skills of members working in the data governance discipline.  The Data Governance Society [10]     The Data Governance Society, Inc. is dedicated to fostering a new paradigm for the effective use and protection of information in which Data is governed and leveraged as a unique corporate asset.  The Data Governance Council [11]     The Data Governance Council is an organization formed by IBM consisting of companies, institutions and technology solution providers with the stated objective to build consistency and quality control in governance, which will help companies better protect critical data.\"  IQ International -- the International Association for Information and Data Quality[12]     IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.  Data governance conferences  A number of major conferences relevant to data governance are held annually:  Data Governance and Information Quality Conference[13]     Commercial conferences held each year in the USA  Data Governance Conference Europe,[14]     Commercial conferences held annually in London, England . Information and Data Quality Conference[15]     Not for profit conference run by IQ International in the USA Master Data Management & Data Governance Conferences[16]     Six major conferences are run annually by the MDM Institute in London, San Francisco, Sydney, Toronto, Madrid, Frankfurt, and New York City. Financial Information Summit series of conferences[17] Hosted by Inside Reference Data magazine in New York, London, Hong Kong, Toronto, Chicago, Frankfurt, Paris and Tokyo.  See also      Information Architecture     Information technology governance     Semantics of Business Vocabulary and Business Rules     Master data management     COBIT     ISO/IEC 38500     ISO/TC 215     Operational risk management     Basel II Accord     HIPAA     Sarbanes-Oxley Act     Information technology controls     Data Protection Directive (EU)     Universal Data Element Framework     Asset Description Metadata Schema  What is a data library?  A data library is a collection of numeric and/or geospatial data sets for secondary use in research. (Scroll down to view definition from the Online Dictionary for Library and Information Science.) A data library is normally part of a larger institution (academic, corporate, scientific, medical, governmental, etc.) established to serve the data users of that organisation. The data library tends to house local data collections and provides access to them through various means (CD-/DVD-ROMs or central server for download). A data library may also maintain subscriptions to licensed data resources for its users to access. Whether a data library is also considered a data archive may depend on the extent of unique holdings in the collection, whether long-term preservation services are offered, and whether it serves a broader community (as national data archives do). Importance of data libraries and data librarianship  In August 2001, the Association of Research Libraries (ARL)  published SPEC Kit 263: Numeric Data Products and Services  , presenting results from a survey of ARL member institutions involved in collecting and providing services for numeric data resources. Services offered by data libraries and data librarians  Library service providing support at the institutional level for the use of numerical and other types of datasets in research. Amongst the support activities typically available:      Reference Assistance — locating numeric or geospatial datasets containing measurable variables on a particular topic or group of topics, in response to a user query.     User Instruction — providing hands-on training to groups of users in locating data resources on particular topics, how to download data and read it into spreadsheet, statistical, database, or GIS packages, how to interpret codebooks and other documentation.     Technical Assistance - including easing registration procedures, troubleshooting problems with the dataset, such as errors in the documentation, reformatting data into something a user can work with, and helping with statistical methodology.     Collection Development & Management - acquire, maintain, and manage a collection of data files used for secondary analysis by the local user community; purchase institutional data subscriptions; act as a site representative to data providers and national data archives for the institution.     Preservation and Data Sharing Services - act on a strategy of preservation of datasets in the collection, such as media refreshment and file format migration; download and keep records on updated versions from a central repository. Also, assist users in preparing original data for secondary use by others; either for deposit in a central or institutional repository, or for less formal ways of sharing data. This may also involve marking up the data into an appropriate XML standard, such as the Data Documentation Initiative, or adding other metadata to facilitate online discovery.  Associations      IASSIST      (International Association for Social Science Information and Service Technology)     DISC-UK      (Data Information Specialists Committee—United Kingdom)     APDU      (Association of Public Data Users - USA)     CAPDU      (Canadian Association of Public Data Users)  Examples of Data Library  The Massachusetts Institute of Technology’s (MIT) Data Management and Publishing tutorial,  The EDINA Research Data Management Training (MANTRA),  The University of Edinburgh’s Data Library and  The University of Minnesota libraries’ Data Management Course for Structural Engineers The London School of Economics and Political Science Data and Statistics  References      Clubb, J., Austin, E., and Geda, C. \"'Sharing research data in the social sciences.'\" In Sharing Research Data, S. Fienberg, M. Martin, and M. Straf, Eds. National Academy Press, Washington, D.C., 1985, 39-88.     Geraci, D., Humphrey, C., and Jacobs, J. Data Basics. Canadian Library Association, Ottawa, ON, 2005.     Martinez, Luis & Macdonald, Stuart, \"'Supporting local data users in the UK academic community'\"      . Ariadne, issue 44, July 2005.     See the IASSIST Bibliography of Selected Works      for articles tracing the history of data libraries and its relationship to the archivist profession, going back to the 1960s and '70s up to 1996.     See IASSIST Quarterly      articles from 1993 to the present, focusing on data libraries, data archives, data support, and information technology for the social sciences.  See also  Digital curation Digital preservation Open Data Data sharing is the practice of making data used for scholarly research available to other investigators. Replication has a long history in science. The motto of The Royal Society is 'Nullius in verba', translated \"Take no man's word for it.\"[1] Many funding agencies, institutions, and publication venues have policies regarding data sharing because transparency and openness are considered by many to be part of the scientific method.  A number of funding agencies and science journals require authors of peer-reviewed papers to share any supplemental information (raw data, statistical methods or source code) necessary to understand, develop or reproduce published research. A great deal of scientific research is not subject to data sharing requirements, and many of these policies have liberal exceptions. In the absence of any binding requirement, data sharing is at the discretion of the scientists themselves. In addition, in certain situations agencies and institutions prohibit or severely limit data sharing to protect proprietary interests, national security, and subject/patient/victim confidentiality. Data sharing may also be restricted to protect institutions and scientists from use of data for political purposes.  Data and methods may be requested from an author years after publication. In order to encourage data sharing and prevent the loss or corruption of data, a number of funding agencies and journals established policies on data archiving. Access to publicly archived data is a recent development in the history of science made possible by technological advances in communications and information technology.  Despite policies on data sharing and archiving, data withholding still happens. Authors may fail to archive data or they only archive a portion of the data. Failure to archive data alone is not data withholding. When a researcher requests additional information, an author sometimes refuses to provide it.[2] When authors withhold data like this, they run the risk of losing the trust of the science community.[3]  Contents      1 U.S. government policies         1.1 Federal law         1.2 NIH data sharing policy         1.3 NSF Policy from Grant General Conditions     2 Office of Research Integrity     3 Ideals in data sharing     4 International policies     5 Data sharing problems         5.1 Academic genetics         5.2 Academic psychology         5.3 Scientists in training     6 Differing approaches in different fields     7 See also     8 References     9 Literature     10 External links  U.S. government policies Federal law  On August 9, 2007, President Bush signed the America COMPETES Act (or the \"America Creating Opportunities to Meaningfully Promote Excellence in Technology, Education, and Science Act\") requiring civilian federal agencies to provide guidelines, policy and procedures, to facilitate and optimize the open exchange of data and research between agencies, the public and policymakers. See Section 1009.[4] NIH data sharing policy      ‘The National Institutes of Health (NIH) Grants Policy Statement defines \"data\" as \"recorded information, regardless of the form or medium on which it may be recorded, and includes writings, films, sound recordings, pictorial reproductions, drawings, designs, or other graphic representations, procedural manuals, forms, diagrams, work flow charts, equipment descriptions, data files, data processing or computer programs (software), statistical records, and other research data.\"’     — Council on Governamental Relations[5]  The NIH Final Statement of Sharing of Research Data says:      ‘Final NIH statement on sharing research data      .’[6]  NSF Policy from Grant General Conditions      36. Sharing of Findings, Data, and Other Research Products      a. NSF …expects investigators to share with other researchers, at no more than incremental cost and within a reasonable time, the data, samples, physical collections and other supporting materials created or gathered in the course of the work. It also encourages awardees to share software and inventions or otherwise act to make the innovations they embody widely useful and usable.     b. Adjustments and, where essential, exceptions may be allowed to safeguard the rights of individuals and subjects, the validity of results, or the integrity of collections or to accommodate legitimate interests of investigators.     — \"National Science Foundation      : Grant General Conditions (GC-1)\", April 1, 2001 (p. 17).  Office of Research Integrity  Allegations of misconduct in medical research carry severe consequences. The United States Department of Health and Human Services established an office to oversee investigations of allegations of misconduct, including data withholding. The website defines the mission:      \"The Office of Research Integrity (ORI) promotes integrity in biomedical and behavioral research supported by the U.S. Public Health Service (PHS) at about 4,000 institutions worldwide. ORI monitors institutional investigations of research misconduct and facilitates the responsible conduct of research (RCR) through educational, preventive, and regulatory activities.\"     — Office of Research Integrity      .  Ideals in data sharing  Some research organizations feel particularly strongly about data sharing. Stanford University's WaveLab has a philosophy about reproducible research and disclosing all algorithms and source code necessary to reproduce the research. In a paper titled \"WaveLab and Reproducible Research,\" the authors describe some of the problems they encountered in trying to reproduce their own research after a period of time. In many cases, it was so difficult they gave up the effort. These experiences are what convinced them of the importance of disclosing source code.[7] The philosophy is described:      The idea is: An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.[8]  The Data Observation Network for Earth (DataONE) and Data Conservancy[9] are projects supported by the National Science Foundation to encourage and facilitate data sharing among research scientists and better support meta-analysis. In environmental sciences, the research community is recognizing that major scientific advances involving integration of knowledge in and across fields will require that researchers overcome not only the technological barriers to data sharing but also the historically entrenched institutional and sociological barriers.[10] Dr. Richard J. Hodes, director of the National Institute on Aging has stated, \"the old model in which researchers jealously guarded their data is no longer applicable\".[11]  The Alliance for Taxpayer Access is a group of organizations that support open access to government sponsored research. The group has expressed a \"Statement of Principles\" explaining why they believe open access is important.[12] They also list a number of international public access policies.[13] International policies      Australia      Austria      Europe — Commission of European Communities      Germany      United Kingdom      'Omic Data Sharing — a list of policies of major science funders BioSharing.org Catalogue of Data Policies   Data sharing problems Academic genetics  Withholding of data has become so commonplace in academic genetics that researchers at Massachusetts General Hospital published a journal article on the subject. The study found that \"Because they were denied access to data, 28% of geneticists reported that they had been unable to confirm published research.\"[14] Academic psychology  In a 2006 study, it was observed that, of 141 authors of a publication from the American Psychology Association (APA) empirical articles, 103 (73%) did not respond with their data over a 6-month period.[15] In a follow up study published in 2015, it was found that 246 out of 394 contacted authors of papers in APA journals did not share their data upon request (62%).[16] Scientists in training  A study of scientists in training indicated many had already experienced data withholding.[17] This study has given rise to the fear the future generation of scientists will not abide by the established practices. Differing approaches in different fields  Requirements for data sharing are more commonly imposed by institutions, funding agencies, and publication venues in the medical and biological sciences than in the physical sciences. Requirements vary widely regarding whether data must be shared at all, with whom the data must be shared, and who must bear the expense of data sharing.  Funding agencies such as the NIH and NSF tend to require greater sharing of data, but even these requirements tend to acknowledge the concerns of patient confidentiality, costs incurred in sharing data, and the legitimacy of the request. Private interests and public agencies with national security interests (defense and law enforcement) often discourage sharing of data and methods through non-disclosure agreements.  Data sharing poses specific challenges in participatory monitoring initiatives, for example where forest communities collect data on local social and environmental conditions. In this case, a rights-based approach to the development of data-sharing protocols can be based on principles of free, prior and informed consent, and prioritise the protection of the rights of those who generated the data, and/or those potentially affected by data-sharing.[18] See also      Data archive     Data publishing     Data citation     re3data.org - Registry of Research Data Repositories", "category": "Edison", "id": 116}
{"skillName": "DSDM03", "skillText": "Data Collection and  Integration  Improving quality levels requires good actionable data. Many times projects stumble due to the difficulties of acquiring data or the key parameters that give data value: other software systems may hold needed information; different sources may have incompatible data formats; complex IT infrastructures can make it difficult for business users to find their data. Data acquisition provides the critical link of information between the shop floor and the rest of the organization. Data Collection and  Integration from ProFicient brings the ability to:      Eliminate the complexity of getting data into the software. Focus on continuous improvement efforts rather than endless data-loading exercises.     Obtain a complete picture of operations. Integrate all the silos of data you need without bothering IT or waiting until a quality team member frees up time to import and export data.     Include data from other commonly used software. Stop worrying about what might have been missed due to the complexity of just getting data from your existing systems and jump-start your quality improvement efforts.     Adjust the level of automation for data collection. Create a start-small win-big approach and roll out the data collection project at your own pace.     Collect data in any environment. Use any mobile device as an untethered way to collect and enter data into the system.  Data integration involves combining data residing in different sources and providing users with a unified view of these data.[1] This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume and the need to share existing data explodes.[2] It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.  Contents      1 History     2 Example     3 Theory of data integration         3.1 Definitions         3.2 Query processing     4 Data Integration tools     5 Data integration in the life sciences     6 See also     7 References  History Figure 1: Simple schematic for a data warehouse. The ETL process extracts information from the source databases, transforms it and then loads it into the data warehouse. Figure 2: Simple schematic for a data-integration solution. A system designer constructs a mediated schema against which users can run queries. The virtual database interfaces with the source databases via wrapper code if required.  Issues with combining heterogeneous data sources, often referred to as information silos, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.[3] The first data integration system driven by structured metadata was designed at the University of Minnesota in 1991, for the Integrated Public Use Microdata Series (IPUMS). IPUMS used a data warehousing approach, which extracts, transforms, and loads data from heterogeneous sources into a single view schema so data from different sources become compatible.[4] By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach offers a tightly coupled architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.[5]  The data warehouse approach is less feasible for datasets that are frequently updated, requiring the ETL process to be continuously re-executed for synchronization. Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data. This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.  As of 2009 the trend in data integration favored loosening the coupling between data[citation needed] and providing a unified query-interface to access real time data over a mediated schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the SOA approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases. Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the \"Global As View\" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the \"Local As View\" (LAV) approach). The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.  As of 2010 some of the work in data integration research concerns the semantic integration problem. This problem addresses not the structuring of the architecture of the integration, but how to resolve semantic conflicts between heterogeneous data sources. For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like \"earnings\" inevitably have different meanings. In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer). A common strategy for the resolution of such problems involves the use of ontologies which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents ontology-based data integration. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.[6]  As of 2011 it was determined that current data modeling methods were imparting data isolation into every data architecture in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.[7][8] One enhanced data modeling method recasts data models by augmenting them with structural metadata in the form of standardized data entities. As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models. Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models. Multiple data models that contain the same standard data entity may participate in the same commonality relationship. When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.  Since 2011, Data hub approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, Data lake approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.[9] These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub. Example  Consider a web application where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.). Traditionally, the information must be stored in a single database with a single schema. But any single enterprise would find information of this breadth somewhat difficult and expensive to collect. Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.  A data-integration solution may address this problem by considering these external resources as materialized views over a virtual mediated schema, resulting in \"virtual data integration\". This means application-developers construct a virtual schema — the mediated schema — to best model the kinds of answers their users want. Next, they design \"wrappers\" or adapters for each data source, such as the crime database and weather website. These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2). When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources. Finally, the virtual database combines the results of these queries into the answer to the user's query.  This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them. It contrasts with ETL systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage virtual mediated schema to implement data harmonization; whereby the data are copied from the designated \"master\" source to the defined targets, field by field. Advanced Data virtualization is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using hub and spoke architecture.  Each data source is disparate and as such is not designed to support reliable joins between data sources. Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets. Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.  One solution is to recast disparate databases to integrate these databases without the need for ETL. The recast databases support commonality constraints where referential integrity may be enforced between databases. The recast databases provide designed data access paths with data value commonality across databases. Theory of data integration  The theory of data integration[1] forms a subset of database theory and formalizes the underlying concepts of the problem in first-order logic. Applying the theories gives indications as to the feasibility and difficulty of data integration. While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,[10] including those that include nested relational / XML databases [11] and those that treat databases as programs.[12] Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as JDBC and are not studied at the theoretical level. Definitions  Data integration systems are formally defined as a triple ⟨ G , S , M ⟩ {\\displaystyle \\left\\langle G,S,M\\right\\rangle } \\left\\langle G,S,M\\right\\rangle where G {\\displaystyle G} G is the global (or mediated) schema, S {\\displaystyle S} S is the heterogeneous set of source schemas, and M {\\displaystyle M} M is the mapping that maps queries between the source and the global schemas. Both G {\\displaystyle G} G and S {\\displaystyle S} S are expressed in languages over alphabets composed of symbols for each of their respective relations. The mapping M {\\displaystyle M} M consists of assertions between queries over G {\\displaystyle G} G and queries over S {\\displaystyle S} S. When users pose queries over the data integration system, they pose queries over G {\\displaystyle G} G and the mapping then asserts connections between the elements in the global schema and the source schemas.  A database over a schema is defined as a set of sets, one for each relation (in a relational database). The database corresponding to the source schema S {\\displaystyle S} S would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the source database. Note that this single source database may actually represent a collection of disconnected databases. The database corresponding to the virtual mediated schema G {\\displaystyle G} G is called the global database. The global database must satisfy the mapping M {\\displaystyle M} M with respect to the source database. The legality of this mapping depends on the nature of the correspondence between G {\\displaystyle G} G and S {\\displaystyle S} S. Two popular ways to model this correspondence exist: Global as View or GAV and Local as View or LAV. Figure 3: Illustration of tuple space of the GAV and LAV mappings.[13] In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.  GAV systems model the global database as a set of views over S {\\displaystyle S} S. In this case M {\\displaystyle M} M associates to each element of G {\\displaystyle G} G a query over S {\\displaystyle S} S. Query processing becomes a straightforward operation due to the well-defined associations between G {\\displaystyle G} G and S {\\displaystyle S} S. The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases. If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.  In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators. For example, consider if one of the sources served a weather website. The designer would likely then add a corresponding element for weather to the global schema. Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website. This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.  On the other hand, in LAV, the source database is modeled as a set of views over G {\\displaystyle G} G. In this case M {\\displaystyle M} M associates to each element of S {\\displaystyle S} S a query over G {\\displaystyle G} G. Here the exact associations between G {\\displaystyle G} G and S {\\displaystyle S} S are no longer well-defined. As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor. The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.[1]  In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources. Consider again if one of the sources serves a weather website. The designer would add corresponding elements for weather to the global schema only if none existed already. Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas. The complexity of adding the new source moves from the designer to the query processor. Query processing  The theory of query processing in data integration systems is commonly expressed using conjunctive queries and Datalog, a purely declarative logic programming language.[14] One can loosely think of a conjunctive query as a logical function applied to the relations of a database such as \" f ( A , B ) {\\displaystyle f(A,B)} f(A,B) where A < B {\\displaystyle A<B} A<B\". If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query. While formal languages like Datalog express these queries concisely and without ambiguity, common SQL queries count as conjunctive queries as well.  In terms of data integration, \"query containment\" represents an important property of conjunctive queries. A query A {\\displaystyle A} A contains another query B {\\displaystyle B} B (denoted A ⊃ B {\\displaystyle A\\supset B} A\\supset B) if the results of applying B {\\displaystyle B} B are a subset of the results of applying A {\\displaystyle A} A for any database. The two queries are said to be equivalent if the resulting sets are equal for any database. This is important because in both GAV and LAV systems, a user poses conjunctive queries over a virtual schema represented by a set of views, or \"materialized\" conjunctive queries. Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query. This corresponds to the problem of answering queries using views (AQUV).[15]  In GAV systems, a system designer writes mediator code to define the query-rewriting. Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source. Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent. While the designer does the majority of the work beforehand, some GAV systems such as Tsimmis  involve simplifying the mediator description process.  In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy. The integration system must execute a search over the space of possible queries in order to find the best rewrite. The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete. As of 2009 the MiniCon algorithm[15] is the leading query rewriting algorithm for LAV data integration systems.  In general, the complexity of query rewriting is NP-complete.[15] If the space of rewrites is relatively small this does not pose a problem — even for integration systems with hundreds of sources. Data Integration tools      Alteryx     Analytics Canvas     Cloud Elements API Integration     DataWatch     Denodo Platform     HiperFabric      Lavastorm     ParseKit (enigma.io)     Paxata     RapidMiner Studio     Red Hat JBoss Data Virtualization. Community project: teiid.     Azure Data Factory (ADF)     SQL Server Integration Services (SSIS)  Data integration in the life sciences  Large-scale questions in science, such as global warming, invasive species spread, and resource depletion, are increasingly requiring the collection of disparate data sets for meta-analysis. This type of data integration is especially challenging for ecological and environmental data because metadata standards are not agreed upon and there are many different data types produced in these fields. National Science Foundation initiatives such as Datanet are intended to make data integration easier for scientists by providing cyberinfrastructure and setting standards. The five funded Datanet initiatives are DataONE,[16] led by William Michener at the University of New Mexico; The Data Conservancy,[17] led by Sayeed Choudhury of Johns Hopkins University; SEAD: Sustainable Environment through Actionable Data,[18] led by Margaret Hedstrom of the University of Michigan; the DataNet Federation Consortium,[19] led by Reagan Moore of the University of North Carolina; and Terra Populus,[20] led by Steven Ruggles of the University of Minnesota. The Research Data Alliance,[21] has more recently explored creating global data integration frameworks. The OpenPHACTS project, funded through the European Union Innovative Medicines Initiative, built a drug discovery platform by linking datasets from providers such as European Bioinformatics Institute, Royal Society of Chemistry, UniProt, WikiPathways and DrugBank. See also      Business semantics management     Core data integration     Customer data integration     Data curation     Data fusion     Data mapping     Data virtualization     Data Warehousing     Data wrangling     Database model     Datalog     Dataspaces     Edge data integration     Enterprise application integration     Enterprise Architecture framework     Enterprise Information Integration (EII)     Enterprise integration     Extract, transform, load     Geodi: Geoscientific Data Integration     Information integration     Information Server     Information silo     Integration Competency Center     Integration Consortium     JXTA     Master data management     Object-relational mapping     Ontology based data integration     Open Text     Schema Matching     Semantic Integration     SQL     Three schema approach     UDEF     Web service       Initiatives and organizations      Health Level 7     Open Knowledge Initiative     OSS through Java     Schools Interoperability Framework (SIF)  Commercial products      Adeptia ESB Suite     Amtrix     Astera Software Centerprise Data Integrator     CEITON     Cloud Elements API Integration     Dell Boomi     ECS Financials IMS     IBM WebSphere Message Broker     Informatica Cloud Data Integration     Information Builders iWay ISM     Intersystems Ensemble     Jitterbit Integration Server     Magic Software xpi Integration Platform     Microsoft BizTalk Server     Mule Enterprise     Oracle SOA Suite     SAP NetWeaver Process Integration (PI)     SnapLogic     Software AG Suite     Tibco ActiveMatrix/Business Works     WebMethods     ACA  Open-source projects      UltraESB     Apache ActiveMQ     Mule ESB     Apache Camel     Guaraná DSL     Fuse ESB (based on Apache ServiceMix)     Fuse Mediation Router (based on Apache Camel)     Fuse Message Broker (based on Apache ActiveMQ)     MuleSoft     Openadaptor     OpenESB     Petals ESB     Talend     Virtuoso Universal Server     RabbitMQ (based on AMQP protocol)    Core data integration From Wikipedia, the free encyclopedia \tThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2007) (Learn how and when to remove this template message)  Core data integration is the use of data integration technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:      ETL (Extract, transform, load) implementations     EAI (Enterprise Application Integration) implementations     SOA (Service-Oriented Architecture) implementations     ESB (Enterprise Service Bus) implementations  Core data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.  Because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create edge data integration, using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. See also      data integration     edge data integration An edge data integration is an implementation of data integration technology undertaken in an ad hoc or tactical fashion. This is also sometimes referred to as point-to-point integration because it connects two types of data directly to serve a narrow purpose. Many edge integrations, and actually the vast majority of all data integration, involves hand-coded scripts. Some may take the form of Business Mashups (web application hybrids), Rich Internet applications, or other browser-based models that take advantage of Web 2.0 technologies to combine data in a Web browser.  Examples of edge data integration projects might be:      extracting a list of customers from a host Sales Force Automation application and writing the results to an Excel spreadsheet     creating a script-driven framework for managing RSS feeds     combining data from a weather Web site, a shipping company's Web site, and a company's internal logistics database to track shipments and estimated arrival times of packages  It has been claimed that edge data integration do not typically require large budgets and centrally managed technologies, which is in contrast to a core data integration. See also      core data integration     Business Mashups     Rich Internet application     Web 2.0     Yahoo! Pipes     Microsoft Popfly     IBM Mashup Center     Enterprise application integration (EAI) is the use of software and computer systems' architectural principles to integrate a set of enterprise computer applications.  Contents      1 Overview         1.1 Improving connectivity         1.2 Purposes         1.3 Patterns             1.3.1 Integration patterns             1.3.2 Access patterns             1.3.3 Lifetime patterns         1.4 Topologies         1.5 Technologies         1.6 Communication architectures         1.7 Implementation pitfalls         1.8 See also             1.8.1 Initiatives and organizations             1.8.2 Commercial products             1.8.3 Open-source projects         1.9 References  Overview  Enterprise application integration is an integration framework composed of a collection of technologies and services which form a middleware or \"middleware framework\" to enable integration of systems and applications across an enterprise.  Many types of business software such as supply chain management applications, ERP systems, CRM applications for managing customers, business intelligence applications, payroll and human resources systems typically cannot communicate with one another in order to share data or business rules. For this reason, such applications are sometimes referred to as islands of automation or information silos. This lack of communication leads to inefficiencies, wherein identical data are stored in multiple locations, or straightforward processes are unable to be automated.  Enterprise application integration is the process of linking such applications within a single organization together in order to simplify and automate business processes to the greatest extent possible, while at the same time avoiding having to make sweeping changes to the existing applications or data structures. Applications can be linked either at the back-end via APIs or (seldomly) the front-end (GUI).[1]  In the words of the Gartner Group, EAI is the \"unrestricted sharing of data and business processes among any connected application or data sources in the enterprise.\"[2]  The various systems that need to be linked together may reside on different operating systems, use different database solutions or computer languages, or different date and time formats, or may be legacy systems that are no longer supported by the vendor who originally created them. In some cases, such systems are dubbed \"stovepipe systems\" because they consist of components that have been jammed together in a way that makes it very hard to modify them in any way. Improving connectivity  If integration is applied without following a structured EAI approach, point-to-point connections grow across an organization. Dependencies are added on an impromptu basis, resulting in a complex structure that is difficult to maintain.[3] This is commonly referred to as spaghetti, an allusion to the programming equivalent of spaghetti code. For example:  The number of connections needed to have fully meshed point-to-point connections, with n {\\displaystyle n} n points, is given by ( n 2 ) = n ( n − 1 ) 2 {\\displaystyle {\\tbinom {n}{2}}={\\frac {n(n-1)}{2}}} {\\tbinom n2}={\\frac {n(n-1)}{2}} (see binomial coefficient). Thus, for ten applications to be fully integrated point-to-point, 10 × 9 2 {\\displaystyle {\\frac {10\\times 9}{2}}} {\\frac {10\\times 9}{2}}, or 45 point-to-point connections are needed.  However the number of connections within organizations does not grow according to the square of the number points. In general, the number of connections to any point is independent of the number of other points in an organization. (Thought experiment: if an additional point is added to your organization, are you aware of it? Does it increase the number of connections other unrelated points have?) There are a small number of \"collection\" points for which this does not apply, but these do not require EAI patterns to manage.  EAI can also increase coupling between systems and therefore increase management overhead and costs.  However, EAI is not just about sharing data between applications; it focuses on sharing both business data and business process. A middleware analyst attending to EAI may also look at the system of systems. Purposes  EAI can be used for different purposes:      Data integration: Ensures that information in multiple systems is kept consistent. This is also known as enterprise information integration (EII).     Vendor independence: Extracts business policies or rules from applications and implements them in the EAI system, so that even if one of the business applications is replaced with a different vendor's application, the business rules do not have to be re-implemented.     Common facade: An EAI system can front-end a cluster of applications, providing a single consistent access interface to these applications and shielding users from having to learn to use different software packages.  Patterns  This section describes common design patterns for implementing EAI, including integration, access and lifetime patterns. These are abstract patterns and can be implemented in many different ways. There are many other patterns commonly used in the industry, ranging from high-level abstract design patterns to highly specific implementation patterns.[4] Integration patterns  There are two patterns that EAI systems implement:[5]  Mediation (intra-communication)     Here, the EAI system acts as the go-between or broker between multiple applications. Whenever an interesting event occurs in an application (for instance, new information is created or a new transaction completed) an integration module in the EAI system is notified. The module then propagates the changes to other relevant applications. Federation (inter-communication)     In this case, the EAI system acts as the overarching facade across multiple applications. All event calls from the 'outside world' to any of the applications are front-ended by the EAI system. The EAI system is configured to expose only the relevant information and interfaces of the underlying applications to the outside world, and performs all interactions with the underlying applications on behalf of the requester.  Both patterns are often used concurrently. The same EAI system could be keeping multiple applications in sync (mediation), while servicing requests from external users against these applications (federation). Access patterns  EAI supports both asynchronous (fire and forget) and synchronous access patterns, the former being typical in the mediation case and the latter in the federation case.[citation needed] Lifetime patterns  An integration operation could be short-lived (e.g. keeping data in sync across two applications could be completed within a second) or long-lived (e.g. one of the steps could involve the EAI system interacting with a human work flow application for approval of a loan that takes hours or days to complete).[citation needed] Topologies  There are two major topologies:-- hub-and-spoke, and bus. Each has its own advantages and disadvantages. In the hub-and-spoke model, the EAI system is at the center (the hub), and interacts with the applications via the spokes. In the bus model, the EAI system is the bus (or is implemented as a resident module in an already existing message bus or message-oriented middleware).  Most large enterprises use zoned network to create layered defense against network oriented threats. For example, an enterprise typically has a credit card processing (PCI-compliant) zone, a non-PCI zone, a data zone, a DMZ zone to proxy external user access, and an IWZ zone to proxy internal user access. Applications need to integrate across multiple zones. The Hub and spoke model would work better in this case. Technologies  Multiple technologies are used in implementing each of the components of the EAI system:  Bus/hub     This is usually implemented by enhancing standard middleware products (application server, message bus) or implemented as a stand-alone program (i. e., does not use any middleware), acting as its own middleware. Application connectivity     The bus/hub connects to applications through a set of adapters (also referred to as connectors). These are programs that know how to interact with an underlying business application. The adapter performs two-way communication, performing requests from the hub against the application, and notifying the hub when an event of interest occurs in the application (a new record inserted, a transaction completed, etc.). Adapters can be specific to an application (e. g., built against the application vendor's client libraries) or specific to a class of applications (e. g., can interact with any application through a standard communication protocol, such as SOAP, SMTP or Action Message Format (AMF)). The adapter could reside in the same process space as the bus/hub or execute in a remote location and interact with the hub/bus through industry standard protocols such as message queues, web services, or even use a proprietary protocol. In the Java world, standards such as JCA allow adapters to be created in a vendor-neutral manner. Data format and transformation     To avoid every adapter having to convert data to/from every other applications' formats, EAI systems usually stipulate an application-independent (or common) data format. The EAI system usually provides a data transformation service as well to help convert between application-specific and common formats. This is done in two steps: the adapter converts information from the application's format to the bus's common format. Then, semantic transformations are applied on this (converting zip codes to city names, splitting/merging objects from one application into objects in the other applications, and so on). Integration modules     An EAI system could be participating in multiple concurrent integration operations at any given time, each type of integration being processed by a different integration module. Integration modules subscribe to events of specific types and process notifications that they receive when these events occur. These modules could be implemented in different ways: on Java-based EAI systems, these could be web applications or EJBs or even POJOs that conform to the EAI system's specifications. Support for transactions     When used for process integration, the EAI system also provides transactional consistency across applications by executing all integration operations across all applications in a single overarching distributed transaction (using two-phase commit protocols or compensating transactions).  Communication architectures  Currently, there are many variations of thought on what constitutes the best infrastructure, component model, and standards structure for Enterprise Application Integration. There seems to be consensus that four components are essential for a modern enterprise application integration architecture:      A centralized broker that handles security, access, and communication. This can be accomplished through integration servers (like the School Interoperability Framework (SIF) Zone Integration Servers) or through similar software like the enterprise service bus (ESB) model that acts as a services manager.     An independent data model based on a standard data structure, also known as a canonical data model. It appears that XML and the use of XML style sheets has become the de facto and in some cases de jure standard for this uniform business language.     A connector, or agent model where each vendor, application, or interface can build a single component that can speak natively to that application and communicate with the centralized broker.     A system model that defines the APIs, data flow and rules of engagement to the system such that components can be built to interface with it in a standardized way.  Although other approaches like connecting at the database or user-interface level have been explored, they have not been found to scale or be able to adjust. Individual applications can publish messages to the centralized broker and subscribe to receive certain messages from that broker. Each application only requires one connection to the broker. This central control approach can be extremely scalable and highly evolvable.  Enterprise Application Integration is related to middleware technologies such as message-oriented middleware (MOM), and data representation technologies such as XML or JSON. Other EAI technologies involve using web services as part of service-oriented architecture as a means of integration. Enterprise Application Integration tends to be data centric. In the near future, it will come to include content integration and business processes. Implementation pitfalls  In 2003 it was reported that 70% of all EAI projects fail. Most of these failures are not due to the software itself or technical difficulties, but due to management issues. Integration Consortium European Chairman Steve Craggs has outlined the seven main pitfalls undertaken by companies using EAI systems and explains solutions to these problems.[6]      Constant change: The very nature of EAI is dynamic and requires dynamic project managers to manage their implementation.     Shortage of EAI experts: EAI requires knowledge of many issues and technical aspects.     Competing standards: Within the EAI field, the paradox is that EAI standards themselves are not universal.     EAI is a tool paradigm: EAI is not a tool, but rather a system and should be implemented as such.     Building interfaces is an art: Engineering the solution is not sufficient. Solutions need to be negotiated with user departments to reach a common consensus on the final outcome. A lack of consensus on interface designs leads to excessive effort to map between various systems data requirements.     Loss of detail: Information that seemed unimportant at an earlier stage may become crucial later.     Accountability: Since so many departments have many conflicting requirements, there should be clear accountability for the system's final structure.  Other potential problems may arise in these areas:      Lack of centralized co-ordination of EAI work.[7]     Emerging Requirements: EAI implementations should be extensible and modular to allow for future changes.     Protectionism: The applications whose data is being integrated often belong to different departments that have technical, cultural, and political reasons for not wanting to share their data with other departments  See also      Enterprise architecture framework     Business semantics management     Data integration     Enterprise information integration     Enterprise integration     Enterprise Integration Patterns     Enterprise service bus     Generalised Enterprise Reference Architecture and Methodology     Integration appliance     Integration competency center     Integration platform     Straight through processing     System integration     Enterprise information integration (EII), is the ability to support a unified view of data and information for an entire organization. In a data virtualization application of EII, a process of information integration, using data abstraction to provide a unified interface (known as uniform data access) for viewing all the data within an organization, and a single set of structures and naming conventions (known as uniform information representation) to represent this data; the goal of EII is to get a large set of heterogeneous data sources to appear to a user or system as a single, homogeneous data source.  Contents      1 Overview     2 Applications     3 Data access technologies     4 See also     5 References  Overview  Data within an enterprise can be stored in heterogeneous formats, including relational databases (which themselves come in a large number of varieties), text files, XML files, spreadsheets and a variety of proprietary storage methods, each with their own indexing and data access methods.  Standardized data access APIs have emerged, that offer a specific set of commands to retrieve and modify data from a generic data source. Many applications exist that implement these APIs' commands across various data sources, most notably relational databases. Such APIs include ODBC, JDBC, XQJ, OLE DB, and more recently ADO.NET.  There are also standard formats for representing data within a file, that are very important to information integration. The best-known of these is XML, which has emerged as a standard universal representation format. There are also more specific XML \"grammars\" defined for specific types of data, such as Geography Markup Language for expressing geographical features, and Directory Service Markup Language, for holding directory-style information. In addition, non-XML standard formats exist, such as iCalendar, for representing calendar information, and vCard, for business card information.  Enterprise Information Integration (EII) applies data integration commercially. Despite the theoretical problems described above, the private sector shows more concern with the problems of data integration as a viable product.[1] EII emphasizes neither on correctness nor tractability, but speed and simplicity. An EII industry has emerged, but many professionals[who?] believe it does not perform to its full potential. Practitioners cite the following major issues which EII must address for the industry to become mature:[citation needed]  Combining disparate data sets      Each data source is disparate and as such is not designed to support EII. Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets. Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.      One solution is to recast disparate databases to integrate these databases without the need for ETL. The recast databases support commonality constraints where referential integrity may be enforced between databases. The recast databases provide designed data access paths with data value commonality across databases. Simplicity of understanding      Answering queries with views arouses interest from a theoretical standpoint, but difficulties in understanding how to incorporate it as an \"enterprise solution\".[citation needed] Some developers[who?] believe it should be merged with EAI. Others[who?] believe it should be incorporated with ETL systems, citing customers' confusion over the differences between the two services.[citation needed] Simplicity of deployment      Even if recognized as a solution to a problem, EII as of 2009 currently takes time to apply and offers complexities in deployment. People have proposed a variety of schema-less solutions such as \"Lean Middleware\",[2] but ease-of-use and speed of employment appear inversely proportional to the generality of such systems.[citation needed] Others[who?] cite the need for standard data interfaces to speed and simplify the integration process in practice. Handling higher-order information      Analysts experience difficulty — even with a functioning information integration system — in determining whether the sources in the database will satisfy a given application. Answering these kinds of questions about a set of repositories requires semantic information like metadata and/or ontologies. The few commercial tools[which?] that leverage this information remain in their infancy.  Applications  EII products enable loose coupling between homogeneous-data consuming client applications and services and heterogeneous-data stores. Such client applications and services include Desktop Productivity Tools (spreadsheets, word processors, presentation software, etc.), development environments and frameworks (Java EE, .NET, Mono, SOAP or RESTful Web services, etc.), business intelligence (BI), business activity monitoring (BAM) software, enterprise resource planning (ERP), Customer relationship management (CRM), business process management (BPM and/or BPEL) Software, and web content management (CMS). Data access technologies      XQuery and XQuery API for Java     Service Data Objects (SDO) for Java, C++ and .Net clients and any type of data source  See also      Big structure     Business Intelligence 2.0 (BI 2.0)     Data integration     Data warehouse     Disparate system     Enterprise integration     Federated database system     Resource Description Framework     Semantic heterogeneity     Semantic integration     Semantic Web     Web 2.0     Web services Enterprise integration is a technical field of Enterprise Architecture, which focused on the study of topics such as system interconnection, electronic data interchange, product data exchange and distributed computing environments.[1]  It is a concept in Enterprise engineering to provide the right information at the right place and at the right time and thereby enable communication between people, machines and computers and their efficient co-operation and co-ordination.[2]  Contents      1 Overview     2 History     3 Enterprise integration topics         3.1 Enterprise modeling         3.2 Enterprise integration needs         3.3 Identification and use of information         3.4 Transfer of information     4 Enterprise Integration Act of 2002     5 See also     6 References     7 Further reading     8 External links  Overview  Requirements and principles deal with determining the business drivers and guiding principles that help in the development of the enterprise architecture. Each functional and non-functional requirement should be traceable to one or more business drivers. Organizations are beginning to become more aware of the need for capturing and managing requirements. Use-case modeling is one of the techniques that is used for doing this. Enterprise Integration, according to Brosey et al. (2001), \"aims to connect and combines people, processes, systems, and technologies to ensure that the right people and the right processes have the right information and the right resources at the right time\".[3]  Enterprise Integration is focused on optimizing operations in a world which could be considered full of continuous and largely unpredictable change. Changes occur in single manufacturing companies just as well as in an \"everchanging set of extended or virtual enterprises\". It enables the actors to make \"quick and accurate decisions and adaptation of operations to respond to emerging threats and opportunities\".[3] History Evolution in Enterprise Integration: This figure summarizes these developments indicating the shift of emphasis from systems integration to enterprise integration with increasing focus on inter enterprise operations or networks.  Enterprise integration has been discussed since the early days of computers in industry and especially in the manufacturing industry with Computer Integrated Manufacturing (CIM) as the acronym for operations integration. In spite of the different understandings of the scope of integration in CIM it has always stood for information integration across at least parts of the enterprise. Information integration essentially consists of providing the right information, at the right place, at the right time.[4]  In the 1990s enterprise integration and enterprise engineering became a focal point of discussions with active contribution of many disciplines. The state of the art in enterprise engineering and integration by the end of the 1990s has been rather confusing, according to Jim Nell and Kurt Kosanke (1997):      On one hand, it claims to provide solutions for many of the issues identified in enterprise integration.     On the other hand, the solutions seem to compete with each other, use conflicting terminology and do not provide any clues on their relations to solutions on other issues.  Workflow modelling, business process modelling, business process reengineering (BPR), and concurrent engineering all aim toward identifying and providing the information needed in the enterprise operation. In addition, numerous integrating-platforms concepts are promoted with only marginal or no recognition or support of information identification. Tools claiming to support enterprise modelling exist in very large numbers, but the support is rather marginal, especially if models are to be used by the end user, for instance, in decision support. Enterprise integration topics Enterprise modeling  In his 1996 book \"Enterprise Modeling and Integration: Principles and Applications\" François Vernadat states, that \"enterprise modeling is concerned with assessing various aspects of an enterprise in order to better understand, restructure or design enterprise operations. It is the basis of business process reengineering and the first step to achieving enterprise integration. Enterprise integration according to Vernadat is a rapidly developing technical field which has already shown proven solutions for system interconnection, electronic data interchange, product data exchange and distributed computing environments. His book combines these two methodologies and advocates a systematic engineering approach called Enterprise Engineering, for modeling, analysing, designing and implementing integrated enterprise systems\".[5] Enterprise integration needs  With this understanding the different needs in enterprise integration can be identified:[4]      Identify the right information: requires a precise knowledge of the information needed and created by the different activities in the enterprise operation. Knowledge has to be structured in the form of an accurate model of the enterprise operation, which describes product and administrative information, resources and organisational aspects of the operational processes and allows what-if analysis in order to optimize these processes.     Provide the right information at the right place: requires information sharing systems and integration platforms capable of handling information transaction across heterogeneous environments consisting of heterogeneous hardware, different operating systems and monolithic software applications (legacy systems). Environments which cross organizational boundaries and link the operation of different organisations on a temporal basis and with short set-up times and limited time horizon (extended and virtual enterprises).     Update the information in real time to reflect the actual state of the enterprise operation: requires not only the up-date of the operational data (information created during the operation), but adapting to environmental changes, which may originate from new customer demands, new technology, new legislation or new philosophies of the society at large. Changes may require modification of the operational processes, the human organization or even the overall scope and goals of the enterprise.     Coordinate business processes: requires precise modelling of the enterprise operation in terms of business processes, their relations with each other, with information, resources and organisation. This goes far beyond exchange of information and information sharing. It takes into account decisional capabilities and know-how within the enterprise for real time decision support and evaluation of operational alternatives.     Organize and adapt the enterprise: requires very detailed and up-to-date knowledge of both the current state of the enterprise operation and its environment (market, technology, society). Knowledge has to be available a priori and very well structured to allow easy identification of and access to relevant information.  Identification and use of information Generalised Enterprise Reference Architecture and Methodology (GERAM) Framework for Enterprise Integration. Example of Enterprise integration: the US National Business Center's Human Resources Line of Business, Innovative Future Direction.  Explicit knowledge on information needs during the operation of the enterprise can be provided by a model of the operational processes. A model which identifies the operational tasks, their required information supply and removal needs as well as the point in time of required information transactions. In order to enable consistent modelling of the enterprise operation the modelling process has to be guided and supported by a reference architecture, a methodology and IT based tools.[6]  The Generalised Enterprise Reference Architecture and Methodology (GERAM) framework defined by the IFAC/IFIP Task Force provides the necessary guidance of the modelling process, see figure, and enables semantic unification of the model contents as well. The framework identifies the set of components necessary and helpful for enterprise modelling. The general concepts identified and defined in the reference architecture consist of life cycle, life history, model views among others. These concept help the user to create and maintain the process models of the operation and use them in her/his daily work. The modelling tools will support both model engineering and model use by providing an appropriate methodology and language for guiding the user and model representation, respectively.[6] Transfer of information  To enable an integrated real time support of the operation, both the process descriptions and the actual information have to be available in real time for decision support, operation monitoring and control, and model maintenance.[6]  The figure illustrates the concept of an integrating infrastructure linking the enterprise model to the real world systems. Integrating services act as a harmonising platform across the heterogeneous system environments (IT and others) and provide the necessary execution support for the model. The process dynamics captured in the enterprise model act as the control flow for model enactment. Therefore, access to information and its transfer to and from the location of use is controlled by the model and supported by the integrating infrastructure. The harmonising characteristics of the integrating infrastructure enables transfer of information across and beyond the organisation. Through the semantic unification of the modelling framework interoperability of enterprise models is assured as well.[6] Enterprise Integration Act of 2002  The Public Law 107-277 (116 Stat. 1936-1938), known as the Enterprise Integration Act of 2002, authorizes the National Institute of Standards and Technology to work with major manufacturing industries on an initiative of standards development and implementation for electronic enterprise integration, etc. It requires the Director of the National Institute of Standards and Technology (NIST) to establish an initiative for advancing enterprise integration within the United States which shall:[7]      involve the various units of NIST, including NIST laboratories, the Manufacturing Extension Partnership program, and the Baldrige Performance Excellence Program, and consortia that include government and industry;     build upon ongoing efforts of NIST and the private sector; and     address the enterprise integration needs of each major U.S. manufacturing industry at the earliest possible date.  See also      AMICE Consortium     Architecture of Integrated Information Systems     Architecture of Interoperable Information Systems     Integration Consortium     Canonical Model     CIMOSA     Configuration Management     Data integration     Enterprise application integration     Enterprise Information Integration     Generalised Enterprise Reference Architecture and Methodology     Semantic integration     Semantic Unification  Information integration (II) (also called deduplication and referential integrity) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in data mining and consolidation of data from unstructured or semi-structured resources. Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content. Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing uncertainty.[1][2]  An example of technologies available to integrate information include string metrics which allow the detection of similar text in different data sources by fuzzy matching. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.  Contents      1 See also     2 General references     3 Books     4 Citations     5 External links  See also      Data fusion (is a subset of Information integration)     Sensor fusion     Data integration     Data deduplication     Dataspaces     Referential integrity", "category": "Edison", "id": 117}
{"skillName": "DSDM05", "skillText": "Data format management (DFM) is the application of a systematic approach to the selection and use of the data formats used to encode information for storage on a computer.  In practical terms, data format management is the analysis of data formats and their associated technical, legal or economic attributes which can either enhance or detract from the ability of a digital asset or a given information systems to meet specified objectives.  Data format management is necessary as the amount of information and number of people creating it grows. This is especially the case as the information with which users are working is difficult to generate, store, costly to acquire, or to be shared.  Data format management as an analytic tool or approach is data format neutral.  Historically individuals, organization and businesses have been categorized by their type of computer or their operating system. Today, however, it is primarily productivity software, such as spreadsheet or word processor programs, and the way these programs store information that also defines an entity. For instance, when browsing the web it is not important which kind of computer is responsible for hosting a site, only that the information it publishes is in a format that is readable by the viewing browser. In this instance the data format of the published information has more to do with defining compatibilities than the underlying hardware or operating system.  Several initiatives have been established to record those data formats commonly used and the software available to read them, for example the Pronom project at the UK National Archives. See also      Digital preservation     File format     Information technology governance     National Digital Library Program (NDLP)     National Digital Information Infrastructure and Preservation Program (NDIIPP)   In library and archival science, digital preservation is a formal endeavor to ensure that digital information of continuing value remains accessible and usable.[1] It involves planning, resource allocation, and application of preservation methods and technologies,[2] and it combines policies, strategies and actions to ensure access to reformatted and \"born-digital\" content, regardless of the challenges of media failure and technological change. The goal of digital preservation is the accurate rendering of authenticated content over time.[3] According to the Harrod's Librarian Glossary, digital preservation is the method of keeping digital material alive so that they remain usable as technological advances render original hardware and software specification obsolete (Nabeela). [4]  Contents      1 Preservation fundamentals         1.1 Appraisal         1.2 Identification (identifiers and descriptive metadata)         1.3 Integrity             1.3.1 Fixity         1.4 Characterization         1.5 Sustainability             1.5.1 Renderability             1.5.2 Physical media obsolescence             1.5.3 Format obsolescence             1.5.4 Significant properties         1.6 Authenticity         1.7 Access         1.8 Preservation metadata     2 Intellectual foundations of digital preservation         2.1 Preserving Digital Information (1996)         2.2 OAIS         2.3 Trusted Digital Repository Model         2.4 InterPARES     3 Challenges of digital preservation     4 Strategies         4.1 Refreshing         4.2 Migration         4.3 Replication         4.4 Emulation         4.5 Encapsulation         4.6 Persistent Archives concept         4.7 Metadata attachment     5 Preservation repository assessment and certification         5.1 Specific tools and methodologies             5.1.1 TRAC             5.1.2 DRAMBORA             5.1.3 European Framework for Audit and Certification of Digital Repositories             5.1.4 nestor Catalogue of Criteria             5.1.5 PLANETS Project             5.1.6 PLATTER             5.1.7 Audit and Certification of Trustworthy Digital Repositories (ISO 16363)     6 Digital preservation best practices         6.1 Audio preservation         6.2 Moving image preservation         6.3 Email preservation         6.4 Video game preservation         6.5 Personal archiving     7 Education for digital preservation     8 Examples of digital preservation initiatives     9 Large-scale digital preservation initiatives     10 See also     11 Footnotes     12 References     13 External links  Preservation fundamentals Appraisal  Archival appraisal (or, alternatively, selection[5]) refers to the process of identifying records and other materials to be preserved by determining their permanent value. Several factors are usually considered when making this decision.[6] It is a difficult and critical process because the remaining selected records will shape researchers’ understanding of that body of records, or fonds. Appraisal is identified as A4.2 within the Chain of Preservation (COP) model[7] created by the InterPARES 2 project.[8] Archival appraisal is not the same as monetary appraisal, which determines fair market value.  Archival appraisal may be performed once or at the various stages of acquisition and processing. Macro appraisal,[9] a functional analysis of records at a high level, may be performed even before the records have been acquired to determine which records to acquire. More detailed, iterative appraisal may be performed while the records are being processed.  Appraisal is performed on all archival materials, not just digital. It has been proposed that, in the digital context, it might be desirable to retain more records than have traditionally been retained after appraisal of analog records, primarily due to a combination of the declining cost of storage and the availability of sophisticated discovery tools which will allow researchers to find value in records of low information density.[10][11] In the analog context, these records may have been discarded or only a representative sample kept. However, the selection, appraisal, and prioritization of materials must be carefully considered in relation to the ability of an organization to responsibly manage the totality of these materials.  Often libraries, and to a lesser extent, archives, are offered the same materials in several different digital or analog formats. They prefer to select the format that they feel has the greatest potential for long-term preservation of the content. The Library of Congress has created a set of recommended formats for long-term preservation.[12] They would be used, for example, if the Library was offered items for copyright deposit directly from a publisher. Identification (identifiers and descriptive metadata)  In digital preservation and collection management, discovery and identification of objects is aided by the use of assigned identifiers and accurate descriptive metadata. An identifier is a unique label that is used to reference an object or record, usually manifested as a number or string of numbers and letters. As a crucial element of metadata to be included in a database record or inventory, it is used in tandem with other descriptive metadata to differentiate objects and their various instantiations.[13]  Descriptive metadata refers to information about an object's content such as title, creator, subject, date etc...[13] Determination of the elements used to describe an object are facilitated by the use of a metadata schema.  Another common type of file identification is the filename. Implementing a file naming protocol is essential to maintaining consistency and efficient discovery and retrieval of objects in a collection, and is especially applicable during digitization of analog media. Using a file naming convention, such as the 8.3 filename, will ensure compatibility with other systems and facilitate migration of data, and deciding between descriptive (containing descriptive words and numbers) and non-descriptive (often randomly generated numbers) file names is generally determined by the size and scope of a given collection.[14] However, filenames are not good for semantic identification, because they are non-permanent labels for a specific location on a system and can be modified without affecting the bit-level profile of a digital file. Integrity  Data integrity provides the cornerstone of digital preservation, representing the intent to “ensure data is recorded exactly as intended [...] and upon later retrieval, ensure the data is the same as it was when it was originally recorded.” Unintentional changes to data are to be avoided, and responsible strategies put in place to detect unintentional changes and react as appropriately determined.  However, digital preservation efforts may necessitate modifications to content or metadata through responsibly-developed procedures and by well-documented policies. Organizations or individuals may choose to retain original, integrity-checked versions of content and/or modified versions with appropriate preservation metadata. Data integrity practices also apply to modified versions, as their state of capture must be maintained and resistant to unintentional modifications. Fixity  File fixity is the property of a digital file being fixed, or unchanged. File fixity checking is the process of validating that a file has not changed or been altered from a previous state.[15] This effort is often enabled by the creation, validation, and management of checksums.  While checksums are the primary mechanism for monitoring fixity at the individual file level, an important additional consideration for monitoring fixity is file attendance. Whereas checksums identify if a file has changed, file attendance identifies if a file in a designated collection is newly created, deleted, or moved. Tracking and reporting on file attendance is a fundamental component of digital collection management and fixity. Characterization  Characterization of digital materials is the identification and description of what a file is and of its defining technical characteristics [16] often captured by technical metadata, which records its technical attributes like creation or production environment.[17] Sustainability  Digital sustainability encompasses a range of issues and concerns that contribute to the longevity of digital information.[18] Unlike traditional, temporary strategies, and more permanent solutions, digital sustainability implies a more active and continuous process. Digital sustainability concentrates less on the solution and technology and more on building an infrastructure and approach that is flexible with an emphasis on interoperability, continued maintenance and continuous development.[19] Digital sustainability incorporates activities in the present that will facilitate access and availability in the future.[20][21] The ongoing maintenance necessary to digital preservation is analogous to the successful, centuries-old, community upkeep of the Uffington White Horse (according to Stuart M. Shieber) or the Ise Grand Shrine (according to Jeffrey Schnapp).[22][23] Renderability  Renderability refers to the continued ability to use and access a digital object while maintaining its inherent significant properties.[24] Physical media obsolescence  Physical media obsolescence can occur when access to digital content requires external dependencies that are no longer manufactured, maintained, or supported. External dependencies can refer to hardware, software, or physical carriers. Format obsolescence  File format obsolescence can occur when adoption of new encoding formats supersedes use of existing formats, or when associated presentation tools are no longer readily available.[25]  Factors that should enter consideration when selecting sustainable file formats include disclosure, adoption, transparency, self-documentation, external dependencies, impact of patents, and technical protection mechanisms.[26]  Formats proprietary to one software vendor are more likely to be affected by format obsolescence. Well-used standards such as Unicode and JPEG are more likely to be readable in future. Significant properties  Significant properties refer to the \"essential attributes of a digital object which affect its appearance, behavior, quality and usability\" and which \"must be preserved over time for the digital object to remain accessible and meaningful.\"[27]  \"Proper understanding of the significant properties of digital objects is critical to establish best practice approaches to digital preservation. It assists appraisal and selection, processes in which choices are made about which significant properties of digital objects are worth preserving; it helps the development of preservation metadata, the assessment of different preservation strategies and informs future work on developing common standards across the preservation community.\"[28] Authenticity  Whether analog or digital, archives strive to maintain records as trustworthy representations of what was originally received. Authenticity has been defined as “. . . the trustworthiness of a record as a record; i.e., the quality of a record that is what it purports to be and that is free from tampering or corruption”.[29] Authenticity should not be confused with accuracy;[30] an inaccurate record may be acquired by an archives and have its authenticity preserved. The content and meaning of that inaccurate record will remain unchanged.  A combination of policies, security procedures, and documentation can be used to ensure and provide evidence that the meaning of the records has not been altered while in the archives’ custody. Access  Digital preservation efforts are largely to enable decision-making in the future. Should an archive or library choose a particular strategy to enact, the content and associated metadata must persist to allow for actions to be taken or not taken at the discretion of the controlling party. Preservation metadata  Preservation metadata is a key component of digital preservation, and includes information that documents the preservation process. It supports collection management practices and allows organizations or individuals to understand the chain of custody. Preservation Metadata: Implementation Strategies (PREMIS), an international working group, sought to “define implementable, core preservation metadata, with guidelines/recommendations” to support digital preservation efforts by clarifying what the metadata is and its usage. Intellectual foundations of digital preservation Preserving Digital Information (1996)  The challenges of long-term preservation of digital information have been recognized by the archival community for years.[31] In December 1994, the Research Libraries Group (RLG) and Commission on Preservation and Access (CPA) formed a Task Force on Archiving of Digital Information with the main purpose of investigating what needed to be done to ensure long-term preservation and continued access to the digital records. The final report published by the Task Force (Garrett, J. and Waters, D., ed. (1996). “Preserving digital information: Report of the task force on archiving of digital information.”[32]) became a fundamental document in the field of digital preservation that helped set out key concepts, requirements, and challenges.[31][33]  The Task Force proposed development of a national system of digital archives that would take responsibility for long-term storage and access to digital information; introduced the concept of trusted digital repositories and defined their roles and responsibilities; identified five features of digital information integrity (content, fixity, reference, provenance, and context) that were subsequently incorporated into a definition of Preservation Description Information in the Open Archival Information System Reference Model; and defined migration as a crucial function of digital archives. The concepts and recommendations outlined in the report laid a foundation for subsequent research and digital preservation initiatives.[34][35] OAIS  To standardize digital preservation practice and provide a set of recommendations for preservation program implementation, the Reference Model for an Open Archival Information System (OAIS) was developed. OAIS is concerned with all technical aspects of a digital object’s life cycle: ingest, archival storage, data management, administration, access and preservation planning.[36] The model also addresses metadata issues and recommends that five types of metadata be attached to a digital object: reference (identification) information, provenance (including preservation history), context, fixity (authenticity indicators), and representation (formatting, file structure, and what \"imparts meaning to an object’s bitstream\").[37] Trusted Digital Repository Model  In March 2000, the Research Libraries Group (RLG) and Online Computer Library Center (OCLC) began a collaboration to establish attributes of a digital repository for research organizations, building on and incorporating the emerging international standard of the Reference Model for an Open Archival Information System (OAIS). In 2002, they published “Trusted Digital Repositories: Attributes and Responsibilities.” In that document a “Trusted Digital Repository” (TDR) is defined as \"one whose mission is to provide reliable, long-term access to managed digital resources to its designated community, now and in the future.\" The TDR must include the following seven attributes: compliance with the reference model for an Open Archival Information System (OAIS), administrative responsibility, organizational viability, financial sustainability, technological and procedural suitability, system security, procedural accountability. The Trusted Digital Repository Model outlines relationships among these attributes. The report also recommended the collaborative development of digital repository certifications, models for cooperative networks, and sharing of research and information on digital preservation with regard to intellectual property rights.[38]  In 2004 Henry M. Gladney proposed another approach to digital object preservation that called for the creation of “Trustworthy Digital Objects” (TDOs). TDOs are digital objects that can speak to their own authenticity since they incorporate a record maintaining their use and change history, which allows the future users to verify that the contents of the object are valid.[39] InterPARES  International Research on Permanent Authentic Records in Electronic Systems (InterPARES) is a collaborative research initiative led by the University of British Columbia that is focused on addressing issues of long-term preservation of authentic digital records. The research is being conducted by focus groups from various institutions in North America, Europe, Asia, and Australia, with an objective of developing theories and methodologies that provide the basis for strategies, standards, policies, and procedures necessary to ensure the trustworthiness, reliability, and accuracy of digital records over time.[40]  Under the direction of archival science professor Luciana Duranti, the project began in 1999 with the first phase, InterPARES 1, which ran to 2001 and focused on establishing requirements for authenticity of inactive records generated and maintained in large databases and document management systems created by government agencies.[41] InterPARES 2 (2002–2007) concentrated on issues of reliability, accuracy and authenticity of records throughout their whole life cycle, and examined records produced in dynamic environments in the course of artistic, scientific and online government activities.[42] The third five-year phase (InterPARES 3) was initiated in 2007. Its goal is to utilize theoretical and methodological knowledge generated by InterPARES and other preservation research projects for developing guidelines, action plans, and training programs on long-term preservation of authentic records for small and medium-sized archival organizations.[43] Challenges of digital preservation  Society's heritage has been presented on many different materials, including stone, vellum, bamboo, silk, and paper. Now a large quantity of information exists in digital forms, including emails, blogs, social networking websites, national elections websites, web photo albums, and sites which change their content over time.[44] With digital media it is easier to create content and keep it up-to-date, but at the same time there are many challenges in the preservation of this content, both technical and economic.[45]  Unlike traditional analog objects such as books or photographs where the user has unmediated access to the content, a digital object always needs a software environment to render it. These environments keep evolving and changing at a rapid pace, threatening the continuity of access to the content.[46] Physical storage media, data formats, hardware, and software all become obsolete over time, posing significant threats to the survival of the content.[3] This process can be referred to as digital obsolescence.  In the case of born-digital content (e.g., institutional archives, Web sites, electronic audio and video content, born-digital photography and art, research data sets, observational data), the enormous and growing quantity of content presents significant scaling issues to digital preservation efforts. Rapidly changing technologies can hinder digital preservationists work and techniques due to outdated and antiquated machines or technology. This has become a common problem and one that is a constant worry for a digital archivist—how to prepare for the future.  Digital content can also present challenges to preservation because of its complex and dynamic nature, e.g., interactive Web pages, virtual reality and gaming environments,[47] learning objects, social media sites.[48] In many cases of emergent technological advances there are substantial difficulties in maintaining the authenticity, fixity, and integrity of objects over time deriving from the fundamental issue of experience with that particular digital storage medium and while particular technologies may prove to be more robust in terms of storage capacity, there are issues in securing a framework of measures to ensure that the object remains fixed while in stewardship.[2]  For the preservation of software as digital content, a specific challenge is the typically non-availability of the source code as commercial software is normally distributed only in compiled binary form. Without the source code an adaption (Porting) on modern computing hardware or operating system is most often impossible, therefore the original hardware and software context needs to be emulated. Another potential challenge for software preservation can be the copyright which prohibits often the bypassing of copy protection mechanisms (Digital Millennium Copyright Act) in case software has become an orphaned work (Abandonware). An exemption from the United States Digital Millennium Copyright Act to permit to bypass copy protection was approved in 2003 for a period of 3 years to the Internet Archive who created an archive of \"vintage software\", as a way to preserve them.[49][50] The exemption was renewed in 2006, and as of 27 October 2009, has been indefinitely extended pending further rulemakings[51] \"for the purpose of preservation or archival reproduction of published digital works by a library or archive.\"[52]  Another challenge surrounding preservation of digital content resides in the issue of scale. The amount of digital information being created along with the \"proliferation of format types\" [2] makes creating trusted digital repositories with adequate and sustainable resources a challenge. The Web is only one example of what might be considered the \"data deluge\".[2] For example, the Library of Congress currently amassed 170 billion tweets between 2006 and 2010 totaling 133.2 terabytes[53] and each Tweet is composed of 50 fields of metadata.[54]  The economic challenges of digital preservation are also great. Preservation programs require significant up front investment to create, along with ongoing costs for data ingest, data management, data storage, and staffing. One of the key strategic challenges to such programs is the fact that, while they require significant current and ongoing funding, their benefits accrue largely to future generations.[55] Strategies  In 2006, the Online Computer Library Center developed a four-point strategy for the long-term preservation of digital objects that consisted of:      Assessing the risks for loss of content posed by technology variables such as commonly used proprietary file formats and software applications.     Evaluating the digital content objects to determine what type and degree of format conversion or other preservation actions should be applied.     Determining the appropriate metadata needed for each object type and how it is associated with the objects.     Providing access to the content.[56]  There are several additional strategies that individuals and organizations may use to actively combat the loss of digital information. Refreshing  Refreshing is the transfer of data between two types of the same storage medium so there are no bitrot changes or alteration of data.[37] For example, transferring census data from an old preservation CD to a new one. This strategy may need to be combined with migration when the software or hardware required to read the data is no longer available or is unable to understand the format of the data. Refreshing will likely always be necessary due to the deterioration of physical media. Migration  Migration is the transferring of data to newer system environments (Garrett et al., 1996). This may include conversion of resources from one file format to another (e.g., conversion of Microsoft Word to PDF or OpenDocument) or from one operating system to another (e.g., Windows to Linux) so the resource remains fully accessible and functional. Two significant problems face migration as a plausible method of digital preservation in the long terms. Due to the fact that digital objects are subject to a state of near continuous change, migration may cause problems in relation to authenticity and migration has proven to be time-consuming and expensive for \"large collections of heterogeneous objects, which would need constant monitoring and intervention.[2] Migration can be a very useful strategy for preserving data stored on external storage media (e.g. CDs, USB flash drives, and 3.5” floppy disks). These types of devices are generally not recommended for long-term use, and the data can become inaccessible due to media and hardware obsolescence or degradation.[57] Replication  Creating duplicate copies of data on one or more systems is called replication. Data that exists as a single copy in only one location is highly vulnerable to software or hardware failure, intentional or accidental alteration, and environmental catastrophes like fire, flooding, etc. Digital data is more likely to survive if it is replicated in several locations. Replicated data may introduce difficulties in refreshing, migration, versioning, and access control since the data is located in multiple places.  Understanding digital preservation means comprehending how digital information is produced and reproduced. Because digital information (e.g., a file) can be exactly replicated down to the bit level, it is possible to create identical copies of data. Exact duplicates allow archives and libraries to manage, store, and provide access to identical copies of data across multiple systems and/or environments. Emulation  Emulation is the replicating of functionality of an obsolete system. According to van der Hoeven, \"Emulation does not focus on the digital object, but on the hard- and software environment in which the object is rendered. It aims at (re)creating the environment in which the digital object was originally created.\".[58] Examples are having the ability to replicate or imitate another operating system.[59] Examples include emulating an Atari 2600 on a Windows system or emulating WordPerfect 1.0 on a Macintosh. Emulators may be built for applications, operating systems, or hardware platforms. Emulation has been a popular strategy for retaining the functionality of old video game systems, such as with the MAME project. The feasibility of emulation as a catch-all solution has been debated in the academic community. (Granger, 2000)  Raymond A. Lorie has suggested a Universal Virtual Computer (UVC) could be used to run any software in the future on a yet unknown platform.[60] The UVC strategy uses a combination of emulation and migration. The UVC strategy has not yet been widely adopted by the digital preservation community.  Jeff Rothenberg, a major proponent of Emulation for digital preservation in libraries, working in partnership with Koninklijke Bibliotheek and National Archief of the Netherlands, developed a software program called Dioscuri, a modular emulator that succeeds in running MS-DOS, WordPerfect 5.1, DOS games, and more.[61]  Another example of emulation as a form of digital preservation can be seen in the example of Emory University and the Salman Rushdie's papers. Rushdie donated an outdated computer to the Emory University library, which was so old that the library was unable to extract papers from the harddrive. In order to procure the papers, the library emulated the old software system and was able to take the papers off his old computer.[62] Encapsulation  This method maintains that preserved objects should be self-describing, virtually \"linking content with all of the information required for it to be deciphered and understood\".[2] The files associated with the digital object would have details of how to interpret that object by using \"logical structures called \"containers\" or \"wrappers\" to provide a relationship between all information components[63] that could be used in future development of emulators, viewers or converters through machine readable specifications.[64] The method of encapsulation is usually applied to collections that will go unused for long periods of time.[64] Persistent Archives concept  Developed by the San Diego Supercomputing Center and funded by the National Archives and Records Administration, this method requires the development of comprehensive and extensive infrastructure that enables \"the preservation of the organisation of collection as well as the objects that make up that collection, maintained in a platform independent form\".[2] A persistent archive includes both the data constituting the digital object and the context that the defines the provenance, authenticity, and structure of the digital entities.[65] This allows for the replacement of hardware or software components with minimal effect on the preservation system. This method can be based on virtual data grids and resembles OAIS Information Model (specifically the Archival Information Package). Metadata attachment  Metadata is data on a digital file that includes information on creation, access rights, restrictions, preservation history, and rights management.[66] Metadata attached to digital files may be affected by file format obsolescence. ASCII is considered to be the most durable format for metadata [67] because it is widespread, backwards compatible when used with Unicode, and utilizes human-readable characters, not numeric codes. It retains information, but not the structure information it is presented in. For higher functionality, SGML or XML should be used. Both markup languages are stored in ASCII format, but contain tags that denote structure and format. Preservation repository assessment and certification  A few of the major frameworks for digital preservation repository assessment and certification are described below. A more detailed list is maintained by the U.S. Center for Research Libraries.[68] Specific tools and methodologies TRAC  In 2007, CRL/OCLC published Trustworthy Repositories Audit & Certification: Criteria & Checklist (TRAC), a document allowing digital repositories to assess their capability to reliably store, migrate, and provide access to digital content. TRAC is based upon existing standards and best practices for trustworthy digital repositories and incorporates a set of 84 audit and certification criteria arranged in three sections: Organizational Infrastructure; Digital Object Management; and Technologies, Technical Infrastructure, and Security.[69]  TRAC \"provides tools for the audit, assessment, and potential certification of digital repositories, establishes the documentation requirements required for audit, delineates a process for certification, and establishes appropriate methodologies for determining the soundness and sustainability of digital repositories\".[70] DRAMBORA  Digital Repository Audit Method Based On Risk Assessment (DRAMBORA), introduced by the Digital Curation Centre (DCC) and DigitalPreservationEurope (DPE) in 2007, offers a methodology and a toolkit for digital repository risk assessment.[71] The tool enables repositories to either conduct the assessment in-house (self-assessment) or to outsource the process.  The DRAMBORA process is arranged in six stages and concentrates on the definition of mandate, characterization of asset base, identification of risks and the assessment of likelihood and potential impact of risks on the repository. The auditor is required to describe and document the repository’s role, objectives, policies, activities and assets, in order to identify and assess the risks associated with these activities and assets and define appropriate measures to manage them.[72] European Framework for Audit and Certification of Digital Repositories  The European Framework for Audit and Certification of Digital Repositories  was defined in a memorandum of understanding signed in July 2010 between Consultative Committee for Space Data Systems (CCSDS), Data Seal of Approval (DSA) Board and German Institute for Standardization (DIN) \"Trustworthy Archives – Certification\" Working Group.  The framework is intended to help organizations in obtaining appropriate certification as a trusted digital repository and establishes three increasingly demanding levels of assessment:      Basic Certification: self-assessment using 16 criteria of the Data Seal of Approval (DSA).     Extended Certification: Basic Certification and additional externally reviewed self-audit against ISO 16363 or DIN 31644 requirements.     Formal Certification: validation of the self-certification with a third-party official audit based on ISO 16363 or DIN 31644.[73]  nestor Catalogue of Criteria  A German initiative, nestor  (the Network of Expertise in Long-Term Storage of Digital Resources) sponsored by the German Ministry of Education and Research, developed a catalogue of criteria for trusted digital repositories in 2004. In 2008 the second version of the document was published. The catalogue, aiming primarily at German cultural heritage and higher education institutions, establishes guidelines for planning, implementing, and self-evaluation of trustworthy long-term digital repositories.[74]  The nestor catalogue of criteria conforms to the OAIS reference model terminology and consists of three sections covering topics related to Organizational Framework, Object Management, and Infrastructure and Security.[75] PLANETS Project  In 2002 the Preservation and Long-term Access through Networked Services (PLANETS) project, part of the EU Framework Programmes for Research and Technological Development 6, addressed core digital preservation challenges. The primary goal for Planets was to build practical services and tools to help ensure long-term access to digital cultural and scientific assets. The Open Planets project ended May 31, 2010.[76] The outputs of the project are now sustained by the follow-on organisation, the Open Planets Foundation.[77][78] On October 7, 2014 the Open Planets Foundation announced that it would be renamed the Open Preservation Foundation to align with the organization's current direction.[79] PLATTER  Planning Tool for Trusted Electronic Repositories (PLATTER) is a tool released by DigitalPreservationEurope (DPE) to help digital repositories in identifying their self-defined goals and priorities in order to gain trust from the stakeholders.[80]  PLATTER is intended to be used as a complementary tool to DRAMBORA, NESTOR, and TRAC. It is based on ten core principles for trusted repositories and defines nine Strategic Objective Plans, covering such areas as acquisition, preservation and dissemination of content, finance, staffing, succession planning, technical infrastructure, data and metadata specifications, and disaster planning. The tool enables repositories to develop and maintain documentation required for an audit.[81] Audit and Certification of Trustworthy Digital Repositories (ISO 16363)  Audit and Certification of Trustworthy Digital Repositories (ISO 16363:2012), developed by the Consultative Committee for Space Data Systems (CCSDS), was approved as a full international standard in March 2012. Extending the OAIS Reference Model and based largely on the TRAC checklist, the standard is designed for all types of digital repositories. It provides a detailed specification of criteria against which the trustworthiness of a digital repository should be evaluated.[82]  The CCSDS Repository Audit and Certification Working Group has also developed and submitted for approval a second standard, Requirements for Bodies Providing Audit and Certification of Candidate Trustworthy Digital Repositories (ISO 16919), that defines the external auditing process and requirements for organizations responsible for assessment and certification of digital repositories.[83] Digital preservation best practices  Although preservation strategies vary for different types of materials and between institutions, adhering to nationally and internationally recognized standards and practices is a crucial part of digital preservation activities. Best or recommended practices define strategies and procedures that may help organizations to implement existing standards or provide guidance in areas where no formal standards have been developed.[84]  Best practices in digital preservation continue to evolve and may encompass processes that are performed on content prior to or at the point of ingest into a digital repository as well as processes performed on preserved files post-ingest over time. Best practices may also apply to the process of digitizing analog material and may include the creation of specialized metadata (such as technical, administrative and rights metadata) in addition to standard descriptive metadata. The preservation of born-digital content may include format transformations to facilitate long-term preservation or to provide better access.[85] Audio preservation  Various best practices and guidelines for digital audio preservation have been developed, including:      Guidelines on the Production and Preservation of Digital Audio Objects IASA-TC 04 (2009),[86] which sets out the international standards for optimal audio signal extraction from a variety of audio source materials, for analogue to digital conversion and for target formats for audio preservation     Capturing Analog Sound for Digital Preservation: Report of a Roundtable Discussion of Best Practices for Transferring Analog Discs and Tapes (2006),[87] which defined procedures for reformatting sound from analog to digital and provided recommendations for best practices for digital preservation     Digital Audio Best Practices (2006) prepared by the Collaborative Digitization Program Digital Audio Working Group, which covers best practices and provides guidance both on digitizing existing analog content and on creating new digital audio resources[88]     Sound Directions: Best Practices for Audio Preservation (2007) published by the Sound Directions Project,[84] which describes the audio preservation workflows and recommended best practices and has been used as the basis for other projects and initiatives[89][90]     Documents developed by the International Association of Sound and Audiovisual Archives (IASA), the European Broadcasting Union (EBU), the Library of Congress, and the Digital Library Federation (DLF).  The Audio Engineering Society (AES) also issues a variety of standards and guidelines relating to the creation of archival audio content and technical metadata.[91] Moving image preservation  The term \"moving images\" includes analog film and video and their born-digital forms: digital video, digital motion picture materials, and digital cinema. As analog videotape and film become obsolete, digitization has become a key preservation strategy, although many archives do continue to perform photochemical preservation of film stock.[92][93]  \"Digital preservation\" has a double meaning for audiovisual collections: analog originals are preserved through digital reformatting, with the resulting digital files preserved; and born-digital content is collected, most often in proprietary formats that pose problems for future digital preservation.  There is currently no broadly accepted standard target digital preservation format for analog moving images.[94]  The following resources offer information on analog to digital reformatting and preserving born-digital audiovisual content.      The Library of Congress tracks the sustainability of digital formats, including moving images.[95]     The Digital Dilemma 2: Perspectives from Independent Filmmakers, Documentarians and Nonprofit Audiovisual Archives (2012).[94] The section on nonprofit archives reviews common practices on digital reformatting, metadata, and storage. There are four case studies.     Federal Agencies Digitization Guidelines Initiative (FADGI)      . Started in 2007, this is a collaborative effort by federal agencies to define common guidelines, methods, and practices for digitizing historical content. As part of this, two working groups are studying issues specific to two major areas, Still Image and Audio Visual.[96]     PrestoCenter publishes general audiovisual information and advice at a European level. Its online library has research and white papers on digital preservation costs and formats.[97]     The Association of Moving Image Archivists (AMIA) sponsors conferences, symposia, and events on all aspects of moving image preservation, including digital. The AMIA Tech Review      contains articles reflecting current thoughts and practices from the archivists’ perspectives. Video Preservation for the Millennia (2012), published in the AMIA Tech Review, details the various strategies and ideas behind the current state of video preservation.[98]  Email preservation  Email poses special challenges for preservation: email client software varies widely; there is no common structure for email messages; email often communicates sensitive information; individual email accounts may contain business and personal messages intermingled; and email may include attached documents in a variety of file formats. Email messages can also carry viruses or have spam content. While email transmission is standardized, there is no formal standard for the long-term preservation of email messages.[99]  Approaches to preserving email may vary according to the purpose for which it is being preserved. For businesses and government entities, email preservation may be driven by the need to meet retention and supervision requirements for regulatory compliance and to allow for legal discovery. (Additional information about email archiving approaches for business and institutional purposes may be found under the separate article, Email archiving.) For research libraries and archives, the preservation of email that is part of born-digital or hybrid archival collections has as its goal ensuring its long-term availability as part of the historical and cultural record.[100]  Several projects developing tools and methodologies for email preservation have been conducted based on various preservation strategies: normalizing email into XML format, migrating email to a new version of the software and emulating email environments: Memories Using Email  (MUSE), Collaborative Electronic Records Project  (CERP), E-Mail Collection And Preservation  (EMCAP), PeDALS Email Extractor Software  (PeDALS), XML Electronic Normalizing of Archives tool  (XENA).  Some best practices and guidelines for email preservation can be found in the following resources:      Curating E-Mails: A Life-cycle Approach to the Management and Preservation of E-mail Messages (2006) by Maureen Pennock.[101]     Technology Watch Report 11-01: Preserving Email (2011) by Christopher J Prom.[100]     Best Practices: Email Archiving by Jo Maitland.[102]  Video game preservation  In 2007 the Keeping Emulation Environments Portable (KEEP) project, part of the EU Framework Programmes for Research and Technological Development 7, developed tools and methodologies to keep digital software objects available in their original context. Digital software objects as video games might get lost because of digital obsolescence and non-availability of required legacy hardware or operating system software; such software is referred to as abandonware. Because the source code is often not available any longer,[47] emulation is the only preservation opportunity. KEEP provided an emulation framework to help the creation of such emulators. KEEP was developed by Vincent Joguin, first launched in February 2009 and was coordinated by Elisabeth Freyre of the French National Library.[103]  In January 2012 the POCOS project funded by JISC organised a workshop on the preservation of gaming environments and virtual worlds.[104] Personal archiving  There are many things consumers and artists can do themselves to help care for their collections at home.      The Software Preservation Society is a group of computer enthusiasts that is concentrating on finding old software disks (mostly games) and taking a snapshot of the disks in a format that can be preserved for the future.     \"Resource Center: Caring For Your Treasures\" by American Institute for Conservation of Historic and Artistic Works details simple strategies for artists and consumers to care for and preserve their work themselves.[105]  The Library of Congress also hosts a list for the self-preserver which includes direction toward programs and guidelines from other institutions that will help the user preserve social media, email, and formatting general guidelines (such as caring for CDs).[106] Some of the programs listed include:      HTTrack Website Copier      : Software tool which allows the user to download a World Wide Web site from the Internet to a local directory, building recursively all directories, getting HTML, images, and other files from the server to their computer.     Muse      : Muse (short for Memories Using Email) is a program that helps users revive memories, using their long-term email archives, run by Stanford University.  Education for digital preservation  The Digital Preservation Outreach and Education (DPOE), as part of the Library of Congress, serves to foster preservation of digital content through a collaborative network of instructors and collection management professionals working in cultural heritage institutions. Composed of Library of Congress staff, the National Trainer Network, the DPOE Steering Committee, and a community of Digital Preservation Education Advocates, as of 2013 the DPOE has 24 working trainers across the six regions of the United States.[107] In 2010 the DPOE conducted an assessment, reaching out to archivists, librarians, and other information professionals around the country. A working group of DPOE instructors then developed a curriculum [108] based on the assessment results and other similar digital preservation curricula designed by other training programs, such as LYRASIS, Educopia Institute, MetaArchive Cooperative, University of North Carolina, DigCCurr (Digital Curation Curriculum) and Cornell University-ICPSR Digital Preservation Management Workshops. The resulting core principles are also modeled on the principles outlined in \"A Framework of Guidance for Building Good Digital Collections\" by the National Information Standards Organization (NISO).[109]  In Europe, Humboldt-Universität zu Berlin and King's College London offer a joint program in Digital Curation  that emphasizes both digital humanities and the technologies necessary for long term curation. The MSc in Information Management and Preservation (Digital)  offered by the HATII at the University of Glasgow has been running since 2005 and is the pioneering program in the field. Examples of digital preservation initiatives For more details on this topic, see List of digital preservation initiatives. Digitization at the British Library of a Dunhuang manuscript for the International Dunhuang Project      The Library of Congress operates the National Digital Stewardship Alliance      The British Library is responsible for several programmes in the area of digital preservation and is a founding member of the Digital Preservation Coalition      and Open Preservation Foundation      . Their digital preservation strategy      is publicly available. The National Archives of the United Kingdom have also pioneered various initiatives in the field of digital preservation.  A number of open source products have been developed to assist with digital preservation, including Archivematica, DSpace, Fedora Commons, OPUS, SobekCM and EPrints. The commercial sector also offers digital preservation software tools, such as Ex Libris Ltd.'s Rosetta, Preservica's Cloud, Standard and Enterprise Editions, CONTENTdm, Digital Commons, Equella, intraLibrary, Open Repository and Vital.[110] Large-scale digital preservation initiatives  Many research libraries and archives have begun or are about to begin large-scale digital preservation initiatives (LSDIs). The main players in LSDIs are cultural institutions, commercial companies such as Google and Microsoft, and non-profit groups including the Open Content Alliance (OCA), the Million Book Project (MBP), and HathiTrust. The primary motivation of these groups is to expand access to scholarly resources.  Approximately 30 cultural entities, including the 12-member Committee on Institutional Cooperation (CIC), have signed digitization agreements with either Google or Microsoft. Several of these cultural entities are participating in the Open Content Alliance and the Million Book Project. Some libraries are involved in only one initiative and others have diversified their digitization strategies through participation in multiple initiatives. The three main reasons for library participation in LSDIs are: access, preservation, and research and development. It is hoped that digital preservation will ensure that library materials remain accessible for future generations. Libraries have a perpetual responsibility for their materials and a commitment to archive their digital materials. Libraries plan to use digitized copies as backups for works in case they go out of print, deteriorate, or are lost and damaged. See also      Backup     Charles M. Dollar     Data curation     Database preservation     Digital artifactual value     Digital asset management     Digital curation     Digital continuity     Digital dark age     Digital library     Digital obsolescence     Digital reformatting     DRAMBORA     Enterprise content management     ENUMERATE     File format     HD-Rosetta     Information Lifecycle Management     List of digital preservation initiatives     New media art preservation     Margaret Hedstrom     Preservation metadata     Section 108 Study Group     Seamus Ross     Trustworthy Repositories Audit & Certification     UVC-based preservation     Web archiving   A data steward is a person responsible for the management and fitness of data elements (also known as critical data elements) - both the content and metadata. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a data custodian.  The overall objective of a Data Steward is metadata management, in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules.  Data stewards begin the stewarding process with the identification of the elements which they will steward, with the ultimate result being standards, controls and data entry.[citation needed] The steward works closely with business glossary standards analysts (for standards), with data architect/modelers (for standards), with DQ analysts (for controls) and with operations team members (good-quality data going in per business rules) while entering data.  Data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.[citation needed] Master data management often[quantify] makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.  Contents      1 Data Steward Responsibilities     2 Benefits of data stewardship     3 Examples     4 See also     5 References  Data Steward Responsibilities  A data steward ensures that each assigned data element:      Has clear and unambiguous data element definition.     Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)     Has clear enumerated value definitions if it is of type Code.     Is still being used (remove unused data elements)     Is being used consistently in various computer systems     Is being used, fit for purpose = Data Fitness.     Has adequate documentation on appropriate usage and notes     Documents the origin and sources of authority on each metadata element     Is protected against unauthorised access or change  Benefits of data stewardship  Systematic data stewardship can foster fitness thru:      consistent use of data management resources     easy mapping of data between computer systems and exchange documents     lower costs associated with migration to (for example) Service Oriented Architecture (SOA)  Assignment of each data element to a person sometimes seems like an unimportant process. But many groups[which?] have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element. Examples [icon] \tThis section needs expansion. You can help by adding to it. (July 2010)  The   EPA metadata registry furnishes an example of data stewardship. Note that each data element therein has a \"POC\" (point of contact). See also      Metadata     Metadata registry     Data curation     Data element     Data element definition     Representation term     ISO/IEC 11179  From Data Deluge to Data Curation   Philip Lord , Alison Macdonald, Liz Lyon, David Giaretta  The Digital Archiving Consultancy Lim ited and the Digital Curation Centre  Abstract  e-Science – or e-Research - enables new forms and layers of research. It generates massive amounts  of data, at different research stages. Yet the many  technologies used also transform data and put its  integrity  at  risk.  Readability  and  usefulness  are  je opardized  not  just  by  technical  factors.  Data’s   future  quality  –  richness,  trustworthiness  –is  a  func tion  of  investment  in  it.  But  should  all  data  be   kept?  What  other  issues  arise,  for  whom?  We  highlight  findings  of  the  recent  e-Science  Data   Curation  report  commissioned  by  JISC  with  the  s upport  of  the  e-Science  Core  Programme,  and   present  the  Digital  Curation  Centre,  the  first  of  its  kind  in  the  world,  and  its  role  in  providing   resources and support for digital curation and research.  1.   Introduction  The volume of data being created is growing at  an astonishing rate i . E-Science, or perhaps more  inclusively  e-Research,  enables  a  new  order  of   collaborative,  more  inter-disciplinary  research,   based  on  shared  research  expertise,  instruments   and    computing    resources,    and,    crucially,     increasing   access   to   collections   of   primary    research  data  and  information.    This  is  the   knowledge base of research.    There  are  challenges,  however:    these  same   technology changes and the flexibility in use of  information  technology  tools  put  the  very  data   they   create   and   transform   at   risk   and   raise    serious  and  complex  issues  of  strategy,  policy   and      practice      regarding      the      creation,       management,  and  long-term  care  of  data  –  its   curation.  A recent study ii  commissioned by the JISC Joint  Committee for the  Support of Research showed  that  much  needs  to  be  done  at  all  levels  to   enable  the  data  which  is  being  created  by  this   revolution   to   remain   available   and   valid   to    future  researchers.  And  much  is  being  done  by   the  e-Science  community,   in  projects,  research   and other initiatives, and which will be reported  at the e-Science All Hand s Meeting of 2004.  As  part  of  their  response  to  this  problem,  the  JISC   and   e-Science   Core   Programme   are   jointly    funding  the  newly  established  Digital  Curation   Centre (DCC) iii .  Its remit is to provide practical  guidance and outreach concerning data curation,  and  to  undertake  research  into  digital  curation.     The  DCC  is  the  first  initiative  of  its  kind  in  the   world,  and  is  expected  to  become  a  centre  of   excellence in the area.  In this paper we highlight some of the technical,  strategic and policy findings emerging from the  e-Science  Data  Curation  report  and  discuss  the   DCC’s  role  in  addressing   some  of  the  practical   challenges to be addressed.  2.   e-Science Curation  This is a relatively new field, and terminologies  are not yet stable.  We have used the following  working definitions of three key activities:   • Curation:     The  activity  of  managing  and   promoting  the  use  of  data  from  its  point  of   creation,  to  ensure  it  is  fit  for  contemporary   purpose,  and  available  for  discovery  and  re- use.    For  dynamic  datasets  this  may  mean   continuous enrichment or updating to keep it  fit for purpose. Higher levels of curation will  also     involve     maintaining     links     with      annotation and other published materials.  • Archiving:  A    curation    activity    which     ensures that data is properly selected, stored,  can  be  accessed  and  that  its  logical  and   physical  integrity  is  maintained  over  time,   including security and authenticity.  • Preservation :    An  activity  within  archiving   in    which    specific    items    of    data    are     maintained over time so that they can still be  accessed and understood through changes in  technology. iv v 2.1   Survey Findings  The  e-Science  Curation  report  surveyed  and   reported  on  the  provision  of  curation  for  e- Science  data  in  the  UK,  listing  some  13  major   findings,        and        making        ten        major         recommendations for action at a strategic level.  Strategic   and   policy   level   findings   are   not    presented  in  detail  here,  but  in  summary  they   showed that:  • Urgent  action  was  needed  for  the  UK  to   capitalize  on  the  opportunities  presented  by   e-Science.  • Action  was  needed  to  address  a  short-term   funding  regime  which  mitigates  against  the   essentially long-term needs of data curation.  • Before   data-based   research   can   flourish,    questions  of  trust  in  data  (and  data  as  it   ages) need to be addressed, such as security,  confidentiality,  ownership,  provenance  and   authenticity.  • Awareness  of  long-term  data  curation  was   generally  low  among  research  workers,  and   researchers need to be encouraged to engage  more in the curation of their own data.  • Provision  of  services  for  curation  tended  to   be  patchy,  but  was  more  advanced  in  some   areas – particularly areas concerning the bio- sciences  and  in  “big”,  collaborative  science   such as astronomy and particle physics.  Areas  for  further  research,  debate  and  action   include:  Preservation:       How   is   data   to   survive   the    constant   changes   in   information   technology,    which  sees  the  rapid  obsolescence  of  hardware   architectures   and   software   and   file   formats?    How  do  we  decide  to  keep  what,  and  how?     Various    proposals    have    been    made    for     addressing  this  problem,  but  the  area  remains   one    where    more    work    is    required,    both     theoretical and practical.  Awareness  and  compliance:     The  viability  of   data over the longer term depends on awareness.   This  means  that  the  originators  of  data,  or  of   data annotation need to be  aware of the issues of  preservation and curation, and they also need to  be given practical guidance to be engaged in the  process.    Forums    such    as    the    e-Science     programme  and  the  All  Hands  Meetings  are   opportunities to spread the curation word, and to  encourage our audience to  do so too. Of course,  there needs to be awareness at all levels.    Trust:       As   we   noted   above,   in   a   digital    environment  it  is  not  obvious  how  to  engender   trust  in  data  which  has  been  passed  on  to  us.     How  can  we  be  sure  of  its  provenance,  its   quality,   freedom   from   corruption,   and   its    continued privacy and security (where that is an  issue  –  as  in  medical  science)?  We  need  to   determine  to  what  extent  these  are  real  issues,   and  for  which  data.    Work  is  proceeding  on  a   number  of  fronts  –  examples  include  the  work   being done by Professor Buneman on databases  and  the  provenance  question,  or  the  Qurator   project,  looking  at  tools  to  help  discover  and   document  the  quality  of  information  resources.     However, we are still a long way from complete  solutions.    Data selection :  What criteria should be applied  when  selecting  data  for  longer-term  retention?     Some  data  is  obviously  of  unique  value,  but   what  else  should  be  kept?    Selection  introduces   uncertainties   –   how   do   we   know   what   we    should keep?  Questions of costs and risks arise.   Who   sets   the   selection   criteria?      How   can    selection  be  assessed,  when,  by  whom?    Or   should we keep everything, bearing in mind the  costs of maintaining it (its curation)?    The  work  being  carried  out  and  the  tools  being   developed such as in the e-Science projects will  contribute  to  the  prac ticality,  economics  and   thus  viability  of  data  curation.  Thanks  to  data   grids,  portals,  defined  taxonomies,  ontologies,   users  will  be  able  to  discover  data  resources   (which  may  include  the  metadata  about  data)   without having to worry about loading the data,  establishing its reliability, or not finding it in the  first place because of a spelling error.    This  work  is  surely  also  important  for  funders:     on  the  one  hand  it  lightens  the  cost  burden   entailed in keeping data, and on the other it can  protect the value of data generated in research.  Grid    infrastructure    provides    a    distributed     computing   environment   which   facilitates   the    creation  and  analysis  of  large  volumes  of  data   from       e-Research       experimentation       and        applications.         It   creates   opportunity   for    versatility  in  model,  as  well  as  opening  the   knowledge   base   described   above   to   much    broader research communities.  Data   curation   is   an   emergent   field   and   an    exciting  one,  with  many  current  areas  of  active   research.    2.1   Curation report recommendations  The  report’s  findings  led  to  endorsement  of  the   creation  of  a  Digital  Curation  Centre  in  the  UK   as  part  of  the  national  provision  of  curation   facilities.  Since the report was drafted the DCC  has  become  a  reality,  and  its  programme  of   work  is  described  briefly  in  section  4  of  this   paper.  Of the other nine strategic   Figure 1  Model of the Curation Process  recommendations   made,   three   are   of   direct    relevance to the DCC’s programme:  • The production of research led-exemplars to  demonstrate    and    promote    benefits    of     curation should be co-ordinated by the DCC.  • National  and  international  activities  should   be initiated to promote incentives which will  foster  a  scientific  culture  of  engagement  in   data curation.  • Educational  materials,  guidelines  and  policy   documents    for    researchers    need    to    be     developed and publicized.  3.   A curation mo del for e-Sciences  The  accompanying  diagram  (Figure  1)  shows  a   model    of    the    newly    emergent    research     knowledge    cycle.    This    has    three    major     components:    research    and    data    creation,     publishing, and the maturing area of curation.  In  this  model  the  traditional  cycle  of  research   findings  going  through  the  publications  process   and back to consumers, the research community  and other consumers (peers, libraries, the public  and  industry)  is  shown  to  the  top  and  right  of   the information flow diagram (indicated in blue  and  white  on  the  diagram,   and  referred  to  as   Level 1 Curation in the report).  More  recently,  on  the  research  side,  this  has   been   augmented   by   research   methods   based    primarily  on  the  re-use  and  interpretation  of   data  held  in  archives  (indicated  by  red  in  the   diagram,  and  referred  to  as  Level  2  Curation  in   the  report).  This  somewhat  enhanced  cycle  is   exemplified   by   the   work   done   by   social    scientists re-using data held in repositories such  as those held by the UK Data Archive (UKDA)  at  the  University  of  Essex;  similar  models  also   appear  in  the  life  sciences,  and  within  the  arts   community  too,  with  the  Arts  and  Humanities   Data  Service  (AHDS)  –  a  distributed  resource   with a central base at Kings College, London.    Another  example  is  the  astronomical  domain   where  there  are  two  types  of  data  collection   which  are  common:  observatory  mode  -  where   data  is  taken  on  behalf  of  the  observer  and  is   processed    by    the    observatory    system,    as     opposed  to  principal-investigator  mode  where   the observer has hands-on control and processes  data  him/herself.  The  latter  case  is  more  likely   to pose problems with archiving and curation.  Scientist Research  Process Secondary (derived)  data Tertiary data for publication Primary  publication Secondary publication Tertiary Publication Peer Review e-Prints Publication Archives Library  -  Peers   -   Public   -   Industry Publication Process Primary  data Web  Content Patent  data Research Process Research based on data Metadata Curation Curator Curation Process Archived data Data  repositories Philip Lord, 2003 Scientist Research  Process Secondary (derived)  data Tertiary data for publication Primary  publication Secondary publication Tertiary Publication Peer Review e-Prints Publication Archives Library  -  Peers   -   Public   -   Industry Publication Process Primary  data Web  Content Patent  data Research Process Research based on data Metadata Curation Curator Curation Process Archived data Data  repositories Philip Lord, 2003 We are now entering a phase where a third level  of   curation   is   demanded.      In   this   matured    situation,  data  repositories  which  are  actively   curated  are  a  reality,  rather  than  mere  archival   stores. This new part of the information cycle is  depicted  in  the  lower  left  of  the  diagram  (in   green).    In  this  phase  the  data  is  not  merely   stored,   but   is   preserved   to   overcome   the    technical  obsolescence  problem  noted  above,   and  is  subject  to  revision  and  enhancement  as   necessary,   perhaps   augmented   with   tools   to    assist       discovery,       (re-)exploitation       and        presentation, such as the use of ontologies.    We   note   that   accompanying   this   trend   to    curation   there   is   a   parallel   movement   of    provision of enhanced bibliographic facilities in  digital libraries, and even more significantly for  the   scientific   information   cycle,   there   is   an    increasing role for enhanced electronic pre-print  services  (e-Prints)  and  electronic  delivery  of   completed   articles.      This   trend   has   been    described in other work sponsored by JISC in its  initiatives  under  the  e-Research  Programme vi and  in  the  Digital  Preservation  and  Continuing   Access Strategy vii viii ix .  A  good  example  of  a  curated  resource  at  this   level     is     the     UniProt/Swiss-Prot     Protein      Knowledgebase.      UniProt/Swiss-Prot x    is   an    annotated protein sequence database, which was  first  established in 1986.   The  knowledgebase  contains    curated  protein   sequence  information  that  provides  a  high  level   of  annotation,  a  minimal  level  of  redundancy   and    high    level    of    integration    with    other     databases.  It  is  a  \"one-stop  shop\"  that  allows   easy access to all publicly  available information  of protein sequence annotation.  It is maintained  collaboratively    by    the    Swiss    Institute    for     Bioinformatics     (SIB)     and     the     European      Bioinformatics   Institute   (EBI).      It   employs    approximately   100   scientist   in   the   curation    process.      Release   43.6   (21-Jun-04)   of   the    knowledgebase    contains     153320    sequence     entries,   comprising   56,402,618   amino   acids    abstracted from 117,067 references.  4.   The Digital Curation Centre   The DCC, was awarded funding from 1 st  March  2004.    It  is  based  at  the  National  e-Science   Centre    in    Edinburgh    and    the    consortium     comprises four partner institutions:  • University      of      Edinburgh      (lead,       Informatics, Law, Information Services  and research institutes)  • University   of   Glasgow   (HATII   and    Information Services)  • UKOLN, University of Bath  • Council  for  the  Central  Laboratory  of   the Research Councils (CCLRC).  The  DCC  aims  to  provide  a  comprehensive   advisory  service,  a  repository  of  user  tools  and   knowledge   base,   outreach   and   dissemination    activities    including    an    e-journal    and    an     innovative research programme.  The   DCC   is   also   forming   an   Associates    Network  to  provide  a  forum  for  engaging  with   the   communities   of   practice   and   with   key    organisations working in this area.  The  Centre  is  currently  gathering  information   and  feedback  from  disciplinary  representatives   and  users,  which  will  inform  the  research  and   development  initiatives  of  the  Centre  and  will   begin  the  process  of  building  a  user  base  and   community network.   The  DCC  is  also  developing  an  “Approach  to   Curation”   which   will   inform   and   provide    underlying  principles  and  technical  standards   for the curation activity. The DCC is monitoring  existing  architecture  work  and  developments   elsewhere  with  the  aim  of  positioning  the  DCC   research  and  development  programmes  within   the  wider  landscape.  Further  information  about   the   DCC   is   presented   in   a   separate   AHM    poster xi .  5.   Conclusion  New  avenues  of  research  within  which  digital   data and its continued care and enhancement are  central  are  now  emerging.  We  can  expect  to   become part of the mainst ream research in a few  years.  To take best advantage of this nationally  and  to  contribute  fully  internationally,  strategic   and  policy  level  recommendations  have  been   recommended.    These  initiatives  are  required   both   on   management   and   technical   fronts.    Action  has  already  been  initiated  on  some  of   these,  most  notably  with  the  founding  of  the   Digital   Curation   Centre   this   year,   with   the    objectives     of     supporting     the     scientific      community  in  taking  best  advantage  of  new   opportunities", "category": "Edison", "id": 118}
{"skillName": "DSDA02", "skillText": "In the preceding chapters basic elements for the proper execution of analytical work such as personnel, laboratory facilities, equipment, and reagents were discussed. Before embarking upon the actual analytical work, however, one more tool for the quality assurance of the work must be dealt with: the statistical operations necessary to control and verify the analytical procedures (Chapter 7) as well as the resulting data (Chapter 8).  It was stated before that making mistakes in analytical work is unavoidable. This is the reason why a complex system of precautions to prevent errors and traps to detect them has to be set up. An important aspect of the quality control is the detection of both random and systematic errors. This can be done by critically looking at the performance of the analysis as a whole and also of the instruments and operators involved in the job. For the detection itself as well as for the quantification of the errors, statistical treatment of data is indispensable.  A multitude of different statistical tools is available, some of them simple, some complicated, and often very specific for certain purposes. In analytical work, the most important common operation is the comparison of data, or sets of data, to quantify accuracy (bias) and precision. Fortunately, with a few simple convenient statistical tools most of the information needed in regular laboratory work can be obtained: the \"t-test, the \"F-test\", and regression analysis. Therefore, examples of these will be given in the ensuing pages.  Clearly, statistics are a tool, not an aim. Simple inspection of data, without statistical treatment, by an experienced and dedicated analyst may be just as useful as statistical figures on the desk of the disinterested. The value of statistics lies with organizing and simplifying data, to permit some objective estimate showing that an analysis is under control or that a change has occurred. Equally important is that the results of these statistical procedures are recorded and can be retrieved. 6.2 Definitions      6.2.1 Error     6.2.2 Accuracy     6.2.3 Precision     6.2.4 Bias  Discussing Quality Control implies the use of several terms and concepts with a specific (and sometimes confusing) meaning. Therefore, some of the most important concepts will be defined first. 6.2.1 Error  Error is the collective noun for any departure of the result from the \"true\" value*. Analytical errors can be:      1. Random or unpredictable deviations between replicates, quantified with the \"standard deviation\".      2. Systematic or predictable regular deviation from the \"true\" value, quantified as \"mean difference\" (i.e. the difference between the true value and the mean of replicate determinations).      3. Constant, unrelated to the concentration of the substance analyzed (the analyte).      4. Proportional, i.e. related to the concentration of the analyte.          * The \"true\" value of an attribute is by nature indeterminate and often has only a very relative meaning. Particularly in soil science for several attributes there is no such thing as the true value as any value obtained is method-dependent (e.g. cation exchange capacity). Obviously, this does not mean that no adequate analysis serving a purpose is possible. It does, however, emphasize the need for the establishment of standard reference methods and the importance of external QC (see Chapter 9).  6.2.2 Accuracy  The \"trueness\" or the closeness of the analytical result to the \"true\" value. It is constituted by a combination of random and systematic errors (precision and bias) and cannot be quantified directly. The test result may be a mean of several values. An accurate determination produces a \"true\" quantitative value, i.e. it is precise and free of bias. 6.2.3 Precision  The closeness with which results of replicate analyses of a sample agree. It is a measure of dispersion or scattering around the mean value and usually expressed in terms of standard deviation, standard error or a range (difference between the highest and the lowest result). 6.2.4 Bias  The consistent deviation of analytical results from the \"true\" value caused by systematic errors in a procedure. Bias is the opposite but most used measure for \"trueness\" which is the agreement of the mean of analytical results with the true value, i.e. excluding the contribution of randomness represented in precision. There are several components contributing to bias:  1. Method bias      The difference between the (mean) test result obtained from a number of laboratories using the same method and an accepted reference value. The method bias may depend on the analyte level.  2. Laboratory bias      The difference between the (mean) test result from a particular laboratory and the accepted reference value.  3. Sample bias      The difference between the mean of replicate test results of a sample and the (\"true\") value of the target population from which the sample was taken. In practice, for a laboratory this refers mainly to sample preparation, subsampling and weighing techniques. Whether a sample is representative for the population in the field is an extremely important aspect but usually falls outside the responsibility of the laboratory (in some cases laboratories have their own field sampling personnel).  The relationship between these concepts can be expressed in the following equation:  Figure  The types of errors are illustrated in Fig. 6-1.  Fig. 6-1. Accuracy and precision in laboratory measurements. (Note that the qualifications apply to the mean of results: in c the mean is accurate but some individual results are inaccurate)  6.3 Basic Statistics      6.3.1 Mean     6.3.2 Standard deviation     6.3.3 Relative standard deviation. Coefficient of variation     6.3.4 Confidence limits of a measurement     6.3.5 Propagation of errors  In the discussions of Chapters 7 and 8 basic statistical treatment of data will be considered. Therefore, some understanding of these statistics is essential and they will briefly be discussed here.  The basic assumption to be made is that a set of data, obtained by repeated analysis of the same analyte in the same sample under the same conditions, has a normal or Gaussian distribution. (When the distribution is skewed statistical treatment is more complicated). The primary parameters used are the mean (or average) and the standard deviation (see Fig. 6-2) and the main tools the F-test, the t-test, and regression and correlation analysis.  Fig. 6-2. A Gaussian or normal distribution. The figure shows that (approx.) 68% of the data fall in the range ¯ x± s, 95% in the range ¯x ± 2s, and 99.7% in the range ¯x ± 3s. 6.3.1 Mean  The average of a set of n data xi:  ¯   (6.1)  6.3.2 Standard deviation  This is the most commonly used measure of the spread or dispersion of data around the mean. The standard deviation is defined as the square root of the variance (V). The variance is defined as the sum of the squared deviations from the mean, divided by n-1. Operationally, there are several ways of calculation:    (6.1)  or    (6.3)  or    (6.4)  The calculation of the mean and the standard deviation can easily be done on a calculator but most conveniently on a PC with computer programs such as dBASE, Lotus 123, Quattro-Pro, Excel, and others, which have simple ready-to-use functions. (Warning: some programs use n rather than n- 1!). 6.3.3 Relative standard deviation. Coefficient of variation  Although the standard deviation of analytical data may not vary much over limited ranges of such data, it usually depends on the magnitude of such data: the larger the figures, the larger s. Therefore, for comparison of variations (e.g. precision) it is often more convenient to use the relative standard deviation (RSD) than the standard deviation itself. The RSD is expressed as a fraction, but more usually as a percentage and is then called coefficient of variation (CV). Often, however, these terms are confused.      (6.5; 6.6)      Note. When needed (e.g. for the F-test, see Eq. 6.11) the variance can, of course, be calculated by squaring the standard deviation:  V = s2   (6.7)  6.3.4 Confidence limits of a measurement  The more an analysis or measurement is replicated, the closer the mean x of the results will approach the \"true\" value m, of the analyte content (assuming absence of bias).  A single analysis of a test sample can be regarded as literally sampling the imaginary set of a multitude of results obtained for that test sample. The uncertainty of such subsampling is expressed by    (6.8)  where      m = \"true\" value (mean of large set of replicates)     ¯x = mean of subsamples     t = a statistical value which depends on the number of data and the required confidence (usually 95%).     s = standard deviation of mean of subsamples     n = number of subsamples  (The term is also known as the standard error of the mean.)  The critical values for t are tabulated in Appendix 1 (they are, therefore, here referred to as ttab ). To find the applicable value, the number of degrees of freedom has to be established by: df = n -1 (see also Section 6.4.2).  Example  For the determination of the clay content in the particle-size analysis, a semi-automatic pipette installation is used with a 20 mL pipette. This volume is approximate and the operation involves the opening and closing of taps. Therefore, the pipette has to be calibrated, i.e. both the accuracy (trueness) and precision have to be established.  A tenfold measurement of the volume yielded the following set of data (in mL):  19.941   19.812   19.829   19.828   19.742  19.797   19.937   19.847   19.885   19.804  The mean is 19.842 mL and the standard deviation 0.0627 mL. According to Appendix 1 for n = 10 is ttab = 2.26 (df = 9) and using Eq. (6.8) this calibration yields:  pipette volume = 19.842 ± 2.26 (0.0627/) = 19.84 ± 0.04 mL  (Note that the pipette has a systematic deviation from 20 mL as this is outside the found confidence interval. See also bias).  In routine analytical work, results are usually single values obtained in batches of several test samples. No laboratory will analyze a test sample 50 times to be confident that the result is reliable. Therefore, the statistical parameters have to be obtained in another way. Most usually this is done by method validation (see Chapter 7) and/or by keeping control charts, which is basically the collection of analytical results from one or more control samples in each batch (see Chapter 8). Equation (6.8) is then reduced to    (6.9)  where      m = \"true\" value     x = single measurement     t = applicable ttab (Appendix 1)     s = standard deviation of set of previous measurements.  In Appendix 1 can be seen that if the set of replicated measurements is large (say > 30), t is close to 2. Therefore, the (95%) confidence of the result x of a single test sample (n = 1 in Eq. 6.8) is approximated by the commonly used and well known expression    (6.10)  where S is the previously determined standard deviation of the large set of replicates (see also Fig. 6-2).      Note: This \"method-s\" or s of a control sample is not a constant and may vary for different test materials, analyte levels, and with analytical conditions.  Running duplicates will, according to Equation (6.8), increase the confidence of the (mean) result by a factor :  where      ¯x = mean of duplicates     s = known standard deviation of large set  Similarly, triplicate analysis will increase the confidence by a factor , etc. Duplicates are further discussed in Section 8.3.3.  Thus, in summary, Equation (6.8) can be applied in various ways to determine the size of errors (confidence) in analytical work or measurements: single determinations in routine work, determinations for which no previous data exist, certain calibrations, etc. 6.3.5 Propagation of errors      6.3.5.1. Propagation of random errors     6.3.5.2 Propagation of systematic errors  The final result of an analysis is often calculated from several measurements performed during the procedure (weighing, calibration, dilution, titration, instrument readings, moisture correction, etc.). As was indicated in Section 6.2, the total error in an analytical result is an adding-up of the sub-errors made in the various steps. For daily practice, the bias and precision of the whole method are usually the most relevant parameters (obtained from validation, Chapter 7; or from control charts, Chapter 8). However, sometimes it is useful to get an insight in the contributions of the subprocedures (and then these have to be determined separately). For instance if one wants to change (part of) the method.  Because the \"adding-up\" of errors is usually not a simple summation, this will be discussed. The main distinction to be made is between random errors (precision) and systematic errors (bias). 6.3.5.1. Propagation of random errors  In estimating the total random error from factors in a final calculation, the treatment of summation or subtraction of factors is different from that of multiplication or division.  I. Summation calculations  If the final result x is obtained from the sum (or difference) of (sub)measurements a, b, c, etc.:  x = a + b + c +...  then the total precision is expressed by the standard deviation obtained by taking the square root of the sum of individual variances (squares of standard deviation):  If a (sub)measurement has a constant multiplication factor or coefficient (such as an extra dilution), then this is included to calculate the effect of the variance concerned, e.g. (2b)2  Example  The Effective Cation Exchange Capacity of soils (ECEC) is obtained by summation of the exchangeable cations:  ECEC = Exch. (Ca + Mg + Na + K + H + Al)  Standard deviations experimentally obtained for exchangeable Ca, Mg, Na, K and (H + Al) on a certain sample, e.g. a control sample, are: 0.30, 0.25, 0.15, 0.15, and 0.60 cmolc/kg respectively. The total precision is:  It can be seen that the total standard deviation is larger than the highest individual standard deviation, but (much) less than their sum. It is also clear that if one wants to reduce the total standard deviation, qualitatively the best result can be expected from reducing the largest individual contribution, in this case the exchangeable acidity.  2. Multiplication calculations  If the final result x is obtained from multiplication (or subtraction) of (sub)measurements according to  then the total error is expressed by the standard deviation obtained by taking the square root of the sum of the individual relative standard deviations (RSD or CV, as a fraction or as percentage, see Eqs. 6.6 and 6.7):  If a (sub)measurement has a constant multiplication factor or coefficient, then this is included to calculate the effect of the RSD concerned, e.g. (2RSDb)2.  Example  The calculation of Kjeldahl-nitrogen may be as follows:  where      a = ml HCl required for titration sample     b = ml HCl required for titration blank     s = air-dry sample weight in gram     M = molarity of HCl     1.4 = 14×10-3×100% (14 = atomic weight of N)     mcf = moisture correction factor  Note that in addition to multiplications, this calculation contains a subtraction also (often, calculations contain both summations and multiplications.)  Firstly, the standard deviation of the titration (a -b) is determined as indicated in Section 7 above. This is then transformed to RSD using Equations (6.5) or (6.6). Then the RSD of the other individual parameters have to be determined experimentally. The found RSDs are, for instance:      distillation: 0.8%,     titration: 0.5%,     molarity: 0.2%,     sample weight: 0.2%,     mcf: 0.2%.  The total calculated precision is:  Here again, the highest RSD (of distillation) dominates the total precision. In practice, the precision of the Kjeldahl method is usually considerably worse (» 2.5%) probably mainly as a result of the heterogeneity of the sample. The present example does not take that into account. It would imply that 2.5% - 1.0% = 1.5% or 3/5 of the total random error is due to sample heterogeneity (or other overlooked cause). This implies that painstaking efforts to improve subprocedures such as the titration or the preparation of standard solutions may not be very rewarding. It would, however, pay to improve the homogeneity of the sample, e.g. by careful grinding and mixing in the preparatory stage.      Note. Sample heterogeneity is also represented in the moisture correction factor. However, the influence of this factor on the final result is usually very small.  6.3.5.2 Propagation of systematic errors  Systematic errors of (sub)measurements contribute directly to the total bias of the result since the individual parameters in the calculation of the final result each carry their own bias. For instance, the systematic error in a balance will cause a systematic error in the sample weight (as well as in the moisture determination). Note that some systematic errors may cancel out, e.g. weighings by difference may not be affected by a biased balance.  The only way to detect or avoid systematic errors is by comparison (calibration) with independent standards and outside reference or control samples. 6.4 Statistical tests      6.4.1 Two-sided vs. one-sided test     6.4.2 F-test for precision     6.4.3 t-Tests for bias     6.4.4 Linear correlation and regression     6.4.5 Analysis of variance (ANOVA)  In analytical work a frequently recurring operation is the verification of performance by comparison of data. Some examples of comparisons in practice are:      - performance of two instruments,      - performance of two methods,      - performance of a procedure in different periods,      - performance of two analysts or laboratories,      - results obtained for a reference or control sample with the \"true\", \"target\" or \"assigned\" value of this sample.  Some of the most common and convenient statistical tools to quantify such comparisons are the F-test, the t-tests, and regression analysis.  Because the F-test and the t-tests are the most basic tests they will be discussed first. These tests examine if two sets of normally distributed data are similar or dissimilar (belong or not belong to the same \"population\") by comparing their standard deviations and means respectively. This is illustrated in Fig. 6-3.  Fig. 6-3. Three possible cases when comparing two sets of data (n1 = n2). A. Different mean (bias), same precision; B. Same mean (no bias), different precision; C. Both mean and precision are different. (The fourth case, identical sets, has not been drawn).  6.4.1 Two-sided vs. one-sided test  These tests for comparison, for instance between methods A and B, are based on the assumption that there is no significant difference (the \"null hypothesis\"). In other words, when the difference is so small that a tabulated critical value of F or t is not exceeded, we can be confident (usually at 95% level) that A and B are not different. Two fundamentally different questions can be asked concerning both the comparison of the standard deviations s1 and s2 with the F-test, and of the means¯x1, and ¯x2, with the t-test:      1. are A and B different? (two-sided test)     2. is A higher (or lower) than B? (one-sided test).  This distinction has an important practical implication as statistically the probabilities for the two situations are different: the chance that A and B are only different (\"it can go two ways\") is twice as large as the chance that A is higher (or lower) than B (\"it can go only one way\"). The most common case is the two-sided (also called two-tailed) test: there are no particular reasons to expect that the means or the standard deviations of two data sets are different. An example is the routine comparison of a control chart with the previous one (see 8.3). However, when it is expected or suspected that the mean and/or the standard deviation will go only one way, e.g. after a change in an analytical procedure, the one-sided (or one-tailed) test is appropriate. In this case the probability that it goes the other way than expected is assumed to be zero and, therefore, the probability that it goes the expected way is doubled. Or, more correctly, the uncertainty in the two-way test of 5% (or the probability of 5% that the critical value is exceeded) is divided over the two tails of the Gaussian curve (see Fig. 6-2), i.e. 2.5% at the end of each tail beyond 2s. If we perform the one-sided test with 5% uncertainty, we actually increase this 2.5% to 5% at the end of one tail. (Note that for the whole gaussian curve, which is symmetrical, this is then equivalent to an uncertainty of 10% in two ways!)  This difference in probability in the tests is expressed in the use of two tables of critical values for both F and t. In fact, the one-sided table at 95% confidence level is equivalent to the two-sided table at 90% confidence level.  It is emphasized that the one-sided test is only appropriate when a difference in one direction is expected or aimed at. Of course it is tempting to perform this test after the results show a clear (unexpected) effect. In fact, however, then a two times higher probability level was used in retrospect. This is underscored by the observation that in this way even contradictory conclusions may arise: if in an experiment calculated values of F and t are found within the range between the two-sided and one-sided values of Ftab, and ttab, the two-sided test indicates no significant difference, whereas the one-sided test says that the result of A is significantly higher (or lower) than that of B. What actually happens is that in the first case the 2.5% boundary in the tail was just not exceeded, and then, subsequently, this 2.5% boundary is relaxed to 5% which is then obviously more easily exceeded. This illustrates that statistical tests differ in strictness and that for proper interpretation of results in reports, the statistical techniques used, including the confidence limits or probability, should always be specified. 6.4.2 F-test for precision  Because the result of the F-test may be needed to choose between the Student's t-test and the Cochran variant (see next section), the F-test is discussed first.  The F-test (or Fisher's test) is a comparison of the spread of two sets of data to test if the sets belong to the same population, in other words if the precisions are similar or dissimilar.  The test makes use of the ratio of the two variances:    (6.11)  where the larger s2 must be the numerator by convention. If the performances are not very different, then the estimates s1, and s2, do not differ much and their ratio (and that of their squares) should not deviate much from unity. In practice, the calculated F is compared with the applicable F value in the F-table (also called the critical value, see Appendix 2). To read the table it is necessary to know the applicable number of degrees of freedom for s1, and s2. These are calculated by:      df1 = n1-1     df2 = n2-1  If Fcal £ Ftab one can conclude with 95% confidence that there is no significant difference in precision (the \"null hypothesis\" that s1, = s, is accepted). Thus, there is still a 5% chance that we draw the wrong conclusion. In certain cases more confidence may be needed, then a 99% confidence table can be used, which can be found in statistical textbooks.  Example I (two-sided test)  Table 6-1 gives the data sets obtained by two analysts for the cation exchange capacity (CEC) of a control sample. Using Equation (6.11) the calculated F value is 1.62. As we had no particular reason to expect that the analysts would perform differently, we use the F-table for the two-sided test and find Ftab = 4.03 (Appendix 2, df1, = df2 = 9). This exceeds the calculated value and the null hypothesis (no difference) is accepted. It can be concluded with 95% confidence that there is no significant difference in precision between the work of Analyst 1 and 2.  Table 6-1. CEC values (in cmolc/kg) of a control sample determined by two analysts.  1   2  10.2   9.7  10.7   9.0  10.5   10.2  9.9   10.3  9.0   10.8  11.2   11.1  11.5   9.4  10.9   9.2  8.9   9.8  10.6   10.2  ¯x:   10.34   9.97  s:   0.819   0.644  n:   10   10  Fcal = 1.62   tcal = 1.12    Ftab = 4.03   ttab = 2.10    Example 2 (one-sided test)  The determination of the calcium carbonate content with the Scheibler standard method is compared with the simple and more rapid \"acid-neutralization\" method using one and the same sample. The results are given in Table 6-2. Because of the nature of the rapid method we suspect it to produce a lower precision then obtained with the Scheibler method and we can, therefore, perform the one sided F-test. The applicable Ftab = 3.07 (App. 2, df1, = 12, df2 = 9) which is lower than Fcal (=18.3) and the null hypothesis (no difference) is rejected. It can be concluded (with 95% confidence) that for this one sample the precision of the rapid titration method is significantly worse than that of the Scheibler method.  Table 6-2. Contents of CaCO3 (in mass/mass %) in a soil sample determined with the Scheibler method (A) and the rapid titration method (B).  A   B  2.5   1.7  2.4   1.9  2.5   2.3  2.6   2.3  2.5   2.8  2.5   2.5  2.4   1.6  2.6   1.9  2.7   2.6  2.4   1.7  -   2.4  -   2.2     2.6  x:   2.51   2.13  s:   0.099   0.424  n:   10   13  Fcal = 18.3   tcal = 3.12    Ftab = 3.07   ttab* = 2.18    (ttab* = Cochran's \"alternative\" ttab) 6.4.3 t-Tests for bias      6.4.3.1. Student's t-test     6.4.3.2 Cochran's t-test     6.4.3.3 t-Test for large data sets (n³ 30)     6.4.3.4 Paired t-test  Depending on the nature of two sets of data (n, s, sampling nature), the means of the sets can be compared for bias by several variants of the t-test. The following most common types will be discussed:      1. Student's t-test for comparison of two independent sets of data with very similar standard deviations;      2. the Cochran variant of the t-test when the standard deviations of the independent sets differ significantly;      3. the paired t-test for comparison of strongly dependent sets of data.  Basically, for the t-tests Equation (6.8) is used but written in a different way:    (6.12)  where      ¯x = mean of test results of a sample     m = \"true\" or reference value     s = standard deviation of test results     n = number of test results of the sample.  To compare the mean of a data set with a reference value normally the \"two-sided t-table of critical values\" is used (Appendix 1). The applicable number of degrees of freedom here is:  df = n-1  If a value for t calculated with Equation (6.12) does not exceed the critical value in the table, the data are taken to belong to the same population: there is no difference and the \"null hypothesis\" is accepted (with the applicable probability, usually 95%).  As with the F-test, when it is expected or suspected that the obtained results are higher or lower than that of the reference value, the one-sided t-test can be performed: if tcal > ttab, then the results are significantly higher (or lower) than the reference value.  More commonly, however, the \"true\" value of proper reference samples is accompanied by the associated standard deviation and number of replicates used to determine these parameters. We can then apply the more general case of comparing the means of two data sets: the \"true\" value in Equation (6.12) is then replaced by the mean of a second data set. As is shown in Fig. 6-3, to test if two data sets belong to the same population it is tested if the two Gauss curves do sufficiently overlap. In other words, if the difference between the means ¯x1-¯x2 is small. This is discussed next.  Similarity or non-similarity of standard deviations  When using the t-test for two small sets of data (n1 and/or n2<30), a choice of the type of test must be made depending on the similarity (or non-similarity) of the standard deviations of the two sets. If the standard deviations are sufficiently similar they can be \"pooled\" and the Student t-test can be used. When the standard deviations are not sufficiently similar an alternative procedure for the t-test must be followed in which the standard deviations are not pooled. A convenient alternative is the Cochran variant of the t-test. The criterion for the choice is the passing or non-passing of the F-test (see 6.4.2), that is, if the variances do or do not significantly differ. Therefore, for small data sets, the F-test should precede the t-test.  For dealing with large data sets (n1, n2,³ 30) the \"normal\" t-test is used (see Section 6.4.3.3 and App. 3). 6.4.3.1. Student's t-test  (To be applied to small data sets (n1, n2 < 30) where s1, and s2 are similar according to F-test.  When comparing two sets of data, Equation (6.12) is rewritten as:    (6.13)  where      ¯x1 = mean of data set 1     ¯x2 = mean of data set 2     sp = \"pooled\" standard deviation of the sets     n1 = number of data in set 1     n2 = number of data in set 2.  The pooled standard deviation sp is calculated by:    6.14  where      s1 = standard deviation of data set 1     s2 = standard deviation of data set 2     n1 = number of data in set 1     n2 = number of data in set 2.  To perform the t-test, the critical ttab has to be found in the table (Appendix 1); the applicable number of degrees of freedom df is here calculated by:      df = n1 + n2 -2  Example  The two data sets of Table 6-1 can be used: With Equations (6.13) and (6.14) tcal, is calculated as 1.12 which is lower than the critical value ttab of 2.10 (App. 1, df = 18, two-sided), hence the null hypothesis (no difference) is accepted and the two data sets are assumed to belong to the same population: there is no significant difference between the mean results of the two analysts (with 95% confidence).      Note. Another illustrative way to perform this test for bias is to calculate if the difference between the means falls within or outside the range where this difference is still not significantly large. In other words, if this difference is less than the least significant difference (lsd). This can be derived from Equation (6.13):    6.15  In the present example of Table 6-1, the calculation yields lsd = 0.69. The measured difference between the means is 10.34 -9.97 = 0.37 which is smaller than the lsd indicating that there is no significant difference between the performance of the analysts.  In addition, in this approach the 95% confidence limits of the difference between the means can be calculated (cf. Equation 6.8):  confidence limits = 0.37 ± 0.69 = -0.32 and 1.06  Note that the value 0 for the difference is situated within this confidence interval which agrees with the null hypothesis of x1 = x2 (no difference) having been accepted. 6.4.3.2 Cochran's t-test  To be applied to small data sets (n1, n2, < 30) where s1 and s2, are dissimilar according to F-test.  Calculate t with:    6.16  Then determine an \"alternative\" critical t-value:    6.17  where      t1 = ttab at n1-1 degrees of freedom     t2 = ttab at n2-1 degrees of freedom  Now the t-test can be performed as usual: if tcal< ttab* then the null hypothesis that the means do not significantly differ is accepted.  Example  The two data sets of Table 6-2 can be used.  According to the F-test, the standard deviations differ significantly so that the Cochran variant must be used. Furthermore, in contrast to our expectation that the precision of the rapid test would be inferior, we have no idea about the bias and therefore the two-sided test is appropriate. The calculations yield tcal = 3.12 and ttab*= 2.18 meaning that tcal exceeds ttab* which implies that the null hypothesis (no difference) is rejected and that the mean of the rapid analysis deviates significantly from that of the standard analysis (with 95% confidence, and for this sample only). Further investigation of the rapid method would have to include the use of more different samples and then comparison with the one-sided t-test would be justified (see 6.4.3.4, Example 1). 6.4.3.3 t-Test for large data sets (n³ 30)  In the example above (6.4.3.2) the conclusion happens to have been the same if the Student's t-test with pooled standard deviations had been used. This is caused by the fact that the difference in result of the Student and Cochran variants of the t-test is largest when small sets of data are compared, and decreases with increasing number of data. Namely, with increasing number of data a better estimate of the real distribution of the population is obtained (the flatter t-distribution converges then to the standardized normal distribution). When n³ 30 for both sets, e.g. when comparing Control Charts (see 8.3), for all practical purposes the difference between the Student and Cochran variant is negligible. The procedure is then reduced to the \"normal\" t-test by simply calculating tcal with Eq. (6.16) and comparing this with ttab at df = n1 + n2-2. (Note in App. 1 that the two-sided ttab is now close to 2).  The proper choice of the t-test as discussed above is summarized in a flow diagram in Appendix 3. 6.4.3.4 Paired t-test  When two data sets are not independent, the paired t-test can be a better tool for comparison than the \"normal\" t-test described in the previous sections. This is for instance the case when two methods are compared by the same analyst using the same sample(s). It could, in fact, also be applied to the example of Table 6-1 if the two analysts used the same analytical method at (about) the same time.  As stated previously, comparison of two methods using different levels of analyte gives more validation information about the methods than using only one level. Comparison of results at each level could be done by the F and t-tests as described above. The paired t-test, however, allows for different levels provided the concentration range is not too wide. As a rule of fist, the range of results should be within the same magnitude. If the analysis covers a longer range, i.e. several powers of ten, regression analysis must be considered (see Section 6.4.4). In intermediate cases, either technique may be chosen.  The null hypothesis is that there is no difference between the data sets, so the test is to see if the mean of the differences between the data deviates significantly from zero or not (two-sided test). If it is expected that one set is systematically higher (or lower) than the other set, then the one-sided test is appropriate.  Example 1  The \"promising\" rapid single-extraction method for the determination of the cation exchange capacity of soils using the silver thiourea complex (AgTU, buffered at pH 7) was compared with the traditional ammonium acetate method (NH4OAc, pH 7). Although for certain soil types the difference in results appeared insignificant, for other types differences seemed larger. Such a suspect group were soils with ferralic (oxic) properties (i.e. highly weathered sesquioxide-rich soils). In Table 6-3 the results often soils with these properties are grouped to test if the CEC methods give different results. The difference d within each pair and the parameters needed for the paired t-test are given also.  Table 6-3. CEC values (in cmolc/kg) obtained by the NH4OAc and AgTU methods (both at pH 7) for ten soils with ferralic properties.  Sample   NH4OAc   AgTU   d  1   7.1   6.5   -0.6  2   4.6   5.6   +1.0  3   10.6   14.5   +3.9  4   2.3   5.6   +3.3  5   25.2   23.8   -1.4  6   4.4   10.4   +6.0  7   7.8   8.4   +0.6  8   2.7   5.5   +2.8  9   14.3   19.2   +4.9  10   13.6   15.0   +1.4  ¯d = +2.19   tcal = 2.89  sd = 2.395   ttab = 2.26  Using Equation (6.12) and noting that m d = 0 (hypothesis value of the differences, i.e. no difference), the t-value can be calculated as:  where      = mean of differences within each pair of data     sd = standard deviation of the mean of differences     n = number of pairs of data  The calculated t value (=2.89) exceeds the critical value of 1.83 (App. 1, df = n -1 = 9, one-sided), hence the null hypothesis that the methods do not differ is rejected and it is concluded that the silver thiourea method gives significantly higher results as compared with the ammonium acetate method when applied to such highly weathered soils.      Note. Since such data sets do not have a normal distribution, the \"normal\" t-test which compares means of sets cannot be used here (the means do not constitute a fair representation of the sets). For the same reason no information about the precision of the two methods can be obtained, nor can the F-test be applied. For information about precision, replicate determinations are needed.  Example 2  Table 6-4 shows the data of total-P in four plant tissue samples obtained by a laboratory L and the median values obtained by 123 laboratories in a proficiency (round-robin) test.  Table 6-4. Total-P contents (in mmol/kg) of plant tissue as determined by 123 laboratories (Median) and Laboratory L.  Sample   Median   Lab L   d  1   93.0   85.2   -7.8  2   201   224   23  3   78.9   84.5   5.6  4   175   185   10  ¯d = 7.70   tcal =1.21  sd = 12.702   ttab = 3.18  To verify the performance of the laboratory a paired t-test can be performed:  Using Eq. (6.12) and noting that m d=0 (hypothesis value of the differences, i.e. no difference), the t value can be calculated as:  The calculated t-value is below the critical value of 3.18 (Appendix 1, df = n - 1 = 3, two-sided), hence the null hypothesis that the laboratory does not significantly differ from the group of laboratories is accepted, and the results of Laboratory L seem to agree with those of \"the rest of the world\" (this is a so-called third-line control). 6.4.4 Linear correlation and regression      6.4.4.1 Construction of calibration graph     6.4.4.2 Comparing two sets of data using many samples at different analyte levels  These also belong to the most common useful statistical tools to compare effects and performances X and Y. Although the technique is in principle the same for both, there is a fundamental difference in concept: correlation analysis is applied to independent factors: if X increases, what will Y do (increase, decrease, or perhaps not change at all)? In regression analysis a unilateral response is assumed: changes in X result in changes in Y, but changes in Y do not result in changes in X.  For example, in analytical work, correlation analysis can be used for comparing methods or laboratories, whereas regression analysis can be used to construct calibration graphs. In practice, however, comparison of laboratories or methods is usually also done by regression analysis. The calculations can be performed on a (programmed) calculator or more conveniently on a PC using a home-made program. Even more convenient are the regression programs included in statistical packages such as Statistix, Mathcad, Eureka, Genstat, Statcal, SPSS, and others. Also, most spreadsheet programs such as Lotus 123, Excel, and Quattro-Pro have functions for this.  Laboratories or methods are in fact independent factors. However, for regression analysis one factor has to be the independent or \"constant\" factor (e.g. the reference method, or the factor with the smallest standard deviation). This factor is by convention designated X, whereas the other factor is then the dependent factor Y (thus, we speak of \"regression of Y on X\").  As was discussed in Section 6.4.3, such comparisons can often been done with the Student/Cochran or paired t-tests. However, correlation analysis is indicated:      1. When the concentration range is so wide that the errors, both random and systematic, are not independent (which is the assumption for the t-tests). This is often the case where concentration ranges of several magnitudes are involved.      2. When pairing is inappropriate for other reasons, notably a long time span between the two analyses (sample aging, change in laboratory conditions, etc.).  The principle is to establish a statistical linear relationship between two sets of corresponding data by fitting the data to a straight line by means of the \"least squares\" technique. Such data are, for example, analytical results of two methods applied to the same samples (correlation), or the response of an instrument to a series of standard solutions (regression).      Note: Naturally, non-linear higher-order relationships are also possible, but since these are less common in analytical work and more complex to handle mathematically, they will not be discussed here. Nevertheless, to avoid misinterpretation, always inspect the kind of relationship by plotting the data, either on paper or on the computer monitor.  The resulting line takes the general form:  y = bx + a   (6.18)  where      a = intercept of the line with the y-axis     b = slope (tangent)  In laboratory work ideally, when there is perfect positive correlation without bias, the intercept a = 0 and the slope = 1. This is the so-called \"1:1 line\" passing through the origin (dashed line in Fig. 6-5).  If the intercept a ¹ 0 then there is a systematic discrepancy (bias, error) between X and Y; when b ¹ 1 then there is a proportional response or difference between X and Y.  The correlation between X and Y is expressed by the correlation coefficient r which can be calculated with the following equation:    6.19  where      xi = data X     ¯x = mean of data X     yi = data Y     ¯y = mean of data Y  It can be shown that r can vary from 1 to -1:      r = 1 perfect positive linear correlation     r = 0 no linear correlation (maybe other correlation)     r = -1 perfect negative linear correlation  Often, the correlation coefficient r is expressed as r2: the coefficient of determination or coefficient of variance. The advantage of r2 is that, when multiplied by 100, it indicates the percentage of variation in Y associated with variation in X. Thus, for example, when r = 0.71 about 50% (r2 = 0.504) of the variation in Y is due to the variation in X.  The line parameters b and a are calculated with the following equations:    6.20  and  a = ¯y - b¯x   6.21  It is worth to note that r is independent of the choice which factor is the independent factory and which is the dependent Y. However, the regression parameters a and do depend on this choice as the regression lines will be different (except when there is ideal 1:1 correlation). 6.4.4.1 Construction of calibration graph  As an example, we take a standard series of P (0-1.0 mg/L) for the spectrophotometric determination of phosphate in a Bray-I extract (\"available P\"), reading in absorbance units. The data and calculated terms needed to determine the parameters of the calibration graph are given in Table 6-5. The line itself is plotted in Fig. 6-4.  Table 6-5 is presented here to give an insight in the steps and terms involved. The calculation of the correlation coefficient r with Equation (6.19) yields a value of 0.997 (r2 = 0.995). Such high values are common for calibration graphs. When the value is not close to 1 (say, below 0.98) this must be taken as a warning and it might then be advisable to repeat or review the procedure. Errors may have been made (e.g. in pipetting) or the used range of the graph may not be linear. On the other hand, a high r may be misleading as it does not necessarily indicate linearity. Therefore, to verify this, the calibration graph should always be plotted, either on paper or on computer monitor.  Using Equations (6.20 and (6.21) we obtain:  and  a = 0.350 - 0.313 = 0.037  Thus, the equation of the calibration line is:  y = 0.626x + 0.037   (6.22)  Table 6-5. Parameters of calibration graph in Fig. 6-4.  xi   yi   x1-¯x   (xi-¯x)2   yi-¯y   (yi-¯y)2   (x1-¯x)(yi-¯y)  0.0   0.05   -0.5   0.25   -0.30   0.090   0.150  0.2   0.14   -0.3   0.09   -0.21   0.044   0.063  0.4   0.29   -0.1   0.01   -0.06   0.004   0.006  0.6   0.43   0.1   0.01   0.08   0.006   0.008  0.8   0.52   0.3   0.09   0.17   0.029   0.051  1.0   0.67   0.5   0.25   0.32   0.102   0.160  3.0   2.10   0   0.70   0   0.2754   0.438 S  ¯x=0.5   ¯y = 0.35    Fig. 6-4. Calibration graph plotted from data of Table 6-5. The dashed lines delineate the 95% confidence area of the graph. Note that the confidence is highest at the centroid of the graph.  During calculation, the maximum number of decimals is used, rounding off to the last significant figure is done at the end (see instruction for rounding off in Section 8.2).  Once the calibration graph is established, its use is simple: for each y value measured the corresponding concentration x can be determined either by direct reading or by calculation using Equation (6.22). The use of calibration graphs is further discussed in Section 7.2.2.      Note. A treatise of the error or uncertainty in the regression line is given.  6.4.4.2 Comparing two sets of data using many samples at different analyte levels  Although regression analysis assumes that one factor (on the x-axis) is constant, when certain conditions are met the technique can also successfully be applied to comparing two variables such as laboratories or methods. These conditions are:      - The most precise data set is plotted on the x-axis     - At least 6, but preferably more than 10 different samples are analyzed     - The samples should rather uniformly cover the analyte level range of interest.  To decide which laboratory or method is the most precise, multi-replicate results have to be used to calculate standard deviations (see 6.4.2). If these are not available then the standard deviations of the present sets could be compared (note that we are now not dealing with normally distributed sets of replicate results). Another convenient way is to run the regression analysis on the computer, reverse the variables and run the analysis again. Observe which variable has the lowest standard deviation (or standard error of the intercept a, both given by the computer) and then use the results of the regression analysis where this variable was plotted on the x-axis.  If the analyte level range is incomplete, one might have to resort to spiking or standard additions, with the inherent drawback that the original analyte-sample combination may not adequately be reflected.  Example  In the framework of a performance verification programme, a large number of soil samples were analyzed by two laboratories X and Y (a form of \"third-line control\", see Chapter 9) and the data compared by regression. (In this particular case, the paired t-test might have been considered also). The regression line of a common attribute, the pH, is shown here as an illustration. Figure 6-5 shows the so-called \"scatter plot\" of 124 soil pH-H2O determinations by the two laboratories. The correlation coefficient r is 0.97 which is very satisfactory. The slope (= 1.03) indicates that the regression line is only slightly steeper than the 1:1 ideal regression line. Very disturbing, however, is the intercept a of -1.18. This implies that laboratory Y measures the pH more than a whole unit lower than laboratory X at the low end of the pH range (the intercept -1.18 is at pHx = 0) which difference decreases to about 0.8 unit at the high end.  Fig. 6-5. Scatter plot of pH data of two laboratories. Drawn line: regression line; dashed line: 1:1 ideal regression line.  The t-test for significance is as follows:  For intercept a: m a = 0 (null hypothesis: no bias; ideal intercept is then zero), standard error =0.14 (calculated by the computer), and using Equation (6.12) we obtain:  Here, ttab = 1.98 (App. 1, two-sided, df = n - 2 = 122 (n-2 because an extra degree of freedom is lost as the data are used for both a and b) hence, the laboratories have a significant mutual bias.  For slope: m b = 1 (ideal slope: null hypothesis is no difference), standard error = 0.02 (given by computer), and again using Equation (6.12) we obtain:  Again, ttab = 1.98 (App. 1; two-sided, df = 122), hence, the difference between the laboratories is not significantly proportional (or: the laboratories do not have a significant difference in sensitivity). These results suggest that in spite of the good correlation, the two laboratories would have to look into the cause of the bias.      Note. In the present example, the scattering of the points around the regression line does not seem to change much over the whole range. This indicates that the precision of laboratory Y does not change very much over the range with respect to laboratory X. This is not always the case. In such cases, weighted regression (not discussed here) is more appropriate than the unweighted regression as used here.      Validation of a method (see Section 7.5) may reveal that precision can change significantly with the level of analyte (and with other factors such as sample matrix).  6.4.5 Analysis of variance (ANOVA)  When results of laboratories or methods are compared where more than one factor can be of influence and must be distinguished from random effects, then ANOVA is a powerful statistical tool to be used. Examples of such factors are: different analysts, samples with different pre-treatments, different analyte levels, different methods within one of the laboratories). Most statistical packages for the PC can perform this analysis.  As a treatise of ANOVA is beyond the scope of the present Guidelines, for further discussion the reader is referred to statistical textbooks, some of which are given in the list of Literature.  Error or uncertainty in the regression line  The \"fitting\" of the calibration graph is necessary because the response points yi, composing the line do not fall exactly on the line. Hence, random errors are implied. This is expressed by an uncertainty about the slope and intercept b and a defining the line. A quantification can be found in the standard deviation of these parameters. Most computer programmes for regression will automatically produce figures for these. To illustrate the procedure, the example of the calibration graph in Section 6.4.3.1 is elaborated here.  A practical quantification of the uncertainty is obtained by calculating the standard deviation of the points on the line; the \"residual standard deviation\" or \"standard error of the y-estimate\", which we assumed to be constant (but which is only approximately so, see Fig. 6-4):    (6.23)  where      = \"fitted\" y-value for each xi, (read from graph or calculated with Eq. 6.22). Thus, is the (vertical) deviation of the found y-values from the line.      n = number of calibration points.      Note: Only the y-deviations of the points from the line are considered. It is assumed that deviations in the x-direction are negligible. This is, of course, only the case if the standards are very accurately prepared.  Now the standard deviations for the intercept a and slope b can be calculated with:    6.24  and    6.25  To make this procedure clear, the parameters involved are listed in Table 6-6.  The uncertainty about the regression line is expressed by the confidence limits of a and b according to Eq. (6.9): a ± t.sa and b ± t.sb  Table 6-6. Parameters for calculating errors due to calibration graph (use also figures of Table 6-5).  xi   yi       0   0.05   0.037   0.013   0.0002  0.2   0.14   0.162   -0.022   0.0005  0.4   0.29   0.287   0.003   0.0000  0.6   0.43   0.413   0.017   0.0003  0.8   0.52   0.538   -0.018   0.0003  1.0   0.67   0.663   0.007   0.0001              0.001364 S  In the present example, using Eq. (6.23), we calculate  and, using Eq. (6.24) and Table 6-5:  and, using Eq. (6.25) and Table 6-5:  The applicable ttab is 2.78 (App. 1, two-sided, df = n -1 = 4) hence, using Eq. (6.9):      a = 0.037 ± 2.78 × 0.0132 = 0.037 ± 0.037     and     b = 0.626 ± 2.78 × 0.0219 = 0.626 ± 0.061  Note that if sa is large enough, a negative value for a is possible, i.e. a negative reading for the blank or zero-standard. (For a discussion about the error in x resulting from a reading in y, which is particularly relevant for reading a calibration graph, see Section 7.2.3)  The uncertainty about the line is somewhat decreased by using more calibration points (assuming sy has not increased): one more point reduces ttab from 2.78 to 2.57 (see Appendix 1).  Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people. The relationship isn't perfect. People of the same height vary in weight, and you can easily think of two people you know where the shorter one is heavier than the taller one. Nonetheless, the average weight of people 5'5'' is less than the average weight of people 5'6'', and their average weight is less than that of people 5'7'', etc. Correlation can tell you just how much of the variation in peoples' weights is related to their heights.  Although this correlation is fairly obvious your data may contain unsuspected correlations. You may also suspect there are correlations, but don't know which are the strongest. An intelligent correlation analysis can lead to a greater understanding of your data. Techniques in Determining Correlation  There are several different correlation techniques. The Survey System's optional Statistics Module includes the most common type, called the Pearson or product-moment correlation. The module also includes a variation on this type called partial correlation. The latter is useful when you want to look at the relationship between two variables while removing the effect of one or two other variables.  Like all statistical techniques, correlation is only appropriate for certain kinds of data. Correlation works for quantifiable data in which numbers are meaningful, usually quantities of some sort. It cannot be used for purely categorical data, such as gender, brands purchased, or favorite color. Rating Scales  Rating scales are a controversial middle case. The numbers in rating scales have meaning, but that meaning isn't very precise. They are not like quantities. With a quantity (such as dollars), the difference between 1 and 2 is exactly the same as between 2 and 3. With a rating scale, that isn't really the case. You can be sure that your respondents think a rating of 2 is between a rating of 1 and a rating of 3, but you cannot be sure they think it is exactly halfway between. This is especially true if you labeled the mid-points of your scale (you cannot assume \"good\" is exactly half way between \"excellent\" and \"fair\").  Most statisticians say you cannot use correlations with rating scales, because the mathematics of the technique assume the differences between numbers are exactly equal. Nevertheless, many survey researchers do use correlations with rating scales, because the results usually reflect the real world. Our own position is that you can use correlations with rating scales, but you should do so with care. When working with quantities, correlations provide precise measurements. When working with rating scales, correlations provide general indications. Correlation Coefficient  The main result of a correlation is called the correlation coefficient (or \"r\"). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.  If r is close to 0, it means there is no relationship between the variables. If r is positive, it means that as one variable gets larger the other gets larger. If r is negative it means that as one gets larger, the other gets smaller (often called an \"inverse\" correlation).  While correlation coefficients are normally reported as r = (a value between -1 and +1), squaring them makes then easier to understand. The square of the coefficient (or r square) is equal to the percent of the variation in one variable that is related to the variation in the other. After squaring r, ignore the decimal point. An r of .5 means 25% of the variation is related (.5 squared =.25). An r value of .7 means 49% of the variance is related (.7 squared = .49).  A correlation report can also show a second result of each test - statistical significance. In this case, the significance level will tell you how likely it is that the correlations reported may be due to chance in the form of random sampling error. If you are working with small sample sizes, choose a report format that includes the significance level. This format also reports the sample size.  A key thing to remember when working with correlations is never to assume a correlation means that a change in one variable causes a change in another. Sales of personal computers and athletic shoes have both risen strongly in the last several years and there is a high correlation between them, but you cannot assume that buying computers causes people to buy athletic shoes (or vice versa).  The second caveat is that the Pearson correlation technique works best with linear relationships: as one variable gets larger, the other gets larger (or smaller) in direct proportion. It does not work well with curvilinear relationships (in which the relationship does not follow a straight line). An example of a curvilinear relationship is age and health care. They are related, but the relationship doesn't follow a straight line. Young children and older people both tend to use much more health care than teenagers or young adults. Multiple regression (also included in the Statistics Module) can be used to examine curvilinear relationships, but it is beyond the scope of this article. The concept of big data has been around for years; most organizations now understand that if they capture all the data that streams into their businesses, they can apply analytics and get significant value from it. But even in the 1950s, decades before anyone uttered the term “big data,” businesses were using basic analytics (essentially numbers in a spreadsheet that were manually examined) to uncover insights and trends.  The new benefits that big data analytics brings to the table, however, are speed and efficiency. Whereas a few years ago a business would have gathered information, run analytics and unearthed information that could be used for future decisions, today that business can identify insights for immediate decisions. The ability to work faster – and stay agile – gives organizations a competitive edge they didn’t have before.    The Importance of Big Data Analytics Graphic Why is big data analytics important?  Big data analytics helps organizations harness their data and use it to identify new opportunities. That, in turn, leads to smarter business moves, more efficient operations, higher profits and happier customers. In his report Big Data in Big Companies, IIA Director of Research Tom Davenport interviewed more than 50 businesses to understand how they used big data. He found they got value in the following ways:      Cost reduction. Big data technologies such as Hadoop and cloud-based analytics bring significant cost advantages when it comes to storing large amounts of data – plus they can identify more efficient ways of doing business.     Faster, better decision making. With the speed of Hadoop and in-memory analytics, combined with the ability to analyze new sources of data, businesses are able to analyze information immediately – and make decisions based on what they’ve learned.     New products and services. With the ability to gauge customer needs and satisfaction through analytics comes the power to give customers what they want. Davenport points out that with big data analytics, more companies are creating new products to meet customers’ needs.   Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains.  Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.  Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.  Contents      1 The process of data analysis         1.1 Data requirements         1.2 Data collection         1.3 Data processing         1.4 Data cleaning         1.5 Exploratory data analysis         1.6 Modeling and algorithms         1.7 Data product         1.8 Communication     2 Quantitative messages     3 Techniques for analyzing quantitative data     4 Analytical activities of data users     5 Barriers to effective analysis         5.1 Confusing fact and opinion         5.2 Cognitive biases         5.3 Innumeracy     6 Other topics         6.1 Analytics and business intelligence         6.2 Education     7 Practitioner notes         7.1 Initial data analysis             7.1.1 Quality of data             7.1.2 Quality of measurements             7.1.3 Initial transformations             7.1.4 Did the implementation of the study fulfill the intentions of the research design?             7.1.5 Characteristics of data sample             7.1.6 Final stage of the initial data analysis             7.1.7 Analysis             7.1.8 Nonlinear analysis         7.2 Main data analysis             7.2.1 Exploratory and confirmatory approaches             7.2.2 Stability of results             7.2.3 Statistical methods     8 Free software for data analysis     9 See also     10 References         10.1 Citations         10.2 Bibliography     11 Further reading  The process of data analysis Data science process flowchart  Analysis refers to breaking a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data and converting it into information useful for decision-making by users. Data is collected and analyzed to answer questions, test hypotheses or disprove theories.[1]  Statistician John Tukey defined data analysis in 1961 as: \"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"[2]  There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases.[3] Data requirements  The data necessary as inputs to the analysis are specified based upon the requirements of those directing the analysis or customers who will use the finished product of the analysis. The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained. Data may be numerical or categorical (i.e., a text label for numbers).[3] Data collection  Data is collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data, such as information technology personnel within an organization. The data may also be collected from sensors in the environment, such as traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.[3] Data processing The phases of the intelligence cycle used to convert raw information into actionable intelligence or knowledge are conceptually similar to the phases in data analysis.  Data initially obtained must be processed or organized for analysis. For instance, this may involve placing data into rows and columns in a table format for further analysis, such as within a spreadsheet or statistical software.[3] Data cleaning  Once processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that data is entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, deduplication, and column segmentation.[4] Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable.[5] Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of data cleaning that depend on the type of data. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spellcheckers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct.[6] Exploratory data analysis  Once the data is cleaned, it can be analyzed. Analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained in the data.[7][8] The process of exploration may result in additional data cleaning or additional requests for data, so these activities may be iterative in nature. Descriptive statistics such as the average or median may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data.[3] Modeling and algorithms  Mathematical formulas or models called algorithms may be applied to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some residual error depending on model accuracy (i.e., Data = Model + Error).[1]  Inferential statistics includes techniques to measure relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X) explains the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as Y = aX + b + error, where the model is designed such that a and b minimize the error when the model predicts Y for a given range of values of X. Analysts may attempt to build models that are descriptive of the data to simplify analysis and communicate results.[1] Data product  A data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. An example is an application that analyzes data about customer purchasing history and recommends other purchases the customer might enjoy.[3] Communication Data visualization to understand the results of a data analysis.[9] Main article: Data visualization  Once the data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.[3]  When determining how to communicate the results, the analyst may consider data visualization techniques to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays such as tables and charts to help communicate key messages contained in the data. Tables are helpful to a user who might lookup specific numbers, while charts (e.g., bar charts or line charts) may help explain the quantitative messages contained in the data. Quantitative messages Main article: Data visualization A time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time. A scatterplot illustrating correlation between two variables (inflation and unemployment) measured at points in time.  Author Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.      Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.     Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.     Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.     Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.     Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis.     Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.     Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.     Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.[10][11]  Techniques for analyzing quantitative data See also: Problem solving  Author Jonathan Koomey has recommended a series of best practices for understanding quantitative data. These include:      Check raw data for anomalies prior to performing your analysis;     Re-perform important calculations, such as verifying columns of data that are formula driven;     Confirm main totals are the sum of subtotals;     Check relationships between numbers that should be related in a predictable way, such as ratios over time;     Normalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;     Break problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.[5]  For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean. An illustration of the MECE principle used for data analysis.  The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE. For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).  Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.  Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.  Necessary condition analysis  (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible. Analytical activities of data users  Users may have particular data points of interest within a data set, as opposed to general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.[12][13][14] # \tTask \tGeneral Description \tPro Forma Abstract \tExamples 1 \tRetrieve Value \tGiven a set of specific cases, find attributes of those cases. \tWhat are the values of attributes {X, Y, Z, ...} in the data cases {A, B, C, ...}? \t- What is the mileage per gallon of the Audi TT?  - How long is the movie Gone with the Wind? 2 \tFilter \tGiven some concrete conditions on attribute values, find data cases satisfying those conditions. \tWhich data cases satisfy conditions {A, B, C...}? \t- What Kellogg's cereals have high fiber?  - What comedies have won awards?  - Which funds underperformed the SP-500? 3 \tCompute Derived Value \tGiven a set of data cases, compute an aggregate numeric representation of those data cases. \tWhat is the value of aggregation function F over a given set S of data cases? \t- What is the average calorie content of Post cereals?  - What is the gross income of all stores combined?  - How many manufacturers of cars are there? 4 \tFind Extremum \tFind data cases possessing an extreme value of an attribute over its range within the data set. \tWhat are the top/bottom N data cases with respect to attribute A? \t- What is the car with the highest MPG?  - What director/film has won the most awards?  - What Robin Williams film has the most recent release date? 5 \tSort \tGiven a set of data cases, rank them according to some ordinal metric. \tWhat is the sorted order of a set S of data cases according to their value of attribute A? \t- Order the cars by weight.  - Rank the cereals by calories. 6 \tDetermine Range \tGiven a set of data cases and an attribute of interest, find the span of values within the set. \tWhat is the range of values of attribute A in a set S of data cases? \t- What is the range of film lengths?  - What is the range of car horsepowers?  - What actresses are in the data set? 7 \tCharacterize Distribution \tGiven a set of data cases and a quantitative attribute of interest, characterize the distribution of that attribute’s values over the set. \tWhat is the distribution of values of attribute A in a set S of data cases? \t- What is the distribution of carbohydrates in cereals?  - What is the age distribution of shoppers? 8 \tFind Anomalies \tIdentify any anomalies within a given set of data cases with respect to a given relationship or expectation, e.g. statistical outliers. \tWhich data cases in a set S of data cases have unexpected/exceptional values? \t- Are there exceptions to the relationship between horsepower and acceleration?  - Are there any outliers in protein? 9 \tCluster \tGiven a set of data cases, find clusters of similar attribute values. \tWhich data cases in a set S of data cases are similar in value for attributes {X, Y, Z, ...}? \t- Are there groups of cereals w/ similar fat/calories/sugar?  - Is there a cluster of typical film lengths? 10 \tCorrelate \tGiven a set of data cases and two attributes, determine useful relationships between the values of those attributes. \tWhat is the correlation between attributes X and Y over a given set S of data cases? \t- Is there a correlation between carbohydrates and fat?  - Is there a correlation between country of origin and MPG?  - Do different genders have a preferred payment method?  - Is there a trend of increasing film length over the years? Barriers to effective analysis  Barriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis. Confusing fact and opinion  You are entitled to your own opinion, but you are not entitled to your own facts. Daniel Patrick Moynihan  Effective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011-2020 time period would add approximately $3.3 trillion to the national debt.[15] Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.  As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects.\" This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous. Cognitive biases  There are a variety of cognitive biases that can adversely effect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.  Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.[16] Innumeracy  Effective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate. Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.[17]  For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization[5] or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.  Analysts may also analyze data under different assumptions or scenarios. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock. Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures. Other topics Analytics and business intelligence Main article: Analytics  Analytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that use data to understand and analyze business performance.[18] Education Analytic activities of data visualization users  In education, most educators have access to a data system for the purpose of analyzing student data.[19] These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.[20] Practitioner notes  This section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article. Initial data analysis  The most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:[21] Quality of data  The quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms, n: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.      Test for common-method variance.  The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.[22] Quality of measurements  The quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature. There are two ways to assess measurement      Analysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's α of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale.[23]  Initial transformations  After assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.[24] Possible transformations of variables are:[25]      Square root transformation (if the distribution differs moderately from normal)     Log-transformation (if the distribution differs substantially from normal)     Inverse transformation (if the distribution differs severely from normal)     Make categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)  Did the implementation of the study fulfill the intentions of the research design?  One should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample. Other possible data distortions that should be checked are:      dropout (this should be identified during the initial data analysis phase)     Item nonresponse (whether this is random or not should be assessed during the initial data analysis phase)     Treatment quality (using manipulation checks).[26]  Characteristics of data sample  In any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase. The characteristics of the data sample can be assessed by looking at:      Basic statistics of important variables     Scatter plots     Correlations and associations     Cross-tabulations[27]  Final stage of the initial data analysis  During the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken. Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:      In the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?     In the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?     In the case of outliers: should one use robust analysis techniques?     In case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?     In the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?     In case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?[28]  Analysis  Several analyses can be used during the initial data analysis phase:[29]      Univariate statistics (single variable)     Bivariate associations (correlations)     Graphical techniques (scatter plots)  It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:[30]      Nominal and ordinal variables         Frequency counts (numbers and percentages)         Associations             circumambulations (crosstabulations)             hierarchical loglinear analysis (restricted to a maximum of 8 variables)             loglinear analysis (to identify relevant/important variables and possible confounders)         Exact tests or bootstrapping (in case subgroups are small)         Computation of new variables     Continuous variables         Distribution             Statistics (M, SD, variance, skewness, kurtosis)             Stem-and-leaf displays             Box plots  Nonlinear analysis  Nonlinear analysis will be necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods. Nonlinear data analysis is closely related to nonlinear system identification.[31] Main data analysis  In the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.[32] Exploratory and confirmatory approaches  In the main analysis phase either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.  Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.[33] Stability of results  It is important to obtain some indication about how generalizable the results are.[34] While this is hard to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing this:      Cross-validation: By splitting the data in multiple parts we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well.     Sensitivity analysis: A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do this is with bootstrapping.  Statistical methods  Many statistical methods have been used for statistical analyses. A very brief list of four of the more popular methods is:      General linear model: A widely used model on which various methods are based (e.g. t test, ANOVA, ANCOVA, MANOVA). Usable for assessing the effect of several predictors on one or more continuous dependent variables.     Generalized linear model: An extension of the general linear model for discrete dependent variables.     Structural equation modelling: Usable for assessing latent structures from measured manifest variables.     Item response theory: Models for (mostly) assessing one latent variable from several binary measured variables (e.g. an exam).  Free software for data analysis      NCA Calculator      - a simple online calculator for finding necessary but not sufficient conditions in datasets     NCA Software      - R package for finding necessary but not sufficient conditions in datasets     Data Applied - an online data mining and data visualization solution.     DataMelt - a multiplatform (Java-based) data analysis framework from the jWork.ORG      community of developers led by Dr. S.Chekanov     DevInfo - a database system endorsed by the United Nations Development Group for monitoring and analyzing human development.     ELKI - data mining framework in Java with data mining oriented visualization functions.     KNIME - the Konstanz Information Miner, a user friendly and comprehensive data analytics framework.     MEPX      - cross platform tool for regression and classification problems.     PAW - FORTRAN/C data analysis framework developed at CERN     Orange - A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.     QSoas - An open source, command-driven program for analyzing y=f(x) data (noise removal, baseline corrections, global fitting the solutions of differential equations or kinetic schemes, and more). Binaries available for Mac OSX and Windows. http://www.qsoas.org      [35]     R - a programming language and software environment for statistical computing and graphics.     ROOT - C++ data analysis framework developed at CERN     dotplot - cloud based visual designer to create analytic models[36]     SciPy - A set of Python tools for data analysis http://scipy.org/stackspec.html      Statsmodels - a Python module that allows users to explore data, estimate statistical models, and perform statistical tests http://statsmodels.sourceforge.net/      Pandas - A software library written for the Python programming language for data manipulation and analysis.     myInvenio [37]- a cloud based solution to automatically discover processes from event logs.  See also Portal icon \tstatistics portal      Analytics     Business intelligence     Censoring (statistics)     Computational physics     Data acquisition     Data governance     Data mining     Data Presentation Architecture     Data science     Digital signal processing     Dimension reduction     Early case assessment     Exploratory data analysis     Fourier analysis     Machine learning     Multilinear PCA     Multilinear subspace learning     Multiway Data Analysis     Nearest neighbor search     nonlinear system identification     Predictive analytics     Principal component analysis     Qualitative research     Scientific computing     Structured data analysis (statistics)     system identification     Test method     Text analytics     Unstructured data     Wavelet       In the Information Age, data is no longer scarce – it’s overpowering. The key is to sift through the overwhelming volume of data available to organizations and businesses and correctly interpret its implications. But to sort through all this information, you need the right statistical data analysis tools.  With the current obsession over “big data,” analysts have produced a lot of fancy tools and techniques available to large organizations. However, there are a handful of basic data analysis tools that most organizations aren’t using…to their detriment.  We suggest starting your data analysis efforts with the following five fundamentals – and learn to avoid their pitfalls – before advancing to more sophisticated techniques. 1. Mean  The arithmetic mean, more commonly known as “the average,” is the sum of a list of numbers divided by the number of items on the list. The mean is useful in determining the overall trend of a data set or providing a rapid snapshot of your data. Another advantage of the mean is that it’s very easy and quick to calculate.  Pitfall:  Taken alone, the mean is a dangerous tool. In some data sets, the mean is also closely related to the mode and the median (two other measurements near the average). However, in a data set with a high number of outliers or a skewed distribution, the mean simply doesn’t provide the accuracy you need for a nuanced decision. 2. Standard Deviation  The standard deviation, often represented with the Greek letter sigma, is the measure of a spread of data around the mean. A high standard deviation signifies that data is spread more widely from the mean, where a low standard deviation signals that more data align with the mean. In a portfolio of data analysis methods, the standard deviation is useful for quickly determining dispersion of data points.  Pitfall:  Just like the mean, the standard deviation is deceptive if taken alone. For example, if the data have a very strange pattern such as a non-normal curve or a large amount of outliers, then the standard deviation won’t give you all the information you need. 3. Regression  Regression models the relationships between dependent and explanatory variables, which are usually charted on a scatterplot. The regression line also designates whether those relationships are strong or weak. Regression is commonly taught in high school or college statistics courses with applications for science or business in determining trends over time.  Pitfall:  Regression is not very nuanced. Sometimes, the outliers on a scatterplot (and the reasons for them) matter significantly. For example, an outlying data point may represent the input from your most critical supplier or your highest selling product. The nature of a regression line, however, tempts you to ignore these outliers. As an illustration, examine a picture of Anscombe’s quartet, in which the data sets have the exact same regression line but include widely different data points. 4. Sample Size Determination  When measuring a large data set or population, like a workforce, you don’t always need to collect information from every member of that population – a sample does the job just as well. The trick is to determine the right size for a sample to be accurate. Using proportion and standard deviation methods, you are able to accurately determine the right sample size you need to make your data collection statistically significant.  Pitfall:  When studying a new, untested variable in a population, your proportion equations might need to rely on certain assumptions. However, these assumptions might be completely inaccurate. This error is then passed along to your sample size determination and then onto the rest of your statistical data analysis 5. Hypothesis Testing  Also commonly called t testing, hypothesis testing assesses if a certain premise is actually true for your data set or population. In data analysis and statistics, you consider the result of a hypothesis test statistically significant if the results couldn’t have happened by random chance. Hypothesis tests are used in everything from science and research to business and economic  Pitfall:  To be rigorous, hypothesis tests need to watch out for common errors. For example, the placebo effect occurs when participants falsely expect a certain result and then perceive (or actually attain) that result. Another common error is the Hawthorne effect (or observer effect), which happens when participants skew results because they know they are being studied.  Overall, these methods of data analysis add a lot of insight to your decision-making portfolio, particularly if you’ve never analyzed a process or data set with statistics before. However, avoiding the common pitfalls associated with each method is just as important. Once you master these fundamental techniques for statistical data analysis, then you’re ready to advance to more powerful data analysis tools. Statistical software are specialized computer programs for analysis in statistics and econometrics.  Contents      1 Open-source     2 Public domain     3 Freeware     4 Proprietary         4.1 Add-ons     5 See also     6 References     7 External links  Open-source gretl is an example of an open-source statistical package      ADaMSoft – a generalized statistical software with data mining algorithms and methods for data management     ADMB – a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation     Bayesian Filtering Library     Chronux – for neurobiological time series data     CBEcon – web-based econometrics and statistical software     DataMelt (DMelt) – Java-based statistical analysis framework for scientists and engineers. It includes an IDE     DAP – free replacement for SAS     Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) a software framework for developing data mining algorithms in Java     Fityk – nonlinear regression software (GUI and command line)     GNU Octave – programming language very similar to MATLAB with statistical features     gretl – gnu regression, econometrics and time-series library     intrinsic Noise Analyzer (iNA) – For analyzing intrinsic fluctuations in biochemical systems     JASP – A free software alternative to IBM SPSS Statistics with additional option for Bayesian methods     Just another Gibbs sampler (JAGS) – a program for analyzing Bayesian hierarchical models using Markov chain Monte Carlo developed by Martyn Plummer. It is similar to WinBUGS     JMulTi     LDT - Automatic Time Series Analysis with Stationary VAR Models     LIBSVM – C++ support vector machine libraries     MLPACK (C++ library) – open-source library for machine learning, exploits C++ language features to provide maximum performance and flexibility while providing a simple and consistent application programming interface (API)     Mondrian – data analysis tool using interactive statistical graphics with a link to R     Neurophysiological Biomarker Toolbox - Matlab toolbox for data-mining of neurophysiological biomarkers     OpenBUGS     OpenEpi – A web-based, open-source, operating-independent series of programs for use in epidemiology and statistics based on JavaScript and HTML     OpenNN – A software library written in the programming language C++ which implements neural networks, a main area of deep learning research     OpenMx – A package for structural equation modeling running in R (programming language)     Orange, a data mining, machine learning, and bioinformatics software     Pandas – High-performance computing (HPC) data structures and data analysis tools for Python in Python and Cython (statsmodels, scikit-learn)     Perl Data Language – Scientific computing with Perl     Ploticus – software for generating a variety of graphs from raw data     PSPP – A free software alternative to IBM SPSS Statistics     R – free implementation of the S (programming language)         Programming with Big Data in R (pbdR) – a series of R packages enhanced by SPMD parallelism for big data analysis         R Commander – GUI interface for R         Rattle GUI – GUI interface for R         Revolution Analytics – production-grade software for the enterprise big data analytics         RStudio – GUI interface and development environment for R     ROOT – an open-source C++ system for data storage, processing and analysis, developed by CERN and used to find the Higgs boson     Salstat - menu-driven statistics software     Scilab – uses GPL-compatible CeCILL license     SciPy – Python library for scientific computing that contains the stats sub-package which is partly based on the venerable |STAT (a.k.a. PipeStat, formerly UNIX|STAT) software         scikit-learn - extends SciPy with a host of machine learning models (classification, clustering, regression, etc.)         statsmodels - extends SciPy with statistical models and tests (regression, plotting, example datasets, generalized linear model (GLM), time series analysis, autoregressive–moving-average model (ARMA), vector autoregression (VAR), non-parametric statistics, ANOVA, empirical likelihood)     Shogun (toolbox) – open-source, large-scale machine learning toolbox that provides several SVM (Support Vector Machine) implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R     Simfit – simulation, curve fitting, statistics, and plotting     SOCR     SOFA Statistics – desktop GUI program focused on ease of use, learn as you go, and beautiful output     Stan (software) – open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. It’s somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors     Statistical Lab – R-based and focusing on educational purposes     Torch (machine learning) – a deep learning software library written in Lua (programming language)     Weka (machine learning) – a suite of machine learning software written at the University of Waikato  Public domain      CSPro     Epi Info     X-12-ARIMA  Freeware      BV4.1     GeoDA     MaxStat Lite – general statistical software     MINUIT     WinBUGS – Bayesian analysis using Markov chain Monte Carlo methods     Winpepi – package of statistical programs for epidemiologists  Proprietary      Analytica - visual analytics and statistics package     Angoss - products KnowledgeSEEKER and KnowledgeSTUDIO incorporate several data mining algorithms     ASReml – for restricted maximum likelihood analyses     BMDP – general statistics package     Data Applied – for building statistical models     DB Lytix - 800+ in-database models     EViews – for econometric analysis     FAME (database) – a system for managing time-series databases     GAUSS – programming language for statistics     Genedata – software solution for integration and interpretation of experimental data in the life science R&D     GenStat – general statistics package     GLIM – early package for fitting generalized linear models     GraphPad InStat – very simple with lots of guidance and explanations     GraphPad Prism – biostatistics and nonlinear regression with clear explanations     IMSL Numerical Libraries – software library with statistical algorithms     JMP – visual analysis and statistics package     LIMDEP – comprehensive statistics and econometrics package     LISREL – statistics package used in structural equation modeling     Maple – programming language with statistical features     Mathematica – a software package with statistical particularlyŋ features     MATLAB – programming language with statistical features     MaxStat Pro – general statistical software     MedCalc – for biomedical sciences     Microfit – econometrics package, time series     Minitab – general statistics package     MLwiN – multilevel models (free to UK academics)     NAG Numerical Library – comprehensive math and statistics library     Neural Designer – commercial deep learning package     NCSS – general statistics package     NLOGIT – comprehensive statistics and econometrics package     NMath Stats – statistical package for .NET Framework     O-Matrix – programming language     OriginPro – statistics and graphing, programming access to NAG library     PASS Sample Size Software (PASS) – power and sample size software from NCSS     Plotly – plotting library and styling interface for analyzing data and creating browser-based graphs. Available for R, Python, MATLAB, Julia, and Perl     Primer-E Primer – environmental and ecological specific     PV-WAVE – programming language comprehensive data analysis and visualization with IMSL statistical package     Qlucore Omics Explorer - interactive and visual data analysis software     Quantum Programming Language – part of the SPSS MR product line, mostly for data validation and tabulation in Marketing and Opinion Research     RapidMiner – machine learning toolbox     Regression Analysis of Time Series (RATS) – comprehensive econometric analysis package     SAS (software) – comprehensive statistical package     SHAZAM (Econometrics and Statistics Software) – comprehensive econometrics and statistics package     Simul - econometric tool for multidimensional (multi-sectoral, multi-regional) modeling     SigmaStat – package for group analysis     SmartPLS - statistics package used in partial least squares path modeling (PLS) and PLS-based structural equation modeling     SOCR – online tools for teaching statistics and probability theory     Speakeasy (computational environment) – numerical computational environment and programming language with many statistical and econometric analysis features     SPSS Modeler – comprehensive data mining and text analytics workbench     SPSS Statistics – comprehensive statistics package that stands for \"Statistical Package for the Social Sciences\"     Stata – comprehensive statistics package     Statgraphics – general statistics package to include cloud computing and Six Sigma for use in business development, process improvement, data visualization and statistical analysis, design of experiment, point processes, geospatial analysis, regression, and time series analysis are all included within this complete statistical package.     Statistica – comprehensive statistics package     StatsDirect – statistics package designed for biomedical, public health and general health science uses     StatXact – package for exact nonparametric and parametric statistics     Systat – general statistics package     SuperCROSS - comprehensive statistics package with ad-hoc, cross tabulation analysis     S-PLUS – general statistics package     Unistat – general statistics package that can also work as Excel add-in     The Unscrambler - free-to-try commercial multivariate analysis software for Windows     Wolfram Language[1] - the computer language that evolved from the program Mathematica. It has similar statistical capabilities as Mathematica.     World Programming System (WPS) – statistical package that supports the SAS language     XploRe  Add-ons      Analyse-it – add-on to Microsoft Excel for statistical analysis     NumXL – add-on to Microsoft Excel for statistical and time series analysis     SigmaXL – add-on to Microsoft Excel for statistical and graphical analysis     SPC XL – add-on to Microsoft Excel for general statistics     Statgraphics Sigma Express - add-on to Microsoft Excel for Six Sigma statistical analysis     SUDAAN – add-on to SAS and SPSS for statistical surveys     XLfit add-on to Microsoft Excel for curve fitting and statistical analysis  See also      Comparison of statistical packages     Econometric software     Free statistical software     List of computer algebra systems     List of graphing software     List of numerical analysis software     List of numerical libraries     Mathematical software     Psychometric software  Statistical Techniques  Statistical studies allow analysts to estimate key parameters of cost or production models. Econometric analyses require a large data set to ensure reliable results.  Obtaining the number of observations needed to derive an efficient and unbiased estimate of cost (or production) structures can often prove to be a difficult task.  Regression results are sensitive to model specification (for example, a linear vs. a non-linear functional form).  In addition, for some models, the interpretation of the error term becomes important.   The early studies tended to utilize Ordinary Least Squares (OLS) to estimate cost functions for firms.  Due to data limitations, most of these studies were cross-sectional in nature.  Besides using data from only a single year researchers utilized data from England and Wales or from the United States. These academic studies often focused on the relative performance of private vs. publicly-owned water and sewerage utilities.  In addition, they investigated the extent of scale economies and economies of joint production (providing both water and sewerage services).  In some cases, they considered the impacts of residential vs. industrial/commercial customers.   As data from Brazil, Peru, and other emerging nations became available, additional country studies were publishedoften using more advanced econometric (parametric) or non-parametric data analysis techniques. Studies of utilities in France, Italy, and other nations began to appear in the academic literature. Techniques associated with Stochastic Frontier Analysis began to be applied to both production functions and cost functions.  Panel data facilitated the incorporation of customer density, topology, and other variables.   The most commonly used parametric methods are ordinary least squares (OLS) ,  corrected ordinary least squares (COLS) models and  Stochastic Frontier Analysis (SFA). The main difference between these models is that COLS attributes all the deviations to inefficiency while SFA models attribute part of the deviations to inefficiency and part of the deviations to random noise. In other words, the SFA models take both inefficiency and random noise into account. The most widely used stochastic frontier models include the stochastic production frontier model, stochastic cost frontier model, and stochastic distance function model. Before selecting a specific model, analysts have to make an initial choice between the two most widely used functional forms: Cobb-Douglas function and translog function.   Ordinary least squares (OLS) models  OLS techniques can be used to perform benchmarking that relates individual firm performance relative to what would be expected: an estimate of an average production or cost function of a sample of firms.  Average benchmarking methods may be used to compare firms with relatively similar costs or when there is a lack of sufficient data of comparable firms for the application of frontier methods.  Basically, the method refers to the estimation of a regression functional form for costs or production using the OLS approach.  Linear regression analysis seeks to derive a relationship between firm performance (in terms of output or total cost) and market conditions and characteristics of the production processes.  Statistical analysis can isolate the impacts of specific conditions or levels of outputso the roles of multiple independent variables can be determined.  Data from the firms being compared can then be used to arrive at expected dimensions of firm performance, given the variables characterizing each firm.   The technique of regression analysis is defined by the following steps: 1) selecting both the cost (or output) measure and exogenous variables, 2) estimating a cost (or production) function for the industry, and 3) calculating the efficiency coefficient for each firm within the industry.  Predicted versus actual output provides a measure of relative performance.  The quality of these results can then be statistically evaluated to provide the policy-maker with a framework for evaluating firms. The linear vs. non-linear issue can be examined by including parameters that capture scale economies or diseconomies.       Advantages:  The statistical method reveals information about cost structures and distinguishes between different variables’ roles in affecting output.  Coefficients can be interpreted in terms of cost drivers or how inputs contribute to output.      Disadvantages:  Large data set is necessary in order to obtain reliable results.  The regression results are sensitive to functional form if the error term is not adequately interpreted, which can lead to widely varying conclusions, depending on how the regression is initially set up.       Application:  The UK water regulator OFWAT applies mean and average methods to the operating costs (OPEX) and capital expenditures (CAPEX) of water utilities when determining the price caps every five years. OFWAT has developed an efficiency analysis relying on mean and average methods that is a key part of its price determination process.   Corrected ordinary least squares (COLS) models  A slightly different approach than OLS involves shifting the line towards the best performing company, which is called Corrected Least Squares methodology (COLS).  In a general sense, COLS is merely a shifted average function. Two steps are needed, one to get the expected value of the error term and another to shift or to “center” the equation.   When using OLS or COLS it is good practice to perform Quantile analysis. Quantile analysis helps to overcome the possible effect of outliers on the estimated mean allowing the analyst to detect the presence of performers on specific or extreme quantiles such as the lower (25%) or the upper (75%) quantiles.      Advantages: The statistical method reveals information about cost structures and distinguishes between different variables’ roles in affecting output.  The adjustment turns the OLS into a “frontier” approach.      Disadvantages: As with OLS, a large data set is necessary in order to obtain reliable results.  The regression results are sensitive to functional form if the error term is not adequately interpreted, which can lead to widely varying conclusions depending on how the regression is initially set up.  Furthermore, the results are especially sensitive to outliers, since the “best” performer along any dimension serves as the anchor for the estimate.  Thus, the performance scores are very sensitive to outliers.      Application:  Most studies that analyze frontier relationships utilize Stochastic Frontier Analysis (SFA).  Some simplicity is then lost, but tests of the sources of different types of errors can be identified with SFA.    Ordinary least squares (OLS) models  Stochastic Frontier Analysis attempts to estimate an efficient frontier which does incorporates the possibility of measurement error or chance factors in its estimation. To separate inefficiency and noise, strong assumptions are needed on the distribution of noise among each observed firm.  Stochastic frontiers may be classified as Production, Cost, and Input Distance frontiers.   A production frontier reveals technical relationships between inputs and outputs of firms and represents an alternative when cost frontiers can not be calculated due to lack of data. The estimated output is the maximum possible output for given inputs of an individual firm.  The output difference obtained in the estimation is interpreted as technical inefficiency of each individual firm.  On a production frontier, variable returns to scale is the sensible option and appropriate scale efficiency changes need to be included when calculating total factor productivity.   A cost frontier shows costs as a function of the level of output/s and the prices of inputs. It is useful when trying to access the wedge between tariff and minimum costs. Conceptually, the minimum cost function defines a frontier showing costs technically possible associated with various levels of inputs and control variables.  Total cost frontier rather than variable or expenditure cost frontier is preferable to account for substitutability of factor inputs.  Separate models for CAPEX and OPEX do not allow for allocation of expenditures between operating and capital expenditure.  Cost efficiency contains the effects of technical and allocative efficiency.   Each approach (production or cost) may yield different results.  The difference will be larger if large allocative distortions are present.  In this case, the parameters of the cost frontier will be biased.  An important factor to consider when choosing between a cost frontier and a production frontier is that usually regulated firms are required to provide the service at a preset tariff and they must meet demand.  In this sense, firms are not allowed to choose their own level of output which makes output an exogenous variable.  The regulated firm maximizes benefits by minimizing its costs of producing a given level of output.  Cost is the choice variable for the firm so a cost frontier approach is a more sensible choice.   Finally, an input distance frontier is the natural option for regulated industries where output quantity is exogenous and input quantities are endogenous, and when the nature of the technology is multiple outputs or there is not data available on price of inputs.  This is the case for water and sewerage as different outputs under the same firm where their provision comes from shared inputs which jointly determine the production function.   A distance function may have either an input or an output orientation.  An input orientation looks at how much the input vector may be proportionally contracted with the output vector held fixed. An output orientation looks at how much the output vector may be proportionally expanded with the input vector held fixed.   Input distance functions can be estimated by either stochastic or DEA methods. The advantage of a distance frontier with regard to a cost frontier is that firm is not assumed to be minimizing costs. With respect to production frontier is that it avoids the endogenous problem.      Advantages of Stochastic Frontiers:  Accounts for data noise such as data errors and omitted variables.  Standard statistical tests can be used to test hypotheses on model specification and significance of the variables included on the model.  It is also more amenable to modeling effects of other variables (e.g., environment, quality)      Disadvantages of Stochastic Frontiers: There is a need of functional form and production technology specification. Also, the separation of noise and inefficiency relies on strong assumptions on the distribution of the error term      Application: A number of studies utilize these techniques, such as the relative efficiency of public and private water companies in East Asia and the Pacific.   Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data.[1] In applying statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]  Some popular definitions are:      Merriam-Webster dictionary defines statistics as \"classified facts representing the conditions of a people in a state – especially the facts that can be stated in numbers or any other tabular or classified arrangement[2]\".     Statistician Sir Arthur Lyon Bowley defines statistics as \"Numerical statements of facts in any department of inquiry placed in relation to each other[3]\".  When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.  Two main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.  A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\").[5] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]  Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.  Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data. Contents      1 Scope         1.1 Mathematical statistics     2 Overview     3 Data collection         3.1 Sampling         3.2 Experimental and observational studies     4 Types of data     5 Terminology and theory of inferential statistics         5.1 Statistics, estimators and pivotal quantities         5.2 Null hypothesis and alternative hypothesis         5.3 Error         5.4 Interval estimation         5.5 Significance         5.6 Examples     6 Misuse         6.1 Misinterpretation: correlation     7 History of statistical science     8 Applications         8.1 Applied statistics, theoretical statistics and mathematical statistics         8.2 Machine learning and data mining         8.3 Statistics in society         8.4 Statistical computing         8.5 Statistics applied to mathematics or the arts     9 Specialized disciplines     10 See also     11 References     12 Further reading     13 External links  Scope  Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[6] or as a branch of mathematics.[7] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[8][9] Mathematical statistics Main article: Mathematical statistics  Mathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state — the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[10][11] Overview  In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".  Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).  When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining. Data collection Sampling  When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.  Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population. Experimental and observational studies  A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data – like natural experiments and observational studies[12] – for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators. Experiments  The basic steps of a statistical experiment are:      Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.     Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.     Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.     Further examining the data set in secondary analyses, to suggest new hypotheses for future study.     Documenting and presenting the results of the study.  Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[13] Observational study  An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group. Types of data Main articles: Statistical data type and Levels of measurement  Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.  Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.  Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[14] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[15] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[16] van den Berg (1991).[17]  The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).[18] Terminology and theory of inferential statistics Statistics, estimators and pivotal quantities  Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[19] The population being examined is described by a probability distribution that may have unknown parameters.  A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.  Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.  A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.  Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.  Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.  This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations. Null hypothesis and alternative hypothesis  Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[20][21]  The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.  What statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis. Error  Working from a null hypothesis, two basic forms of error are recognized:      Type I errors where the null hypothesis is falsely rejected giving a \"false positive\".     Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\".  Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.  A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).  Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error. A least squares fit: in red the points to be fitted, in blue the fitted line.  Many statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.  Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.[22] Interval estimation Main article: Interval estimation Confidence intervals: the red line is true value for the mean in this example, the blue lines are random confidence intervals for 100 realizations.  Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.  In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds. Significance Main article: Statistical significance  Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value). In this graph the black line is probability distribution for the test statistic, the critical region is the set of values to the right of the observed data point (observed value of the test statistic) and the p-value is represented by the green area.  The standard approach[19] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.  Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.  While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.  Some problems are usually associated with this framework (See criticism of hypothesis testing):      A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.     Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.[23]     Rejecting the null hypothesis does not automatically prove the alternative hypothesis.     As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.  Examples  Some well-known statistical tests and procedures are:      Analysis of variance (ANOVA)     Chi-squared test     Correlation     Factor analysis     Mann–Whitney U     Mean square weighted deviation (MSWD)     Pearson product-moment correlation coefficient     Regression analysis     Spearman's rank correlation coefficient     Student's t-test     Time series analysis     Conjoint Analysis  Misuse Main article: Misuse of statistics  Misuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.  Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.  There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[24] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[24] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[25]  Ways to avoid misuse of statistics include using proper diagrams and avoiding bias.[26] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[27] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[26] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[27] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[28] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[29]  To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[30]      Who says so? (Does he/she have an axe to grind?)     How does he/she know? (Does he/she have the resources to know the facts?)     What’s missing? (Does he/she give us a complete picture?)     Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)     Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)  The confounding variable problem: X and Y may be correlated, not because there is causal relationship between them, but because both depend on a third variable Z. Z is called a confounding factor. Misinterpretation: correlation  The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.) History of statistical science Gerolamo Cardano, the earliest pioneer on the mathematics of probability. Main articles: History of statistics and Founders of statistics  Statistical methods date back at least to the 5th century BC.  Some scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[31] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.  Its mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[32] The method of least squares was first described by Adrien-Marie Legendre in 1805. Karl Pearson, a founder of mathematical statistics.  The modern field of statistics emerged in the late 19th and early 20th century in three stages.[33] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics – height, weight, eyelash length among others.[34] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[35] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[36] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[37]  Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[38][39]  The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[40][41][42][43] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[44] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[45]). Nevertheless, A. W. F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\".[45] (about the sex ratio), the Fisherian runaway,[46][47][48][49][50][51] a concept in sexual selection about a positive feedback runaway affect found in evolution.  The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[52]  Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[53] Applications Applied statistics, theoretical statistics and mathematical statistics  \"Applied statistics\" comprises descriptive statistics and the application of inferential statistics.[54][55] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments. Machine learning and data mining  There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis. Statistics in society  Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions. Statistical computing gretl, an example of an open source statistical package Main article: Computational statistics  The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.  Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Statistics applied to mathematics or the arts  Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.      In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.     Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.[citation needed]     The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed.[citation needed] With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.[citation needed]     Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.     Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.  Specialized disciplines Main article: List of fields of application of statistics  Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:      Actuarial science (assesses risk in the insurance and finance industries)     Applied information economics     Astrostatistics (statistical evaluation of astronomical data)     Biostatistics     Business statistics     Chemometrics (for analysis of data from chemistry)     Data mining (applying statistics and pattern recognition to discover knowledge from data)     Data science     Demography     Econometrics (statistical analysis of economic data)     Energy statistics     Engineering statistics     Epidemiology (statistical analysis of disease)     Geography and Geographic Information Systems, specifically in Spatial analysis     Image processing     Medical Statistics     Psychological statistics     Reliability engineering     Social statistics     Statistical Mechanics  In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:      Bootstrap / Jackknife resampling     Multivariate statistics     Statistical classification     Structured data analysis (statistics)     Structural equation modelling     Survey methodology     Survival analysis     Statistics in various sports, particularly baseball - known as Sabermetrics - and cricket  Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool. See also Library resources about Statistics      Resources in your library   Main article: Outline of statistics      Abundance estimation     Data science     Glossary of probability and statistics     List of academic statistical associations     List of important publications in statistics     List of national and international statistical services     List of statistical packages (software)     List of statistics articles     List of university statistical consulting centers     Notation in probability and statistics  Foundations and major areas of statistics      Foundations of statistics     List of statisticians     Official statistics     Multivariate analysis of variance", "category": "Edison", "id": 119}
{"skillName": "DSRMP05", "skillText": "What are the research objectives?  In general, research objectives describe what we expect to achieve by a project.  Research objectives are usually expressed in lay terms and are directed as much to the client as to the researcher. Research objectives may be linked with a hypothesis or used as a statement of purpose in a study that does not have a hypothesis.  Even if the nature of the research has not been clear to the layperson from the hypotheses, s/he should be able to understand the research from the objectives.  A statement of research objectives can serve to guide the activities of research. Consider the following examples.      Objective: To describe what factors farmers take into account in making such decisions as whether to adopt a new technology or what crops to grow.     Objective: To develop a budget for reducing pollution by a particular enterprise.     Objective: To describe the habitat of the giant panda in China.  In the above examples the intent of the research is largely descriptive.      In the case of the first example, the research will end the study by being able to specify factors which emerged in household decisions.     In the second, the result will be the specification of a pollution reduction budget.     In the third, creating a picture of the habitat of the giant panda in China.  These observations might prompt researchers to formulate hypotheses which could be tested in another piece of research. So long as the aim of the research is exploratory, ie to describe what is, rather than to test an explanation for what is, a research objective will provide an adequate guide to the research.   Research comprises \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\"[1] It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects, or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life,technological,etc.  Contents      1 Forms of research     2 Etymology     3 Definitions     4 Steps in conducting research     5 Scientific research     6 Historical method     7 Research methods         7.1 Research method controversies             7.1.1 Quantitative vs. Qualitative war             7.1.2 Anti-methodology             7.1.3 Methodological academic imperialism     8 Professionalisation         8.1 In Russia     9 Publishing     10 Research funding     11 Original research         11.1 Different forms     12 Artistic research     13 See also     14 References     15 Further reading     16 External links  Forms of research  Scientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, such as business schools, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).[2]  Research in the humanities involves different methods such as for example hermeneutics and semiotics, and a different, more relativist epistemology. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past.  Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth. Etymology Aristotle, 384 BC – 322 BC, - one of the early figures in the development of the scientific method.[3]  The word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'.[4] The earliest recorded use of the term was in 1577.[4] Definitions  Research has been defined in a number of different ways.  A broad definition of research is given by Martyn Shuttleworth - \"In the broadest sense of the word, the definition of research includes any gathering of data, information and facts for the advancement of knowledge.\"[5]  Another definition of research is given by Creswell who states that - \"Research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: Pose a question, collect data to answer the question, and present an answer to the question.[6]  The Merriam-Webster Online Dictionary defines research in more detail as \"a studious inquiry or examination; especially investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".[4] Steps in conducting research  Research is often conducted using the hourglass model structure of research.[7] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[8]      Identification of research problem     Literature review     Specifying the purpose of research     Determine specific research questions     Specification of a conceptual framework, usually a set of hypotheses[9]     Choice of a methodology (for data collection)     Data collection     Verify data     Analyzing and interpreting the data     Reporting and evaluating research     Communicating the research findings and, possibly, recommendations  The steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[10] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[11] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in confirming or failing to reject the Null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the flip approach: starting with articulating findings and discussion of them, moving \"up\" to identification research problem that emerging in the findings and literature review introducing the findings. The flip approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings fully emerged and interpreted.  Rudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"[12]  Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrase in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"[13] Scientific research Main article: Scientific method Primary scientific research being carried out at the Microscopy Laboratory of the Idaho National Laboratory. Scientific research equipment at MIT.  Generally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:      Observations and Formation of the topic: Consists of the subject area of ones interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.     Hypothesis: A testable prediction which designates the relationship between two or more variables.     Conceptual definition: Description of a concept by relating it to other concepts.     Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.     Gathering of data: Consists of identifying a population and selecting samples, gathering information from and/or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.     Analysis of data: Involves breaking down the individual pieces of data in order to draw conclusions about it.     Data Interpretation: This can be represented through tables, figures and pictures, and then described in words.     Test, revising of hypothesis     Conclusion, reiteration if necessary  A common misconception is that a hypothesis will be proven (see, rather, Null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.  A useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which state no relationship or difference between the independent or dependent variables. A null hypothesis uses a sample of all possible people to make a conclusion about the population.[14] Historical method Main article: Historical method German historian Leopold von Ranke (1795-1886), considered to be one of the founders of modern source-based history.  The historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[15]      Identification of origin date     Evidence of localization     Recognition of authorship     Analysis of data     Identification of integrity     Attribution of credibility  Research methods The research room at the New York Public Library, an example of secondary research in progress. Maurice Hilleman is credited with saving more lives than any other scientist of the 20th century.[16]  The goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):      Exploratory research, which helps to identify and define a problem or question.     Constructive research, which tests theories and proposes solutions to a problem or question.     Empirical research, which tests the feasibility of a solution using empirical evidence.  There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:  Qualitative research     Understanding of human behavior and the reasons that govern such behavior. Asking a broad question and collecting data in the form of words, images, video etc that is analyzed and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time-consuming, and typically limited to a single set of research subjects.[citation needed] Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses.[citation needed] Qualitative research is linked with the philosophical and theoretical stance of social constructionism.  Quantitative research     Systematic empirical investigation of quantitative properties and phenomena and their relationships. Asking a narrow question and collecting numerical data to analyze utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[17] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.  The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories.[citation needed] These methods produce results that are easy to summarize, compare, and generalize.[citation needed] Quantitative research is concerned with testing hypotheses derived from theory and/or being able to estimate the size of a phenomenon of interest. Depending on the research question, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics in order to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[18]  In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[19]  Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[20]  Big data has brought big impacts on research methods that now researchers do not put much effort on data collection, and also methods to analyze easily available huge amount of data have also changed.[21]  Nonempirical refers to an approach that is grounded in theory as opposed to using observation and experimentation to achieve the outcome. As such, nonempirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool existing and established knowledge. Nonempirical is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose within life and in science. A simple example of a nonempirical task could the prototyping of a new drug using a differentiated application of existing knowledge; similarly, it could be the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Empirical research on the other hand seeks to create new knowledge through observations and experiments in which established knowledge can either be contested or supplements. Research method controversies  There have been many controversies about research methods stemmed from a philosophical positivism promise to distinguish the science from other practices (especially religion) by its method. This promise leads to methodological hegemony and methodology wars where diverse researchers, often coming from opposing paradigms, try to impose their own methodology on the entire field or even on the science practice in general as the only legitimate one.[citation needed] Quantitative vs. Qualitative war Anti-methodology  According to this view, general scientific methodology does not exist and attempts to impose it on scientists is counterproductive. Each particular research with its emerging particular inquiries requires and should produce its own way (method) of researching. Similar to the art practice, the notion of methodology has to be replaced with the notion of research mastery.[22] Methodological academic imperialism  Epistemologies of different national sciences and cultural communities may differ and, thus, they may produce different methods of research. For example, psychological research in Russia tends to be rooted in philosophy while in the US and UK in empirism.[23][24][25] Rich countries (and dominant cultural communities within them) and their national sciences may dominate scientific discourse through funding and publications. This academic hegemony can translate into impositions of certain research methodologies through the gatekeeping process of international academic publications, conference presentation selection, institutional review boards, and funding.[26] Professionalisation Globe icon. \tThe examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (January 2014) (Learn how and when to remove this template message) See also: Academic ranks, Academics, and Scientists  In several national and private academic systems, the professionalization of research has resulted in formal job titles. In Russia  In present-day Russia, the former Soviet Union and in some Post-Soviet states the term researcher (Russian: Научный сотрудник, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc.  The following ranks are known:      Junior Researcher (Junior Research Associate)     Researcher (Research Associate)     Senior Researcher (Senior Research Associate)     Leading Researcher (Leading Research Associate)[27]     Chief Researcher (Chief Research Associate)  Publishing Cover of the first issue of Nature, 4 November 1869.  Academic publishing describes a system that is necessary in order for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field, and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.  Most established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields; from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently.[28] It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its factors in order to prevent the publication of unproven findings.[29] Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access.[30] There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web. Research funding Main article: Funding of science  Most funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA[31] and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research, but also as a source of merit.  The Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources. Original research Original research redirects here, for the Wikipedia policy see Wikipedia:No original research  Original research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).[32][33] Different forms  Original research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[34]  The degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review.[35] Graduate students are commonly required to perform original research as part of a dissertation.[36] Artistic research  The controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[37] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[38]  Artistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods and criticality. Through presented documentation, the insights gained shall be placed in a context.\"[39] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[40] For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.[41]  According to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\".[42] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[43]  The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[44][45] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[46][47][48] a searchable, documentary database of artistic research, to which anyone can contribute.  Patricia Leavy addresses eight arts-based research (ABR) genres, they are: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[49] See also      European Charter for Researchers     Undergraduate research     Internet research     List of countries by research and development spending     Open research     Operations research     Participatory action research     Primary research     Psychological research methods     Research-intensive cluster     Scholarly research     Secondary research     Society for Artistic Research     Timeline of the history of scientific method   Research question  Interest in a particular topic usually begins the research process, but it is the familiarity with the subject that helps define an appropriate research question for a study.1 Questions then arise out of a perceived knowledge deficit within a subject area or field of study.2 Indeed, Haynes suggests that it is important to know “where the boundary between current knowledge and ignorance lies.”1 The challenge in developing an appropriate research question is in determining which clinical uncertainties could or should be studied and also rationalizing the need for their investigation.  Increasing one’s knowledge about the subject of interest can be accomplished in many ways. Appropriate methods include systematically searching the literature, in-depth interviews and focus groups with patients (and proxies) and interviews with experts in the field. In addition, awareness of current trends and technological advances can assist with the development of research questions.2 It is imperative to understand what has been studied about a topic to date in order to further the knowledge that has been previously gathered on a topic. Indeed, some granting institutions (e.g., Canadian Institute for Health Research) encourage applicants to conduct a systematic review of the available evidence if a recent review does not already exist and preferably a pilot or feasibility study before applying for a grant for a full trial.  In-depth knowledge about a subject may generate a number of questions. It then becomes necessary to ask whether these questions can be answered through one study or if more than one study needed.1 Additional research questions can be developed, but several basic principles should be taken into consideration.1 All questions, primary and secondary, should be developed at the beginning and planning stages of a study. Any additional questions should never compromise the primary question because it is the primary research question that forms the basis of the hypothesis and study objectives. It must be kept in mind that within the scope of one study, the presence of a number of research questions will affect and potentially increase the complexity of both the study design and subsequent statistical analyses, not to mention the actual feasibility of answering every question.1 A sensible strategy is to establish a single primary research question around which to focus the study plan.3 In a study, the primary research question should be clearly stated at the end of the introduction of the grant proposal, and it usually specifies the population to be studied, the intervention to be implemented and other circumstantial factors.4  Hulley and colleagues2 have suggested the use of the FINER criteria in the development of a good research question (Box 1). The FINER criteria highlight useful points that may increase the chances of developing a successful research project. A good research question should specify the population of interest, be of interest to the scientific community and potentially to the public, have clinical relevance and further current knowledge in the field (and of course be compliant with the standards of ethical boards and national research standards). Box 1 FINER criteria for a good research question F\tFeasible\t      Adequate number of subjects     Adequate technical expertise     Affordable in time and money     Manageable in scope  I\tInteresting\t      Getting the answer intrigues investigator, peers and community  N\tNovel\t      Confirms, refutes or extends previous findings  E\tEthical\t      Amenable to a study that institutional review board will approve  R\tRelevant\t      To scientific knowledge     To clinical and health policy     To future research  Adapted with permission from Wolters Kluwer Health.2  Whereas the FINER criteria outline the important aspects of the question in general, a useful format to use in the development of a specific research question is the PICO format — consider the population (P) of interest, the intervention (I) being studied, the comparison (C) group (or to what is the intervention being compared) and the outcome of interest (O).3,5,6 Often timing (T) is added to PICO (Box 2) — that is, “Over what time frame will the study take place?”1 The PICOT approach helps generate a question that aids in constructing the framework of the study and subsequently in protocol development by alluding to the inclusion and exclusion criteria and identifying the groups of patients to be included. Knowing the specific population of interest, intervention (and comparator) and outcome of interest may also help the researcher identify an appropriate outcome measurement tool.7 The more defined the population of interest, and thus the more stringent the inclusion and exclusion criteria, the greater the effect on the interpretation and subsequent applicability and generalizability of the research findings.1,2 A restricted study population (and exclusion criteria) may limit bias and increase the internal validity of the study; however, this approach will limit external validity of the study and, thus, the generalizability of the findings to the practical clinical setting. Conversely, a broadly defined study population and inclusion criteria may be representative of practical clinical practice but may increase bias and reduce the internal validity of the study. Box 2 PICOT criteria1 P\tPopulation (patients)\t      What specific patient population are you interested in?  I\tIntervention (for intervention studies only)\t      What is your investigational intervention?  C\tComparison group\t      What is the main alternative to compare with the intervention?  O\tOutcome of interest\t      What do you intend to accomplish, measure, improve or affect?  T\tTime\t      What is the appropriate follow-up time to assess outcome  A poorly devised research question may affect the choice of study design, potentially lead to futile situations and, thus, hamper the chance of determining anything of clinical significance, which will then affect the potential for publication. Without devoting appropriate resources to developing the research question, the quality of the study and subsequent results may be compromised. During the initial stages of any research study, it is therefore imperative to formulate a research question that is both clinically relevant and answerable. Research hypothesis  The primary research question should be driven by the hypothesis rather than the data.1,2 That is, the research question and hypothesis should be developed before the start of the study. This sounds intuitive; however, if we take, for example, a database of information, it is potentially possible to perform multiple statistical comparisons of groups within the database to find a statistically significant association. This could then lead one to work backward from the data and develop the “question.” This is counterintuitive to the process because the question is asked specifically to then find the answer, thus collecting data along the way (i.e., in a prospective manner). Multiple statistical testing of associations from data previously collected could potentially lead to spuriously positive findings of association through chance alone.2 Therefore, a good hypothesis must be based on a good research question at the start of a trial and, indeed, drive data collection for the study.  The research or clinical hypothesis is developed from the research question and then the main elements of the study — sampling strategy, intervention (if applicable), comparison and outcome variables — are summarized in a form that establishes the basis for testing, statistical and ultimately clinical significance.3 For example, in a research study comparing computer-assisted acetabular component insertion versus freehand acetabular component placement in patients in need of total hip arthroplasty, the experimental group would be computer-assisted insertion and the control/conventional group would be free-hand placement. The investigative team would first state a research hypothesis. This could be expressed as a single outcome (e.g., computer-assisted acetabular component placement leads to improved functional outcome) or potentially as a complex/composite outcome; that is, more than one outcome (e.g., computer-assisted acetabular component placement leads to both improved radiographic cup placement and improved functional outcome).  However, when formally testing statistical significance, the hypothesis should be stated as a “null” hypothesis.2 The purpose of hypothesis testing is to make an inference about the population of interest on the basis of a random sample taken from that population. The null hypothesis for the preceding research hypothesis then would be that there is no difference in mean functional outcome between the computer-assisted insertion and free-hand placement techniques. After forming the null hypothesis, the researchers would form an alternate hypothesis stating the nature of the difference, if it should appear. The alternate hypothesis would be that there is a difference in mean functional outcome between these techniques. At the end of the study, the null hypothesis is then tested statistically. If the findings of the study are not statistically significant (i.e., there is no difference in functional outcome between the groups in a statistical sense), we cannot reject the null hypothesis, whereas if the findings were significant, we can reject the null hypothesis and accept the alternate hypothesis (i.e., there is a difference in mean functional outcome between the study groups), errors in testing notwithstanding. In other words, hypothesis testing confirms or refutes the statement that the observed findings did not occur by chance alone but rather occurred because there was a true difference in outcomes between these surgical procedures. The concept of statistical hypothesis testing is complex, and the details are beyond the scope of this article.  Another important concept inherent in hypothesis testing is whether the hypotheses will be 1-sided or 2-sided. A 2-sided hypothesis states that there is a difference between the experimental group and the control group, but it does not specify in advance the expected direction of the difference. For example, we asked whether there is there an improvement in outcomes with computer-assisted surgery or whether the outcomes worse with computer-assisted surgery. We presented a 2-sided test in the above example because we did not specify the direction of the difference. A 1-sided hypothesis states a specific direction (e.g., there is an improvement in outcomes with computer-assisted surgery). A 2-sided hypothesis should be used unless there is a good justification for using a 1-sided hypothesis. As Bland and Atlman 8 stated, “One-sided hypothesis testing should never be used as a device to make a conventionally nonsignificant difference significant.”  The research hypothesis should be stated at the beginning of the study to guide the objectives for research. Whereas the investigators may state the hypothesis as being 1-sided (there is an improvement with treatment), the study and investigators must adhere to the concept of clinical equipoise. According to this principle, a clinical (or surgical) trial is ethical only if the expert community is uncertain about the relative therapeutic merits of the experimental and control groups being evaluated.9 It means there must exist an honest and professional disagreement among expert clinicians about the preferred treatment.9  Designing a research hypothesis is supported by a good research question and will influence the type of research design for the study. Acting on the principles of appropriate hypothesis development, the study can then confidently proceed to the development of the research objective. Research objective  The primary objective should be coupled with the hypothesis of the study. Study objectives define the specific aims of the study and should be clearly stated in the introduction of the research protocol.7 From our previous example and using the investigative hypothesis that there is a difference in functional outcomes between computer-assisted acetabular component placement and free-hand placement, the primary objective can be stated as follows: this study will compare the functional outcomes of computer-assisted acetabular component insertion versus free-hand placement in patients undergoing total hip arthroplasty. Note that the study objective is an active statement about how the study is going to answer the specific research question. Objectives can (and often do) state exactly which outcome measures are going to be used within their statements. They are important because they not only help guide the development of the protocol and design of study but also play a role in sample size calculations and determining the power of the study.7 These concepts will be discussed in other articles in this series.  From the surgeon’s point of view, it is important for the study objectives to be focused on outcomes that are important to patients and clinically relevant. For example, the most methodologically sound randomized controlled trial comparing 2 techniques of distal radial fixation would have little or no clinical impact if the primary objective was to determine the effect of treatment A as compared to treatment B on intraoperative fluoroscopy time. However, if the objective was to determine the effect of treatment A as compared to treatment B on patient functional outcome at 1 year, this would have a much more significant impact on clinical decision-making. Second, more meaningful surgeon–patient discussions could ensue, incorporating patient values and preferences with the results from this study.6,7 It is the precise objective and what the investigator is trying to measure that is of clinical relevance in the practical setting.  The following is an example from the literature about the relation between the research question, hypothesis and study objectives:  Study: Warden SJ, Metcalf BR, Kiss ZS, et al. Low-intensity pulsed ultrasound for chronic patellar tendinopathy: a randomized, double-blind, placebo-controlled trial. Rheumatology 2008;47:467–71.  Research question: How does low-intensity pulsed ultrasound (LIPUS) compare with a placebo device in managing the symptoms of skeletally mature patients with patellar tendinopathy?  Research hypothesis: Pain levels are reduced in patients who receive daily active-LIPUS (treatment) for 12 weeks compared with individuals who receive inactive-LIPUS (placebo).  Objective: To investigate the clinical efficacy of LIPUS in the management of patellar tendinopathy symptoms. Conclusion  The development of the research question is the most important aspect of a research project. A research project can fail if the objectives and hypothesis are poorly focused and underdeveloped. Useful tips for surgical researchers are provided in Box 3. Designing and developing an appropriate and relevant research question, hypothesis and objectives can be a difficult task. The critical appraisal of the research question used in a study is vital to the application of the findings to clinical practice. Focusing resources, time and dedication to these 3 very important tasks will help to guide a successful research project, influence interpretation of the results and affect future publication efforts.", "category": "Edison", "id": 120}
{"skillName": "DSDM02", "skillText": "database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model, which uses a table-based format.  Contents      1 Examples     2 Relationships and functions     3 Flat model     4 Early data models         4.1 Hierarchical model         4.2 Network model         4.3 Inverted file model     5 Relational model         5.1 Dimensional model     6 Post-relational database models         6.1 Graph model         6.2 Multivalue model         6.3 Object-oriented database models     7 References  Examples  Common logical data models for databases include:      Hierarchical database model     Network model     Relational model     Entity–relationship model         Enhanced entity–relationship model     Object model     Document model     Entity–attribute–value model     Star schema  An object-relational database combines the two related structures.  Physical data models include:      Inverted index     Flat file  Other models include:      Associative model     Multidimensional model     Multivalue model     Semantic model     XML database     Named graph     Triplestore  Relationships and functions  A given database management system may provide one or more models. The optimal structure depends on the natural organization of the application's data, and on the application's requirements, which include transaction rate (speed), reliability, maintainability, scalability, and cost. Most database management systems are built around one particular data model, although it is possible for products to offer support for more than one model.  Various physical data models can implement any given logical model. Most database software will offer the user some level of control in tuning the physical implementation, since the choices that are made have a significant effect on performance.  A model is not just a way of structuring data: it also defines a set of operations that can be performed on the data. The relational model, for example, defines operations such as select (project) and join. Although these operations may not be explicit in a particular query language, they provide the foundation on which a query language is built. Flat model Flat File Model Main articles: Flat file database and Spreadsheet  The flat (or table) model consists of a single, two-dimensional array of data elements, where all members of a given column are assumed to be similar values, and all members of a row are assumed to be related to one another. For instance, columns for name and password that might be used as a part of a system security database. Each row would have the specific password associated with an individual user. Columns of the table often have a type associated with them, defining them as character data, date or time information, integers, or floating point numbers. This tabular format is a precursor to the relational model. Early data models  These models were popular in the 1960s, 1970s, but nowadays can be found primarily in old legacy systems. They are characterized primarily by being navigational with strong connections between their logical and physical representations, and deficiencies in data independence. Hierarchical model Hierarchical Model Main article: Hierarchical model  In a hierarchical model, data is organized into a tree-like structure, implying a single parent for each record. A sort field keeps sibling records in a particular order. Hierarchical structures were widely used in the early mainframe database management systems, such as the Information Management System (IMS) by IBM, and now describe the structure of XML documents. This structure allows one one-to-many relationship between two types of data. This structure is very efficient to describe many relationships in the real world; recipes, table of contents, ordering of paragraphs/verses, any nested and sorted information.  This hierarchy is used as the physical order of records in storage. Record access is done by navigating downward through the data structure using pointers combined with sequential accessing. Because of this, the hierarchical structure is inefficient for certain database operations when a full path (as opposed to upward link and sort field) is not also included for each record. Such limitations have been compensated for in later IMS versions by additional logical hierarchies imposed on the base physical hierarchy. Network model Network Model Main article: Network model  The network model expands upon the hierarchical structure, allowing many-to-many relationships in a tree-like structure that allows multiple parents. It was most popular before being replaced by the relational model, and is defined by the CODASYL specification.  The network model organizes data using two fundamental concepts, called records and sets. Records contain fields (which may be organized hierarchically, as in the programming language COBOL). Sets (not to be confused with mathematical sets) define one-to-many relationships between records: one owner, many members. A record may be an owner in any number of sets, and a member in any number of sets.  A set consists of circular linked lists where one record type, the set owner or parent, appears once in each circle, and a second record type, the subordinate or child, may appear multiple times in each circle. In this way a hierarchy may be established between any two record types, e.g., type A is the owner of B. At the same time another set may be defined where B is the owner of A. Thus all the sets comprise a general directed graph (ownership defines a direction), or network construct. Access to records is either sequential (usually in each record type) or by navigation in the circular linked lists.  The network model is able to represent redundancy in data more efficiently than in the hierarchical model, and there can be more than one path from an ancestor node to a descendant. The operations of the network model are navigational in style: a program maintains a current position, and navigates from one record to another by following the relationships in which the record participates. Records can also be located by supplying key values.  Although it is not an essential feature of the model, network databases generally implement the set relationships by means of pointers that directly address the location of a record on disk. This gives excellent retrieval performance, at the expense of operations such as database loading and reorganization.  Popular DBMS products that utilized it were Cincom Systems' Total and Cullinet's IDMS. IDMS gained a considerable customer base; in the 1980s, it adopted the relational model and SQL in addition to its original tools and languages.  Most object databases (invented in the 1990s) use the navigational concept to provide fast navigation across networks of objects, generally using object identifiers as \"smart\" pointers to related objects. Objectivity/DB, for instance, implements named one-to-one, one-to-many, many-to-one, and many-to-many named relationships that can cross databases. Many object databases also support SQL, combining the strengths of both models. Inverted file model Main article: Inverted index  In an inverted file or inverted index, the contents of the data are used as keys in a lookup table, and the values in the table are pointers to the location of each instance of a given content item. This is also the logical structure of contemporary database indexes, which might only use the contents from a particular columns in the lookup table. The inverted file data model can put indexes in a second set of files next to existing flat database files, in order to efficiently directly access needed records in these files.  Notable for using this data model is the ADABAS DBMS of Software AG, introduced in 1970. ADABAS has gained considerable customer base and exists and supported until today. In the 1980s it has adopted the relational model and SQL in addition to its original tools and languages.  Document-oriented database Clusterpoint uses inverted indexing model to provide fast full-text search for XML or JSON data objects and to deliver scale out ability for Big data. Clusterpoint has built-in computing engine that allows execution of a combined SQL query, free text search and JavaScript code right inside the distributed database. Both data and inverted index through scalable sharding and replication can be distributed across a large number of servers to support billions of data objects in the same Clusterpoint database. Clusterpoint query language JS/SQL blends together SQL and JavaScript syntax with full text search, where inverted index is being used to deliver milliseconds-range text search performance and relevant pagination in web and mobile applications. In Clusterpoint database architecture inverted index also supports programmable relevance ranking enabling to customize search output without extra coding efforts. Similarly to relational databases, Clusterpoint supports distributed ACID-compliant database transactions for strong document database consistency, where inverted index data is immediately updated along any XML or JSON document content updates. Inverted index is also used to support near real-time Big data reporting, analytics, drill-down and data mining over REST API in Clusterpoint database. Relational model Two tables with a relationship Main article: Relational model  The relational model was introduced by E.F. Codd in 1970[1] as a way to make database management systems more independent of any particular application. It is a mathematical model defined in terms of predicate logic and set theory, and systems implementing it have been used by mainframe, midrange and microcomputer systems.  The products that are generally referred to as relational databases in fact implement a model that is only an approximation to the mathematical model defined by Codd. Three key terms are used extensively in relational database models: relations, attributes, and domains. A relation is a table with columns and rows. The named columns of the relation are called attributes, and the domain is the set of values the attributes are allowed to take.  The basic data structure of the relational model is the table, where information about a particular entity (say, an employee) is represented in rows (also called tuples) and columns. Thus, the \"relation\" in \"relational database\" refers to the various tables in the database; a relation is a set of tuples. The columns enumerate the various attributes of the entity (the employee's name, address or phone number, for example), and a row is an actual instance of the entity (a specific employee) that is represented by the relation. As a result, each tuple of the employee table represents various attributes of a single employee.  All relations (and, thus, tables) in a relational database have to adhere to some basic rules to qualify as relations. First, the ordering of columns is immaterial in a table. Second, there can't be identical tuples or rows in a table. And third, each tuple will contain a single value for each of its attributes.  A relational database contains multiple tables, each similar to the one in the \"flat\" database model. One of the strengths of the relational model is that, in principle, any value occurring in two different records (belonging to the same table or to different tables), implies a relationship among those two records. Yet, in order to enforce explicit integrity constraints, relationships between records in tables can also be defined explicitly, by identifying or non-identifying parent-child relationships characterized by assigning cardinality (1:1, (0)1:M, M:M). Tables can also have a designated single attribute or a set of attributes that can act as a \"key\", which can be used to uniquely identify each tuple in the table.  A key that can be used to uniquely identify a row in a table is called a primary key. Keys are commonly used to join or combine data from two or more tables. For example, an Employee table may contain a column named Location which contains a value that matches the key of a Location table. Keys are also critical in the creation of indexes, which facilitate fast retrieval of data from large tables. Any column can be a key, or multiple columns can be grouped together into a compound key. It is not necessary to define all the keys in advance; a column can be used as a key even if it was not originally intended to be one.  A key that has an external, real-world meaning (such as a person's name, a book's ISBN, or a car's serial number) is sometimes called a \"natural\" key. If no natural key is suitable (think of the many people named Brown), an arbitrary or surrogate key can be assigned (such as by giving employees ID numbers). In practice, most databases have both generated and natural keys, because generated keys can be used internally to create links between rows that cannot break, while natural keys can be used, less reliably, for searches and for integration with other databases. (For example, records in two independently developed databases could be matched up by social security number, except when the social security numbers are incorrect, missing, or have changed.)  The most common query language used with the relational model is the Structured Query Language (SQL). Dimensional model  The dimensional model is a specialized adaptation of the relational model used to represent data in data warehouses in a way that data can be easily summarized using online analytical processing, or OLAP queries. In the dimensional model, a database schema consists of a single large table of facts that are described using dimensions and measures. A dimension provides the context of a fact (such as who participated, when and where it happened, and its type) and is used in queries to group related facts together. Dimensions tend to be discrete and are often hierarchical; for example, the location might include the building, state, and country. A measure is a quantity describing the fact, such as revenue. It is important that measures can be meaningfully aggregated—for example, the revenue from different locations can be added together.  In an OLAP query, dimensions are chosen and the facts are grouped and aggregated together to create a summary.  The dimensional model is often implemented on top of the relational model using a star schema, consisting of one highly normalized table containing the facts, and surrounding denormalized tables containing each dimension. An alternative physical implementation, called a snowflake schema, normalizes multi-level hierarchies within a dimension into multiple tables.  A data warehouse can contain multiple dimensional schemas that share dimension tables, allowing them to be used together. Coming up with a standard set of dimensions is an important part of dimensional modeling.  Its high performance has made the dimensional model the most popular database structure for OLAP. Post-relational database models  Products offering a more general data model than the relational model are sometimes classified as post-relational.[2] Alternate terms include \"hybrid database\", \"Object-enhanced RDBMS\" and others. The data model in such products incorporates relations but is not constrained by E.F. Codd's Information Principle, which requires that      all information in the database must be cast explicitly in terms of values in relations and in no other way     — [3]  Some of these extensions to the relational model integrate concepts from technologies that pre-date the relational model. For example, they allow representation of a directed graph with trees on the nodes. The German company sones implements this concept in its GraphDB.  Some post-relational products extend relational systems with non-relational features. Others arrived in much the same place by adding relational features to pre-relational systems. Paradoxically, this allows products that are historically pre-relational, such as PICK and MUMPS, to make a plausible claim to be post-relational.  The resource space model (RSM) is a non-relational data model based on multi-dimensional classification.[4] Graph model Main article: Graph database  Graph databases allow even more general structure than a network database; any node may be connected to any other node. Multivalue model Main article: MultiValue  Multivalue databases are \"lumpy\" data, in that they can store exactly the same way as relational databases, but they also permit a level of depth which the relational model can only approximate using sub-tables. This is nearly identical to the way XML expresses data, where a given field/attribute can have multiple right answers at the same time. Multivalue can be thought of as a compressed form of XML.  An example is an invoice, which in either multivalue or relational data could be seen as (A) Invoice Header Table - one entry per invoice, and (B) Invoice Detail Table - one entry per line item. In the multivalue model, we have the option of storing the data as on table, with an embedded table to represent the detail: (A) Invoice Table - one entry per invoice, no other tables needed.  The advantage is that the atomicity of the Invoice (conceptual) and the Invoice (data representation) are one-to-one. This also results in fewer reads, less referential integrity issues, and a dramatic decrease in the hardware needed to support a given transaction volume. Object-oriented database models Object-Oriented Model Main articles: Object-relational model and Object model  In the 1990s, the object-oriented programming paradigm was applied to database technology, creating a new database model known as object databases. This aims to avoid the object-relational impedance mismatch - the overhead of converting information between its representation in the database (for example as rows in tables) and its representation in the application program (typically as objects). Even further, the type system used in a particular application can be defined directly in the database, allowing the database to enforce the same data integrity invariants. Object databases also introduce the key ideas of object programming, such as encapsulation and polymorphism, into the world of databases.  A variety of these ways have been tried[by whom?]for storing objects in a database. Some[which?] products have approached the problem from the application programming end, by making the objects manipulated by the program persistent. This typically requires the addition of some kind of query language, since conventional programming languages do not have the ability to find objects based on their information content. Others[which?] have attacked the problem from the database end, by defining an object-oriented data model for the database, and defining a database programming language that allows full programming capabilities as well as traditional query facilities.  Object databases suffered because of a lack of standardization: although standards were defined by ODMG, they were never implemented well enough to ensure interoperability between products. Nevertheless, object databases have been used successfully in many applications: usually specialized applications such as engineering databases or molecular biology databases rather than mainstream commercial data processing. However, object database ideas were picked up by the relational vendors and influenced extensions made to these products and indeed to the SQL language.  An alternative to translating between objects and relational databases is to use an object-relational mapping (ORM) library. Data models define how the logical structure of a database is modeled. Data Models are fundamental entities to introduce abstraction in a DBMS. Data models define how data is connected to each other and how they are processed and stored inside the system.  The very first data model could be flat data-models, where all the data used are to be kept in the same plane. Earlier data models were not so scientific, hence they were prone to introduce lots of duplication and update anomalies. Entity-Relationship Model  Entity-Relationship (ER) Model is based on the notion of real-world entities and relationships among them. While formulating real-world scenario into the database model, the ER Model creates entity set, relationship set, general attributes and constraints.  ER Model is best used for the conceptual design of a database.  ER Model is based on −      Entities and their attributes.      Relationships among entities.  These concepts are explained below.      Entity − An entity in an ER Model is a real-world entity having properties called attributes. Every attribute is defined by its set of values called domain. For example, in a school database, a student is considered as an entity. Student has various attributes like name, age, class, etc.      Relationship − The logical association among entities is called relationship. Relationships are mapped with entities in various ways. Mapping cardinalities define the number of association between two entities.      Mapping cardinalities −         one to one         one to many         many to one         many to many  Relational Model  The most popular data model in DBMS is the Relational Model. It is more scientific a model than others. This model is based on first-order predicate logic and defines a table as an n-ary relation. Relational Model Table  The main highlights of this model are −      Data is stored in tables called relations.     Relations can be normalized.     In normalized relations, values saved are atomic values.     Each row in a relation contains a unique value.     Each column in a relation contains values from a same domain.  A Data Model is an abstract model that organizes elements of data and standardizes how they relate to one another and to properties of the real world. For instance, a data model may specify that a data element representing a car comprise a number of other elements which in turn represent the color, size and owner of the car. Overview of data modeling context: Data model is based on Data, Data relationship, Data semantic and Data constraint. A data model provides the details of information to be stored, and is of primary use when the final product is the generation of computer software code for an application or the preparation of a functional specification to aid a computer software make-or-buy decision. The figure is an example of the interaction between process and data models.[1]  A data model explicitly determines the structure of data. Data models are specified in a data modeling notation, which is often graphical in form.[2]  A data model can be sometimes referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models.  Contents      1 Overview         1.1 The role of data models         1.2 Three perspectives     2 History     3 Types of data models         3.1 Database model         3.2 Data Structure Diagram         3.3 Entity-relationship model         3.4 Geographic data model         3.5 Generic data model         3.6 Semantic data model     4 Data model topics         4.1 Data architecture         4.2 Data modeling         4.3 Data properties         4.4 Data organization         4.5 Data structure         4.6 Data model theory         4.7 Patterns     5 Related models         5.1 Data flow diagram         5.2 Information model         5.3 Object model         5.4 Object-Role Model         5.5 Unified Modeling Language models     6 See also     7 References     8 Further reading  Overview  Managing large quantities of structured and unstructured data is a primary function of information systems. Data models describe the structure, manipulation and integrity aspects of the data stored in data management systems such as relational databases. They typically do not describe unstructured data, such as word processing documents, email messages, pictures, digital audio, and video. The role of data models How data models deliver benefit.[3]  The main aim of data models is to support the development of information systems by providing the definition and format of data. According to West and Fowler (1999) \"if this is done consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data. The results of this are indicated above. However, systems and interfaces often cost more than they should, to build, operate, and maintain. They may also constrain the business rather than support it. A major cause is that the quality of the data models implemented in systems and interfaces is poor\".[3]      \"Business rules, specific to how things are done in a particular place, are often fixed in the structure of a data model. This means that small changes in the way business is conducted lead to large changes in computer systems and interfaces\".[3]     \"Entity types are often not identified, or incorrectly identified. This can lead to replication of data, data structure, and functionality, together with the attendant costs of that duplication in development and maintenance\".[3]     \"Data models for different systems are arbitrarily different. The result of this is that complex interfaces are required between systems that share data. These interfaces can account for between 25-70% of the cost of current systems\".[3]     \"Data cannot be shared electronically with customers and suppliers, because the structure and meaning of data has not been standardized. For example, engineering design data and drawings for process plant are still sometimes exchanged on paper\".[3]  The reason for these problems is a lack of standards that will ensure that data models will both meet business needs and be consistent.[3]  A data model explicitly determines the structure of data or structured data. Typical applications of data models include database models, design of information systems, and enabling exchange of data. Usually data models are specified in a data modeling language.[3]  A data model can be sometimes referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models. Three perspectives The ANSI/SPARC three level architecture. This shows that a data model can be an external model (or view), a conceptual model, or a physical model. This is not the only way to look at data models, but it is a useful way, particularly when comparing models.[3]  A data model instance may be one of three kinds according to ANSI in 1975:[4]      Conceptual data model : describes the semantics of a domain, being the scope of the model. For example, it may be a model of the interest area of an organization or industry. This consists of entity classes, representing kinds of things of significance in the domain, and relationship assertions about associations between pairs of entity classes. A conceptual schema specifies the kinds of facts or propositions that can be expressed using the model. In that sense, it defines the allowed expressions in an artificial 'language' with a scope that is limited by the scope of the model.     Logical data model : describes the semantics, as represented by a particular data manipulation technology. This consists of descriptions of tables and columns, object oriented classes, and XML tags, among other things.     Physical data model : describes the physical means by which data are stored. This is concerned with partitions, CPUs, tablespaces, and the like.  The significance of this approach, according to ANSI, is that it allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual model. The table/column structure can change without (necessarily) affecting the conceptual model. In each case, of course, the structures must remain consistent with the other model. The table/column structure may be different from a direct translation of the entity classes and attributes, but it must ultimately carry out the objectives of the conceptual entity class structure. Early phases of many software development projects emphasize the design of a conceptual data model. Such a design can be detailed into a logical data model. In later stages, this model may be translated into physical data model. However, it is also possible to implement a conceptual model directly. History  One of the earliest pioneering works in modelling information systems was done by Young and Kent (1958),[5][6] who argued for \"a precise and abstract way of specifying the informational and time characteristics of a data processing problem\". They wanted to create \"a notation that should enable the analyst to organize the problem around any piece of hardware\". Their work was a first effort to create an abstract specification and invariant basis for designing different alternative implementations using different hardware components. A next step in IS modelling was taken by CODASYL, an IT industry consortium formed in 1959, who essentially aimed at the same thing as Young and Kent: the development of \"a proper structure for machine independent problem definition language, at the system level of data processing\". This led to the development of a specific IS information algebra.[6]  In the 1960s data modeling gained more significance with the initiation of the management information system (MIS) concept. According to Leondes (2002), \"during that time, the information system provided the data and information for management purposes. The first generation database system, called Integrated Data Store (IDS), was designed by Charles Bachman at General Electric. Two famous database models, the network data model and the hierarchical data model, were proposed during this period of time\".[7] Towards the end of the 1960s Edgar F. Codd worked out his theories of data arrangement, and proposed the relational model for database management based on first-order predicate logic.[8]  In the 1970s entity relationship modeling emerged as a new type of conceptual data modeling, originally proposed in 1976 by Peter Chen. Entity relationship models were being used in the first stage of information system design during the requirements analysis to describe information needs or the type of information that is to be stored in a database. This technique can describe any ontology, i.e., an overview and classification of concepts and their relationships, for a certain area of interest.  In the 1970s G.M. Nijssen developed \"Natural Language Information Analysis Method\" (NIAM) method, and developed this in the 1980s in cooperation with Terry Halpin into Object-Role Modeling (ORM).  Bill Kent, in his 1978 book Data and Reality[9] compared a data model to a map of a territory, emphasizing that in the real world, \"highways are not painted red, rivers don't have county lines running down the middle, and you can't see contour lines on a mountain\". In contrast to other researchers who tried to create models that were mathematically clean and elegant, Kent emphasized the essential messiness of the real world, and the task of the data modeller to create order out of chaos without excessively distorting the truth.  In the 1980s according to Jan L. Harrington (2000) \"the development of the object-oriented paradigm brought about a fundamental change in the way we look at data and the procedures that operate on data. Traditionally, data and procedures have been stored separately: the data and their relationship in a database, the procedures in an application program. Object orientation, however, combined an entity's procedure with its data.\"[10] Types of data models Database model Main article: Database model  A database model is a specification describing how a database is structured and used.  Several such models have been suggested. Common models include:  Flat model     This may not strictly qualify as a data model. The flat (or table) model consists of a single, two-dimensional array of data elements, where all members of a given column are assumed to be similar values, and all members of a row are assumed to be related to one another. Hierarchical model     In this model data is organized into a tree-like structure, implying a single upward link in each record to describe the nesting, and a sort field to keep the records in a particular order in each same-level list. Network model     This model organizes data using two fundamental constructs, called records and sets. Records contain fields, and sets define one-to-many relationships between records: one owner, many members. Relational model     is a database model based on first-order predicate logic. Its core idea is to describe a database as a collection of predicates over a finite set of predicate variables, describing constraints on the possible values and combinations of values. Object-relational model     Similar to a relational database model, but objects, classes and inheritance are directly supported in database schemas and in the query language. Star schema     The simplest style of data warehouse schema. The star schema consists of a few \"fact tables\" (possibly only one, justifying the name) referencing any number of \"dimension tables\". The star schema is considered an important special case of the snowflake schema.      Flat model      Hierarchical model      Network model      Relational model      Concept-oriented model      Star schema  Data Structure Diagram Main article: Data structure diagram Example of a Data Structure Diagram.  A data structure diagram (DSD) is a diagram and data model used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them. The basic graphic elements of DSDs are boxes, representing entities, and arrows, representing relationships. Data structure diagrams are most useful for documenting complex data entities.  Data structure diagrams are an extension of the entity-relationship model (ER model). In DSDs, attributes are specified inside the entity boxes rather than outside of them, while relationships are drawn as boxes composed of attributes which specify the constraints that bind entities together. The E-R model, while robust, doesn't provide a way to specify the constraints between relationships, and becomes visually cumbersome when representing entities with several attributes. DSDs differ from the ER model in that the ER model focuses on the relationships between different entities, whereas DSDs focus on the relationships of the elements within an entity and enable users to fully see the links and relationships between each entity.  There are several styles for representing data structure diagrams, with the notable difference in the manner of defining cardinality. The choices are between arrow heads, inverted arrow heads (crow's feet), or numerical representation of the cardinality. Example of an IDEF1X Entity relationship diagrams used to model IDEF1X itself.[11] Entity-relationship model Main article: Entity-relationship model  An entity-relationship model (ERM), sometimes referred to as an entity-relationship diagram (ERD), is an abstract conceptual data model (or semantic data model) used in software engineering to represent structured data. There are several notations used for ERMs. Geographic data model Main article: Data model (GIS)  A data model in Geographic information systems is a mathematical construct for representing geographic objects or surfaces as data. For example,      the vector data model represents geography as collections of points, lines, and polygons;     the raster data model represent geography as cell matrixes that store numeric values;     and the Triangulated irregular network (TIN) data model represents geography as sets of contiguous, nonoverlapping triangles.[12]      Groups relate to process of making a map[13]      NGMDB data model applications[13]      NGMDB databases linked together[13]      Representing 3D map information[13]  Generic data model Main article: Generic data model  Generic data models are generalizations of conventional data models. They define standardised general relation types, together with the kinds of things that may be related by such a relation type. Generic data models are developed as an approach to solve some shortcomings of conventional data models. For example, different modelers usually produce different conventional data models of the same domain. This can lead to difficulty in bringing the models of different people together and is an obstacle for data exchange and data integration. Invariably, however, this difference is attributable to different levels of abstraction in the models and differences in the kinds of facts that can be instantiated (the semantic expression capabilities of the models). The modelers need to communicate and agree on certain elements which are to be rendered more concretely, in order to make the differences less significant. Semantic data model Main article: Semantic data model Semantic data models.[11]  A semantic data model in software engineering is a technique to define the meaning of data within the context of its interrelationships with other data. A semantic data model is an abstraction which defines how the stored symbols relate to the real world.[11] A semantic data model is sometimes called a conceptual data model.  The logical data structure of a database management system (DBMS), whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. Therefore, the need to define data from a conceptual view has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure. The real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.[11] Data model topics Data architecture Main article: Data architecture  Data architecture is the design of data for use in defining the target state and the subsequent planning needed to hit the target state. It is usually one of several architecture domains that form the pillars of an enterprise architecture or solution architecture.  A data architecture describes the data structures used by a business and/or its applications. There are descriptions of data in storage and data in motion; descriptions of data stores, data groups and data items; and mappings of those data artifacts to data qualities, applications, locations etc.  Essential to realizing the target state, Data architecture describes how data is processed, stored, and utilized in a given system. It provides criteria for data processing operations that make it possible to design data flows and also control the flow of data in the system. Data modeling Main article: Data modeling The data modeling process.  Data modeling in software engineering is the process of creating a data model by applying formal data model descriptions using data modeling techniques. Data modeling is a technique for defining business requirements for a database. It is sometimes called database modeling because a data model is eventually implemented in a database.[14]  The figure illustrates the way data models are developed and used today. A conceptual data model is developed based on the data requirements for the application that is being developed, perhaps in the context of an activity model. The data model will normally consist of entity types, attributes, relationships, integrity rules, and the definitions of those objects. This is then used as the start point for interface or database design.[3] Data properties  Some important properties of data for which requirements need to be met are:      definition-related properties[3]         relevance: the usefulness of the data in the context of your business.         clarity: the availability of a clear and shared definition for the data.         consistency: the compatibility of the same type of data from different sources.  Some important properties of data.[3]      content-related properties         timeliness: the availability of data at the time required and how up to date that data is.         accuracy: how close to the truth the data is.     properties related to both definition and content         completeness: how much of the required data is available.         accessibility: where, how, and to whom the data is available or not available (e.g. security).         cost: the cost incurred in obtaining the data, and making it available for use.  Data organization  Another kind of data model describes how to organize data using a database management system or other data management technology. It describes, for example, relational tables and columns or object-oriented classes and attributes. Such a data model is sometimes referred to as the physical data model, but in the original ANSI three schema architecture, it is called \"logical\". In that architecture, the physical model describes the storage media (cylinders, tracks, and tablespaces). Ideally, this model is derived from the more conceptual data model described above. It may differ, however, to account for constraints like processing capacity and usage patterns.  While data analysis is a common term for data modeling, the activity actually has more in common with the ideas and methods of synthesis (inferring general concepts from particular instances) than it does with analysis (identifying component concepts from more general ones). {Presumably we call ourselves systems analysts because no one can say systems synthesists.} Data modeling strives to bring the data structures of interest together into a cohesive, inseparable, whole by eliminating unnecessary data redundancies and by relating data structures with relationships.  A different approach is to use adaptive systems such as artificial neural networks that can autonomously create implicit models of data. Data structure Main article: Data structure A binary tree, a simple type of branching linked data structure.  A data structure is a way of storing data in a computer so that it can be used efficiently. It is an organization of mathematical and logical concepts of data. Often a carefully chosen data structure will allow the most efficient algorithm to be used. The choice of the data structure often begins from the choice of an abstract data type.  A data model describes the structure of the data within a given domain and, by implication, the underlying structure of that domain itself. This means that a data model in fact specifies a dedicated grammar for a dedicated artificial language for that domain. A data model represents classes of entities (kinds of things) about which a company wishes to hold information, the attributes of that information, and relationships among those entities and (often implicit) relationships among those attributes. The model describes the organization of the data to some extent irrespective of how data might be represented in a computer system.  The entities represented by a data model can be the tangible entities, but models that include such concrete entity classes tend to change over time. Robust data models often identify abstractions of such entities. For example, a data model might include an entity class called \"Person\", representing all the people who interact with an organization. Such an abstract entity class is typically more appropriate than ones called \"Vendor\" or \"Employee\", which identify specific roles played by those people.      Array      Hash table      Linked list      Stack (data structure)  Data model theory  The term data model can have two meanings:[15]      A data model theory, i.e. a formal description of how data may be structured and accessed.     A data model instance, i.e. applying a data model theory to create a practical data model instance for some particular application.  A data model theory has three main components:[15]      The structural part: a collection of data structures which are used to create databases representing the entities or objects modeled by the database.     The integrity part: a collection of rules governing the constraints placed on these data structures to ensure structural integrity.     The manipulation part: a collection of operators which can be applied to the data structures, to update and query the data contained in the database.  For example, in the relational model, the structural part is based on a modified concept of the mathematical relation; the integrity part is expressed in first-order logic and the manipulation part is expressed using the relational algebra, tuple calculus and domain calculus.  A data model instance is created by applying a data model theory. This is typically done to solve some business enterprise requirement. Business requirements are normally captured by a semantic logical data model. This is transformed into a physical data model instance from which is generated a physical database. For example, a data modeler may use a data modeling tool to create an entity-relationship model of the corporate data repository of some business enterprise. This model is transformed into a relational model, which in turn generates a relational database. Patterns  Patterns[16] are common data modeling structures that occur in many data models. Related models Data flow diagram Main article: Data flow diagram Data Flow Diagram example.[17]  A data flow diagram (DFD) is a graphical representation of the \"flow\" of data through an information system. It differs from the flowchart as it shows the data flow instead of the control flow of the program. A data flow diagram can also be used for the visualization of data processing (structured design). Data flow diagrams were invented by Larry Constantine, the original developer of structured design,[18] based on Martin and Estrin's \"data flow graph\" model of computation.  It is common practice to draw a context-level Data flow diagram first which shows the interaction between the system and outside entities. The DFD is designed to show how a system is divided into smaller portions and to highlight the flow of data between those parts. This context-level Data flow diagram is then \"exploded\" to show more detail of the system being modeled Information model Main article: Information model Example of an EXPRESS G Information model.  An Information model is not a type of data model, but more or less an alternative model. Within the field of software engineering both a data model and an information model can be abstract, formal representations of entity types that includes their properties, relationships and the operations that can be performed on them. The entity types in the model may be kinds of real-world objects, such as devices in a network, or they may themselves be abstract, such as for the entities used in a billing system. Typically, they are used to model a constrained domain that can be described by a closed set of entity types, properties, relationships and operations.  According to Lee (1999)[19] an information model is a representation of concepts, relationships, constraints, rules, and operations to specify data semantics for a chosen domain of discourse. It can provide sharable, stable, and organized structure of information requirements for the domain context.[19] More in general the term information model is used for models of individual things, such as facilities, buildings, process plants, etc. In those cases the concept is specialised to Facility Information Model, Building Information Model, Plant Information Model, etc. Such an information model is an integration of a model of the facility with the data and documents about the facility.  An information model provides formalism to the description of a problem domain without constraining how that description is mapped to an actual implementation in software. There may be many mappings of the information model. Such mappings are called data models, irrespective of whether they are object models (e.g. using UML), entity relationship models or XML schemas. Document Object Model, a standard object model for representing HTML or XML. Object model Main article: Object model  An object model in computer science is a collection of objects or classes through which a program can examine and manipulate some specific parts of its world. In other words, the object-oriented interface to some service or system. Such an interface is said to be the object model of the represented service or system. For example, the Document Object Model (DOM) [1]  is a collection of objects that represent a page in a web browser, used by script programs to examine and dynamically change the page. There is a Microsoft Excel object model[20] for controlling Microsoft Excel from another program, and the ASCOM Telescope Driver[21] is an object model for controlling an astronomical telescope.  In computing the term object model has a distinct second meaning of the general properties of objects in a specific computer programming language, technology, notation or methodology that uses them. For example, the Java object model, the COM object model, or the object model of OMT. Such object models are usually defined using concepts such as class, message, inheritance, polymorphism, and encapsulation. There is an extensive literature on formalized object models as a subset of the formal semantics of programming languages. Object-Role Model Main article: Object-Role Modeling Example of the application of Object-Role Modeling in a \"Schema for Geologic Surface\", Stephen M. Richard (1999).[22]  Object-Role Modeling (ORM) is a method for conceptual modeling, and can be used as a tool for information and rules analysis.[23]  Object-Role Modeling is a fact-oriented method for performing systems analysis at the conceptual level. The quality of a database application depends critically on its design. To help ensure correctness, clarity, adaptability and productivity, information systems are best specified first at the conceptual level, using concepts and language that people can readily understand.  The conceptual design may include data, process and behavioral perspectives, and the actual DBMS used to implement the design might be based on one of many logical data models (relational, hierarchic, network, object-oriented etc.).[24] Unified Modeling Language models Main article: Unified Modeling Language  The Unified Modeling Language (UML) is a standardized general-purpose modeling language in the field of software engineering. It is a graphical language for visualizing, specifying, constructing, and documenting the artifacts of a software-intensive system. The Unified Modeling Language offers a standard way to write a system's blueprints, including:[25]      Conceptual things such as business processes and system functions     Concrete things such as programming language statements, database schemas, and     Reusable software components.  UML offers a mix of functional models, data models, and database models. See also      Business process model     Core Architecture Data Model     Data dictionary     JC3IEDM     Process model     Data Format Description Language (DFDL)     Structured Search     Key-objects  The goals of this article are to overview fundamental data modeling skills that all developers should have, skills that can be applied on both traditional projects that take a serial approach to agile projects that take an evolutionary approach. My personal philosophy is that every IT professional should have a basic understanding of data modeling. They don’t need to be experts at data modeling, but they should be prepared to be involved in the creation of such a model, be able to read an existing data model, understand when and when not to create a data model, and appreciate fundamental data design techniques. This article is a brief introduction to these skills.  The primary audience for this article is application developers who need to gain an understanding of some of the critical activities performed by an Agile DBA. This understanding should lead to an appreciation of what Agile DBAs do and why they do them, and it should help to bridge the communication gap between these two roles. Table of Contents      What is data modeling?         How are data models used in practice?           What about conceptual models?           Common data modeling notations     How to model data           Identify entity types         Identify attributes         Apply naming conventions         Identify relationships         Apply data model patterns         Assign keys           Normalize to reduce data redundancy         Denormalize to improve performance     Evolutionary/agile data modeling     How to become better at modeling data    1. What is Data Modeling?  Data modeling is the act of exploring data-oriented structures. Like other modeling artifacts data models can be used for a variety of purposes, from high-level conceptual models to physical data models.  From the point of view of an object-oriented developer data modeling is conceptually similar to class modeling. With data modeling you identify entity types whereas with class modeling you identify classes. Data attributes are assigned to entity types just as you would assign attributes and operations to classes.  There are associations between entities, similar to the associations between classes – relationships, inheritance, composition, and aggregation are all applicable concepts in data modeling.  Traditional data modeling is different from class modeling because it focuses solely on data – class models allow you to explore both the behavior and data aspects of your domain, with a data model you can only explore data issues.  Because of this focus data modelers have a tendency to be much better at getting the data “right\" than object modelers. However, some people will model database methods (stored procedures, stored functions, and triggers) when they are physical data modeling. It depends on the situation of course, but I personally think that this is a good idea and promote the concept in my UML data modeling profile (more on this later).  Although the focus of this article is data modeling, there are often alternatives to data-oriented artifacts (never forget Agile Modeling’s Multiple Models principle). For example, when it comes to conceptual modeling ORM diagrams aren’t your only option – In addition to LDMs it is quite common for people to create UML class diagrams and even Class Responsibility Collaborator (CRC) cards instead.  In fact, my experience is that CRC cards are superior to ORM diagrams because it is very easy to get project stakeholders actively involved in the creation of the model. Instead of a traditional, analyst-led drawing session you can instead facilitate stakeholders through the creation of CRC cards.  1.1 How are Data Models Used in Practice? Although methodology issues are covered later, we need to discuss how data models can be used in practice to better understand them.  You are likely to see three basic styles of data model:      Conceptual data models. These models, sometimes called domain models, are typically used to explore domain concepts with project stakeholders.  On Agile teams high-level conceptual models are often created as part of your initial requirements envisioning efforts as they are used to explore the high-level static business structures and concepts. On traditional teams conceptual data models are often created as the precursor to LDMs or as alternatives to LDMs.      Logical data models (LDMs).  LDMs are used to explore the domain concepts, and their relationships, of your problem domain.  This could be done for the scope of a single project or for your entire enterprise. LDMs depict the logical entity types, typically referred to simply as entity types, the data attributes describing those entities, and the relationships between the entities. LDMs are rarely used on Agile projects although often are on traditional projects (where they rarely seem to add much value in practice).     Physical data models (PDMs). PDMs are used to design the internal schema of a database, depicting the data tables, the data columns of those tables, and the relationships between the tables. PDMs often prove to be useful on both Agile and traditional projects and as a result the focus of this article is on physical modeling.  \t  Data and Databases Although LDMs and PDMs sound very similar, and they in fact are, the level of detail that they model can be significantly different. This is because the goals for each diagram is different – you can use an LDM to explore domain concepts with your stakeholders and the PDM to define your database design. Figure 1 presents a simple LDM and Figure 2 a simple PDM, both modeling the concept of customers and addresses as well as the relationship between them.  Both diagrams apply the Barker notation, summarized below.  Notice how the PDM shows greater detail, including an associative table required to implement the association as well as the keys needed to maintain the relationships. More on these concepts later. PDMs should also reflect your organization’s database naming standards, in this case an abbreviation of the entity name is appended to each column name and an abbreviation for “Number\" was consistently introduced.  A PDM should also indicate the data types for the columns, such as integer and char(5). Although Figure 2 does not show them, lookup tables (also called reference tables or description tables) for how the address is used as well as for states and countries are implied by the attributes ADDR_USAGE_CODE, STATE_CODE, and COUNTRY_CODE.     Figure 1. A simple logical data model.  Simple LDM     Figure 2. A simple physical data model.  Simple PDM  An important observation about Figures 1 and 2 is that I’m not slavishly following Barker’s approach to naming relationships.  For example, between Customer and Address there really should be two names “Each CUSTOMER may be located in one or more ADDRESSES\" and “Each ADDRESS may be the site of one or more CUSTOMERS\".  Although these names explicitly define the relationship I personally think that they’re visual noise that clutter the diagram.  I prefer simple names such as “has\" and then trust my readers to interpret the name in each direction. I’ll only add more information where it’s needed, in this case I think that it isn’t. However, a significant advantage of describing the names the way that Barker suggests is that it’s a good test to see if you actually understand the relationship – if you can’t name it then you likely don’t understand it.  Data models can be used effectively at both the enterprise level and on projects. Enterprise architects will often create one or more high-level LDMs that depict the data structures that support your enterprise, models typically referred to as enterprise data models or enterprise information models.  An enterprise data model is one of several views that your organization’s enterprise architects may choose to maintain and support – other views may explore your network/hardware infrastructure, your organization structure, your software infrastructure, and your business processes (to name a few).  Enterprise data models provide information that a project team can use both as a set of constraints as well as important insights into the structure of their system.   Project teams will typically create LDMs as a primary analysis artifact when their implementation environment is predominantly procedural in nature, for example they are using structured COBOL as an implementation language. LDMs are also a good choice when a project is data-oriented in nature, perhaps a data warehouse or reporting system is being developed (having said that, experience seems to show that usage-centered approaches appear to work even better).  However LDMs are often a poor choice when a project team is using object-oriented or component-based technologies because the developers would rather work with UML diagrams or when the project is not data-oriented in nature. As Agile Modeling advises, apply the right artifact(s) for the job. Or, as your grandfather likely advised you, use the right tool for the job. It's important to note that traditional approaches to Master Data Management (MDM) will often motivate the creation and maintenance of detailed LDMs, an effort that is rarely justifiable in practice when you consider the total cost of ownership (TCO) when calculating the return on investment (ROI) of those sorts of efforts.  When a relational database is used for data storage project teams are best advised to create a PDMs to model its internal schema.  My experience is that a PDM is often one of the critical design artifacts for business application development projects.      2.2. What About Conceptual Models?  Halpin (2001) points out that many data professionals prefer to create an Object-Role Model (ORM), an example is depicted in Figure 3, instead of an LDM for a conceptual model.  The advantage is that the notation is very simple, something your project stakeholders can quickly grasp, although the disadvantage is that the models become large very quickly. ORMs enable you to first explore actual data examples instead of simply jumping to a potentially incorrect abstraction – for example Figure 3 examines the relationship between customers and addresses in detail.   For more information about ORM, visit www.orm.net.     Figure 3. A simple Object-Role Model.  My experience is that people will capture information in the best place that they know. As a result I typically discard ORMs after I’m finished with them.  I sometimes user ORMs to explore the domain with project stakeholders but later replace them with a more traditional artifact such as an LDM, a class diagram, or even a PDM. As a generalizing specialist, someone with one or more specialties who also strives to gain general skills and knowledge, this is an easy decision for me to make; I know that this information that I’ve just “discarded\" will be captured in another artifact – a model, the tests, or even the code – that I understand.  A specialist who only understands a limited number of artifacts and therefore “hands-off\" their work to other specialists doesn’t have this as an option. Not only are they tempted to keep the artifacts that they create but also to invest even more time to enhance the artifacts. Generalizing specialists are more likely than specialists to travel light. \tThe Object Primer 3rd Edition: Agile Model Driven Development (AMDD) with UML 2  2.3. Common Data Modeling Notations  Figure 4 presents a summary of the syntax of four common data modeling notations: Information Engineering (IE), Barker, IDEF1X, and the Unified Modeling Language (UML).  This diagram isn’t meant to be comprehensive, instead its goal is to provide a basic overview.  Furthermore, for the sake of brevity I wasn’t able to depict the highly-detailed approach to relationship naming that Barker suggests. Although I provide a brief description of each notation in Table 1 I highly suggest David Hay’s paper A Comparison of Data Modeling Techniques as he goes into greater detail than I do.     Figure 4. Comparing the syntax of common data modeling notations.  Data modeling notation summary     Table 1. Discussing common data modeling notations.  Notation \t  Comments  IE \t  The IE notation (Finkelstein 1989) is simple and easy to read, and is well suited for high-level logical and enterprise data modeling. The only drawback of this notation, arguably an advantage, is that it does not support the identification of attributes of an entity. The assumption is that the attributes will be modeled with another diagram or simply described in the supporting documentation.  Barker \t  The Barker notation is one of the more popular ones, it is supported by Oracle’s toolset, and is well suited for all types of data models. It’s approach to subtyping can become clunky with hierarchies that go several levels deep.  IDEF1X \t  This notation is overly complex.  It was originally intended for physical modeling but has been misapplied for logical modeling as well. Although popular within some U.S. government agencies, particularly the Department of Defense (DoD), this notation has been all but abandoned by everyone else. Avoid it if you can.  UML \t  This is not an official data modeling notation (yet).  Although several suggestions for a data modeling profile for the UML exist, none are complete and more importantly are not “official\" UML yet. However, the Object Management Group (OMG) in December 2005 announced an RFP for data-oriented models.    3. How to Model Data It is critical for an application developer to have a grasp of the fundamentals of data modeling so they can not only read data models but also work effectively with Agile DBAs who are responsible for the data-oriented aspects of your project. Your goal reading this section is not to learn how to become a data modeler, instead it is simply to gain an appreciation of what is involved.  The following tasks are performed in an iterative manner:      Identify entity types     Identify attributes     Apply naming conventions     Identify relationships     Apply data model patterns     Assign keys     Normalize to reduce data redundancy     Denormalize to improve performance    \t  The Data Modeling Handbook   \t  Very good practical books about data modeling include Joe Celko’s Data & Databases and Data Modeling for Information Professionals as they both focus on practical issues with data modeling.  The Data Modeling Handbook and Data Model Patterns are both excellent resources once you’ve mastered the fundamentals.  An Introduction to Database Systems is a good academic treatise for anyone wishing to become a data specialist.    3.1 Identify Entity Types An entity type, also simply called entity (not exactly accurate terminology, but very common in practice), is similar conceptually to object-orientation’s concept of a class – an entity type represents a collection of similar objects.  An entity type could represent a collection of people, places, things, events, or concepts. Examples of entities in an order entry system would include Customer, Address, Order, Item, and Tax. If you were class modeling you would expect to discover classes with the exact same names. However, the difference between a class and an entity type is that classes have both data and behavior whereas entity types just have data.   Ideally an entity should be normal, the data modeling world’s version of cohesive. A normal entity depicts one concept, just like a cohesive class models one concept. For example, customer and order are clearly two different concepts; therefore it makes sense to model them as separate entities.      3.2 Identify Attributes Each entity type will have one or more data attributes.  For example, in Figure 1 you saw that the Customer entity has attributes such as First Name and Surname and in Figure 2 that the TCUSTOMER table had corresponding data columns CUST_FIRST_NAME and CUST_SURNAME (a column is the implementation of a data attribute within a relational database).   Attributes should also be cohesive from the point of view of your domain, something that is often a judgment call. – in Figure 1 we decided that we wanted to model the fact that people had both first and last names instead of just a name (e.g. “Scott\" and “Ambler\" vs. “Scott Ambler\") whereas we did not distinguish between the sections of an American zip code (e.g. 90210-1234-5678). Getting the level of detail right can have a significant impact on your development and maintenance efforts. Refactoring a single data column into several columns can be difficult, database refactoring is described in detail in Database Refactoring, although over-specifying an attribute (e.g. having three attributes for zip code when you only needed one) can result in overbuilding your system and hence you incur greater development and maintenance costs than you actually needed.     3.3 Apply Data Naming Conventions Your organization should have standards and guidelines applicable to data modeling, something you should be able to obtain from your enterprise administrators (if they don’t exist you should lobby to have some put in place). These guidelines should include naming conventions for both logical and physical modeling, the logical naming conventions should be focused on human readability whereas the physical naming conventions will reflect technical considerations.  You can clearly see that different naming conventions were applied in Figures 1 and 2.   As you saw in Introduction to Agile Modeling, AM includes the Apply Modeling Standards practice. The basic idea is that developers should agree to and follow a common set of modeling standards on a software project. Just like there is value in following common coding conventions, clean code that follows your chosen coding guidelines is easier to understand and evolve than code that doesn't, there is similar value in following common modeling conventions.      3.4 Identify Relationships In the real world entities have relationships with other entities.  For example, customers PLACE orders, customers LIVE AT addresses, and line items ARE PART OF orders. Place, live at, and are part of are all terms that define relationships between entities.  The relationships between entities are conceptually identical to the relationships (associations) between objects.    Figure 5 depicts a partial LDM for an online ordering system.  The first thing to notice is the various styles applied to relationship names and roles – different relationships require different approaches.  For example the relationship between Customer and Order has two names, places and is placed by, whereas the relationship between Customer and Address has one.  In this example having a second name on the relationship, the idea being that you want to specify how to read the relationship in each direction, is redundant – you’re better off to find a clear wording for a single relationship name, decreasing the clutter on your diagram.  Similarly you will often find that by specifying the roles that an entity plays in a relationship will often negate the need to give the relationship a name (although some CASE tools may inadvertently force you to do this).  For example the role of billing address and the label billed to are clearly redundant, you really only need one.  For example the role part of that Line Item has in its relationship with Order is sufficiently obvious without a relationship name.  Figure 5. A logical data model (Information Engineering notation).  Identifying relationships    You also need to identify the cardinality and optionality of a relationship (the UML combines the concepts of optionality and cardinality into the single concept of multiplicity). Cardinality represents the concept of “how many\" whereas optionality represents the concept of “whether you must have something.\" For example, it is not enough to know that customers place orders.  How many orders can a customer place?  None, one, or several? Furthermore, relationships are two-way streets: not only do customers place orders, but orders are placed by customers.  This leads to questions like: how many customers can be enrolled in any given order and is it possible to have an order with no customer involved? Figure 5 shows that customers place zero or more orders and that any given order is placed by one customer and one customer only.  It also shows that a customer lives at one or more addresses and that any given address has zero or more customers living at it.  Although the UML distinguishes between different types of relationships – associations, inheritance, aggregation, composition, and dependency – data modelers often aren’t as concerned with this issue as much as object modelers are. Subtyping, one application of inheritance, is often found in data models, an example of which is the is a relationship between Item and it’s two “sub entities\" Service and Product.   Aggregation and composition are much less common and typically must be implied from the data model, as you see with the part of role that Line Item takes with Order. UML dependencies are typically a software construct and therefore wouldn’t appear on a data model, unless of course it was a very highly detailed physical model that showed how views, triggers, or stored procedures depended on other aspects of the database schema.     3.5 Apply Data Model Patterns Some data modelers will apply common data model patterns, David Hay’s book Data Model Patterns is the best reference on the subject, just as object-oriented developers will apply analysis patterns (Fowler 1997; Ambler 1997) and design patterns (Gamma et al. 1995).  Data model patterns are conceptually closest to analysis patterns because they describe solutions to common domain issues.  Hay’s book is a very good reference for anyone involved in analysis-level modeling, even when you’re taking an object approach instead of a data approach because his patterns model business structures from a wide variety of business domains. \t  Data Model Patterns   3.6 Assign Keys There are two fundamental strategies for assigning keys to tables.  First, you could assign a natural key which is one or more existing data attributes that are unique to the business concept.  The Customer table of Figure 6 there was two candidate keys, in this case CustomerNumber and SocialSecurityNumber. Second, you could introduce a new column, called a surrogate key, which is a key that has no business meaning. An example of which is the AddressID column of the Address table in Figure 6. Addresses don’t have an “easy\" natural key because you would need to use all of the columns of the Address table to form a key for itself (you might be able to get away with just the combination of Street and ZipCode depending on your problem domain), therefore introducing a surrogate key is a much better option in this case.   Figure 6. Customer and Address revisited (UML notation).    Let's consider Figure 6 in more detail. Figure 6 presents an alternative design to that presented in Figure 2, a different naming convention was adopted and the model itself is more extensive. In Figure 6 the Customer table has the CustomerNumber column as its primary key and SocialSecurityNumber as an alternate key. This indicates that the preferred way to access customer information is through the value of a person’s customer number although your software can get at the same information if it has the person’s social security number.  The CustomerHasAddress table has a composite primary key, the combination of CustomerNumber and AddressID.  A foreign key is one or more attributes in an entity type that represents a key, either primary or secondary, in another entity type.  Foreign keys are used to maintain relationships between rows.  For example, the relationships between rows in the CustomerHasAddress table and the Customer table is maintained by the CustomerNumber column within the CustomerHasAddress table. The interesting thing about the CustomerNumber column is the fact that it is part of the primary key for CustomerHasAddress as well as the foreign key to the Customer table. Similarly, the AddressID column is part of the primary key of CustomerHasAddress as well as a foreign key to the Address table to maintain the relationship with rows of Address.  Although the \"natural vs. surrogate\" debate is one of the great religious issues within the data community, the fact is that neither strategy is perfect and you'll discover that in practice (as we see in Figure 6 ) sometimes it makes sense to use natural keys and sometimes it makes sense to use surrogate keys. In Choosing a Primary Key: Natural or Surrogate? I describe the relevant issues in detail.    3.7 Normalize to Reduce Data Redundancy Data normalization is a process in which data attributes within a data model are organized to increase the cohesion of entity types.  In other words, the goal of data normalization is to reduce and even eliminate data redundancy, an important consideration for application developers because it is incredibly difficult to stores objects in a relational database that maintains the same information in several places.  Table 2 summarizes the three most common normalization rules describing how to put entity types into a series of increasing levels of normalization. Higher levels of data normalization (Date 2000) are beyond the scope of this book.  With respect to terminology, a data schema is considered to be at the level of normalization of its least normalized entity type.  For example, if all of your entity types are at second normal form (2NF) or higher then we say that your data schema is at 2NF. \t  Data Modeling for Information Professionals Table 2. Data Normalization Rules.  Level \tRule First normal form (1NF) \tAn entity type is in 1NF when it contains no repeating groups of data. Second normal form (2NF) \tAn entity type is in 2NF when it is in 1NF and when all of its non-key attributes are fully dependent on its primary key.  Third normal form (3NF) \tAn entity type is in 3NF when it is in 2NF and when all of its attributes are directly dependent on the primary key.    Figure 7 depicts a database schema in ONF whereas Figure 8 depicts a normalized schema in 3NF. Read the Introduction to Data Normalization essay for details.   Why data normalization?  The advantage of having a highly normalized data schema is that information is stored in one place and one place only, reducing the possibility of inconsistent data. Furthermore, highly-normalized data schemas in general are closer conceptually to object-oriented schemas because the object-oriented goals of promoting high cohesion and loose coupling between classes results in similar solutions (at least from a data point of view). This generally makes it easier to map your objects to your data schema.  Unfortunately, normalization usually comes at a performance cost.  With the data schema of Figure 7 all the data for a single order is stored in one row (assuming orders of up to nine order items), making it very easy to access.  With the data schema of Figure 7 you could quickly determine the total amount of an order by reading the single row from the Order0NF table.  To do so with the data schema of Figure 8 you would need to read data from a row in the Order table, data from all the rows from the OrderItem table for that order and data from the corresponding rows in the Item table for each order item. For this query, the data schema of Figure 7 very likely provides better performance.     Figure 7. An Initial Data Schema for Order (UML Notation).  Order in 0NF     Figure 8. A normalized schema in 3NF (UML Notation).  Order Fully Normalized  In class modeling, there is a similar concept called Class Normalization although that is beyond the scope of this article. 3.8 Denormalize to Improve Performance Normalized data schemas, when put into production, often suffer from performance problems. This makes sense – the rules of data normalization focus on reducing data redundancy, not on improving performance of data access.  An important part of data modeling is to denormalize portions of your data schema to improve database access times.  For example, the data model of Figure 9 looks nothing like the normalized schema of Figure 8. To understand why the differences between the schemas exist you must consider the performance needs of the application. The primary goal of this system is to process new orders from online customers as quickly as possible. To do this customers need to be able to search for items and add them to their order quickly, remove items from their order if need be, then have their final order totaled and recorded quickly. The secondary goal of the system is to the process, ship, and bill the orders afterwards.     Figure 9. A Denormalized Order Data Schema (UML notation).  Order Denormalized     To denormalize the data schema the following decisions were made:      To support quick searching of item information the Item table was left alone.     To support the addition and removal of order items to an order the concept of an OrderItem table was kept, albeit split in two to support outstanding orders and fulfilled orders. New order items can easily be inserted into the OutstandingOrderItem table, or removed from it, as needed.     To support order processing the Order and OrderItem tables were reworked into pairs to handle outstanding and fulfilled orders respectively. Basic order information is first stored in the OutstandingOrder and OutstandingOrderItem tables and then when the order has been shipped and paid for the data is then removed from those tables and copied into the FulfilledOrder and FulfilledOrderItem tables respectively. Data access time to the two tables for outstanding orders is reduced because only the active orders are being stored there. On average an order may be outstanding for a couple of days, whereas for financial reporting reasons may be stored in the fulfilled order tables for several years until archived. There is a performance penalty under this scheme because of the need to delete outstanding orders and then resave them as fulfilled orders, clearly something that would need to be processed as a transaction.      The contact information for the person(s) the order is being shipped and billed to was also denormalized back into the Order table, reducing the time it takes to write an order to the database because there is now one write instead of two or three.  The retrieval and deletion times for that data would also be similarly improved.  Note that if your initial, normalized data design meets the performance needs of your application then it is fine as is.  Denormalization should be resorted to only when performance testing shows that you have a problem with your objects and subsequent profiling reveals that you need to improve database access time. As my grandfather said, if it ain’t broke don’t fix it.   5. Evolutionary/Agile Data Modeling  Evolutionary data modeling is data modeling performed in an iterative and incremental manner. The article Evolutionary Development explores evolutionary software development in greater detail.  Agile data modeling is evolutionary data modeling done in a collaborative manner.  The article Agile Data Modeling: From Domain Modeling to Physical Modeling works through a case study which shows how to take an agile approach to data modeling.  Although you wouldn’t think it, data modeling can be one of the most challenging tasks that an Agile DBA can be involved with on an agile software development project. Your approach to data modeling will often be at the center of any controversy between the agile software developers and the traditional data professionals within your organization. Agile software developers will lean towards an evolutionary approach where data modeling is just one of many activities whereas traditional data professionals will often lean towards a big design up front (BDUF) approach where data models are the primary artifacts, if not THE artifacts.  This problem results from a combination of the cultural impedance mismatch, a misguided need to enforce the \"one truth\", and “normal\" political maneuvering within your organization. As a result Agile DBAs often find that navigating the political waters is an important part of their data modeling efforts.    6. How to Become Better At Modeling Data How do you improve your data modeling skills?  Practice, practice, practice. Whenever you get a chance you should work closely with Agile DBAs, volunteer to model data with them, and ask them questions as the work progresses.  Agile DBAs will be following the AM practice Model With Others so should welcome the assistance as well as the questions – one of the best ways to really learn your craft is to have someone as “why are you doing it that way\". You should be able to learn physical data modeling skills from Agile DBAs, and often logical data modeling skills as well.      Similarly you should take the opportunity to work with the enterprise architects within your organization.  As you saw in Agile Enterprise Architecture they should be taking an active role on your project, mentoring your project team in the enterprise architecture (if any), mentoring you in modeling and architectural skills, and aiding in your team’s modeling and development efforts. Once again, volunteer to work with them and ask questions when you are doing so.  Enterprise architects will be able to teach you conceptual and logical data modeling skills as well as instill an appreciation for enterprise issues.  You also need to do some reading.  Although this article is a good start it is only a brief introduction.  The best approach is to simply ask the Agile DBAs that you work with what they think you should read.   My final word of advice is that it is critical for application developers to understand and appreciate the fundamentals of data modeling.  This is a valuable skill to have and has been since the 1970s.  It also provides a common framework within which you can work with Agile DBAs, and may even prove to be the initial skill that enables you to make a career transition into becoming a full-fledged Agile DBA.", "category": "Edison", "id": 121}
{"skillName": "DSDA03", "skillText": "The decision-making process is marked by two kinds of : organizational and technical. The organizational  are those related to companies’ day-to-day functioning, where decisions must be made and aligned with the companies’ strategy. The technical  include the toolset used to aid the decision making process such as information systems, data repositories, formal modeling, and analysis of decisions. This work highlights a subset of the  combined to define an integrated model of decision making using big data, business intelligence, decision support systems, and organizational learning all working together to provide the decision maker with a reliable visualization of the decision-related opportunities. The  objective of this work is to perform a theoretical analysis and discussion about these , thus  an understanding of why and how they work together.   Organizations need to use a structured view of information to improve their decision- making process. To achieve this structured view, they have to collect and store data, perform an analysis, and transform the results into useful and valuable information. To perform these analytical and transformational processes, it is necessary to use of an appropriate environment composed of a large and generalist repository, a processor core with the appropriate intelligence (Business Intelligence [BI]), and a user-friendly interface. The repository must be filled with data originating from many different kinds of external and internal data sources. These repositories are the data warehouses (gener‐ alists) and data marts (when considering a specific company activity or sector), and most recently, Big Data. The Big Data concept and its applications have emerged from the increasing volumes of external and internal data from organizations that are differentiated from other data‐ bases in four aspects: volume, velocity, variety, and value. Volume considers the data amount, velocity refers to the speediness with which data may be analyzed and processed, © Springer International Publishing Switzerland 2015 B. Delibašić et al. (Eds.): ICDSST 2015, LNBIP 216, pp. 10–21, 2015. DOI: 10.1007/978-3-319-18533-0_2The Roles of Big Data in the Decision-Support Process 11 variety describes the different kinds and sources of data that may be structured, and value refers to valuable discoveries hidden in  datasets [1]. Big Data has the potential to aid in identifying opportunities related to decision in the intelligence  of Simon’s [2] model. In some cases, the stored data may be used to aid the decision-making process. In this context, the term “intelligence” refers to knowledge discovery with mining algorithms. In this way, Big Data use can be aligned with the application of Business Intelligence (BI) tools to provide an intelligent aid for organizational processes. The data necessary to obtain the business perceptions must be acquired, filtered, stored, and analyzed after the available data are heterogeneous and in a  volume. The processes of filtering and analysis of the data are very complex, because of that it is necessary the use BI strategies and tools. The  proposal of the present study is to develop an investigation that describes the roles of Big Data, and BI in the decision-making process, and to provide researchers and practitioners with a clear vision of the challenges and opportunities of applying data storage technologies so that new knowledge can be discovered. The sequence of this work is as follows. Section 2 provides a background for Big Data and some of its applications. Section 3 introduces the concept of DSS. Section 4 concep‐ tualize BI and presents its organizational and technological components. Section 5 presents a scheme for the integration between Big Data, BI, decision structuring and making process, and organizational learning. Section 6 contains a discussion about the integration perspective of the decision-making process, according the scheme presented in Sect. 5. Finally, the conclusion presents the limitations of this study and highlights the insights this work has gained. 2 Big Data With data increasing globally, the term “Big Data” is ly used to describe large datasets. Compared with other traditional databases, Big Data includes a large amount of unstructured data that must be analyzed in real time. Big Data also brings new oppor‐ tunities for the discovery of new values that are temporarily hidden [3]. Big Data is a broad and abstract concept that is receiving  recognition and is being highlighted both in academics and business. It is a tool to support the decision- making, process by using technology to rapidly analyze large amounts of data of different types (e.g., structured data from relational databases and unstructured data such as images, videos, emails, transaction data, and social media interactions) from a variety of sources to produce a stream of actionable knowledge [4]. After the data is collected and stored, the biggest challenge is not just about managing it but also the analysis and extraction of information with significant value for the organization. Big Data works in the presence of unstructured data and techniques of data analysis that are structured to solve the problem [1]. A combination called the 4Vs characterizes Big Data in the literature: volume, velocity, variety, and value [5]. Volume has a  influence when describing Big Data as large amounts of data are generated by individuals, groups, and organizations. Zikopoulus et al. reports that the estimated data production by 2010 was about 35 zettabytes [6].12 T. Poleto et al. The second item, velocity, refers to the rates at which Big Data are collected, processed, and prepared—a huge, steady stream of data that is impossible to process with traditional solutions, For this reason, it is important to consider not only “where” data are stored but also “how” they are stored. The third item, variety, is related to the types of data generated from social sources, including mobile and traditional data. With the explosion of social networks, smart devices, and sensors, data have become complex because they include semi-structured and unstructured information from log files, web pages, index searches, cross- media, e-mail, documents, and forums. Finally, the value can be discovered from the analysis of the hidden data, so Big Data can provide new findings of new values and opportunities to assist in making decisions. However, management of this data can be considered as a challenge for organizations [1]. In order to demonstrate the differentiation between Big Data and Small Data, we analyzed them using five  characteristics: s, data location, data structure, data preparation, and analysis, in Table 1. Importantly, relational databases are not obsolete, on the contrary, they continue to be useful to a number of applications. In practice, how larger a database becomes, the higher the cost of processing and labor, so it is necessary to optimize and add new solutions to improve storage  er flexibility. For the purpose to better understand the impact of science and Big Data solutions, the applications and Big Data solutions in the following different contexts will be presented: education, social media and social networking, and smart cities. Grillenberger and Fau used educational data to analyze student performance [7]. Their learning styles were also clarified by the use of Big Data in conjunction with teaching strategies to gain a better understanding of the students’ knowledge and an assessment of their progress. These data can also help groups of students with similar learning styles or their difficulties, thus defining a new form of personalized learning resources based on and supported by computational models. Big Data has created new opportunities for researchers to achieve high relevance when working in social networks. In this context, Chang, Kauffman and Kwon used communications environments to discuss the causes of the paradigm shift and explored the ways that decision support is researched, and, more broadly, applied to the social sciences [8]. In the context of a smart city, Dobre and Xhafa provide a platform for process auto‐ mation collection and aggregation of large-scale information. Moreover, they present an application for an intelligent transportation system [9]. The application is designed to assist users and cities to resolving the traffic problems in big cities. The combination of these services provides support for the application in intelligent cities that can, benefit from using the information dataset. The value of Big Data is driving the creation of new tools and systems to facilitate intelligence in consumer behavior, economic forecasting, and capital markets. Market domination may be driven by which companies absorb and use the best data the fastest. Understanding the social context of individuals’ and organizations’ actions means a company can track not only what their customers do but also get much closer to learning why they do what they do.The Roles of Big Data in the Decision-Support Process 13 Table 1. Comparison of  characteristics of Big Data and Small Data. Aspects s Big Data In general, they are projected from a predetermined Small Data They are generally designed to answer a specific  and have a er level of flexibility, question and control in a particular context. considering the context of the problem. For For example, inventory control, get only example, the market scenario analysis to iden‐ information concerning the entry and exit of tify forms to accelerate the sales can be consid‐ goods, is not always done interaction between ered customer and supplier, acting on the basis of current market Data location The location normally aggregates data spread In general, the data come from the internal organ‐ across different media, which can be in several ization and the data files. For example, Internet servers. The architecture consists of a working with spreadsheets results in  distributed computing where multiple servers increases on internal control work together to store and process information. High power scalability, low cost of implemen‐ tation Data structure The structure is usually able to absorb unstructured The structure usually contains structured data. data (e.g., free text documents, images, movies, Data are represented by uniform records in an sound recordings, and physical objects). In orderly spreadsheet. For example, the enter‐ others words, Big Data is just to be able to work prise resource planning (ERP) that are with many variables simultaneously, as reading systems that have a pre-defined architecture and rendering images in minimal time and very and their records represents a structured way efficiently. For example, smart city applica‐ to work with data within organizations tions, using real-time information to describe the traffic of a big city Data prepara‐ tion In general, the data come from different sources In many cases, the data users prepare their own and are prepared by several users. People who data for their own purposes. For example, use the data rarely are the ones who prepared. presenting the results according to a specific In this context, different people in different context to which the user is located organizational roles contributes to disseminate information Analysis Analysis is usually done in incremental steps. The In most cases, all the data for the project can be data are extracted, revised, normalized, analyzed all at once. In this case, the structure processed, visualized, interpreted, and then is pre-defined and based on the specific analyzed with different methods. For example, context. Also are used Structured Query complex techniques of data analysis combining Language (SQL) combined with appropriate data mining and artificial intelligence programming languages to create procedures to mining, process and analyze the data Sources: [4, 1].14 T. Poleto et al. To date, for the use of Big Data, a modern infrastructure is needed to overcome the limitations related to language and methodology. Guidelines are needed in a short time in order to deal with such complexities, as different tools and techniques and specific solutions have to be defined and implemented. Furthermore, different channels through which data are collected daily increases the difficulties of companies in identifying which is the right solution to get relevant results from the data path. In this context, the tech‐ nology of BI and DSS will be presented. 3 Decision Support Systems (DSS) Information and knowledge are the most valuable assets for organizations’ decision- making processes and need a medium to process data into information loaded with value and relevance for use in organizational processes. Information Systems (IS) represent these media. Specifically focused on the decision-making process, the DSS work for the processing, analyzing, sharing, visualizing of important information to aid in the process of knowledge aggregation and transformation, and thereby improve the organizational knowledge. DSS are IS designed to support solutions for decision-making problems. The term DSS has its origin in two streams: the original studies of Simon’s research team in the late 1950s and the early 1960s and the technical works on interactive computer systems by Gerrity’s research team in the 1960s [10]. In a more detailed definition, DSS are interactive, computer-based IS that help decision-makers utilize data, models, solvers, visualizations, and the user interface to solve semi-structured or unstructured problems. DSS are built using a DSS Generator (DSSG) as an assembling component [11]. DSS have a strict link with intelligence-design-choice model, but acting with more power in the choice  [2]. Their  objective is to support a decision by determining which alternatives to solve the problem are more appropriate. Although the choice is made by a human agent (a manager, treated as a decision-maker within this process), the DSS role is to provide a friendly interface where the agents can build scenarios and simulate and obtain reports and visualizations to support the decisions [12]. This kind of system has a set of basic  that includes a data base and a model base with their respective management, the business rules to process data according a chosen model (e.g., the core of the system), and a user interface [10]. Data and model bases and their respective management systems allow for business rules in processing data according to a model to formulate the possibilities of solutions for the problem. 4 Business Intelligence An organization’s decision-making process begins with the intelligence  of Simon’s [2] model. In this , the perception is made that there is a problem to be solved in the future by applying problem structuring methods. Also in this , the tools of BI may be used to support the organization’s discovery of opportunities for decision-making by,  advanced analytics and assuring data integration [13].The Roles of Big Data in the Decision-Support Process 15 So, besides the problem solving, decision opportunities can be added to the set of benefits that BI can bequeath to the scope of decision support. The definition of BI can be expressed in two ways: in a holistic organizational deci‐ sion-making approach and in the technical point of view [14]. Similarly Handzic, Ozlen and Durmic presents two kinds of concepts for BI: one centered on data analytics supporting decision-making processes in organizations and the other focused on tools and technologies for data storage and mining for knowledge discovery [15]. This study will adopt the strategy of considering BI as a system that includes both organizational and technical perspectives that provide the information needed to perform an analysis the aid the generation of decision opportunities, the decision-making process, and the organizational learning process. Regarding this definition, BI covers all the processes involved in extracting valuable and useful information from the mass of data that exists within a typical organization to support the decision-making process. Business Intelligence Systems (BIS) are those that makes use of a combined process involving IT solutions and business experts’ knowl‐ edge on the operation of business, integration and organizational management obtained as a result of intelligent decision-making [16, 17]. According to Azma and Mostafapour, there are two  features of data: the organ‐ izational learning process and the smart processing of data [17]. The organizational learning includes the discovery of new knowledge and dissemination of this knowledge to those who need it. The smart processing includes analyzing and assessing the infor‐ mation,  decision support to ensure the aligning of the future performance of the organization with the planning, and  knowledge feedback about the involved processes to be combined with pre-existing (explicit) knowledge. Chang, Hsu and Shiau set up BI like both a product and a process. From the process perspective, the   of BI is to aid the decision-making process and reduce the time spent on the decision [18]. For this to occur, it is necessary that all the sets of basic components be defined and implemented. From the product perspective, the BI is the IT component that contains the referred to set of basic components and that can be used as the core engine of DSS to generate analytics for managers as the decision-makers. From an organizational perspective, BI is part of a decision environment that combines both technology sets and human capacities in order to obtain decisions stra‐ tegically aligned with the organization’s planning. This is the holistic organizational decision-making approach that Işık, Jones and Sidorova mentioned and that still includes BI capabilities such as data quality, integration with other systems, user access, system flexibility, and risk management support [14]. These authors conceptualized the first three capabilities as technological BI and the last two as organizational BI. Handzic reinforces the Knowledge Management (KM) and organizational learning point of view for BI, considering some organizational aspects focused on human inter‐ action which the authors called socio-technical perspective: organizational culture, leadership, and measurement for successful implementations [19]. Figure 1 shows this perspective. Considering the definitions by Handzic [19] and Grünwald and Taubner [20] leads to understanding that the information evolutionary scale is made possible through a technical toolset that supports the necessary transformations between the points of the16 T. Poleto et al. Fig. 1. Business process support using BI. Source: Adapted from [20]. scale (since data is until wisdom), including  of Artificial Intelligence (AI). The business process, in turn, includes the decision-making process and the action to imple‐ ment the decision. In Fig. 1, the combination of BI, business process, and the technical toolset could be understood as the  that define a DSS, because they represent information transformations followed by their application in decision-making and the provision of feedback to initiate a new cycle of information transformation. The works cited previously describe intrinsic issues to Big Data and highlight the use of BI alone. The differential of our proposal is to get an integrated view of the technologies involved, aggregating value to the decision-making process. 5 Integrated Model for Decision-Making Process, Big Data, and BI Tools Simon’s decision model summarizes the decision-making process into three s, as introduced previously. Each  this model is susceptible to the use of methods and tools from organizational and technological perspectives. The organizational perspec‐ tive may use Problem Structuring Methods (PSM); Multi-criteria Decision Aid (MCDA); and KM techniques such as brainstorming, communities of practice, best practices, narratives, yellow pages, peers assistance, and knowledge mapping. These methods and techniques aids in the knowledge elicitation of the actors involved in the decision-making process, thus contributing to the necessary expertise necessary for solve the specific problem in question, as in the case of PSM and KM techniques, or acting to provide recommendations to solve this problem, as in the case of MCDA. Technological tools involve data repositories (e.g., data warehouses and data marts) filled with data from public sources, BI or even AI and Problem Solving Methods (PSolM) originated from Knowledge Engineering (KE) (e.g., CommonKADs and Meth‐ odology and Tools Oriented to Knowledge-Based Engineering Applications [MOKA]).The Roles of Big Data in the Decision-Support Process 17 These tools are important  that contribute to store, access and analyze infor‐ mation, discovering and sharing knew knowledge in databases and even supporting the application of the organizational perspective’s methods and technics. The  purpose of this work is the integration of the decision-making process with some of these tools presented in Fig. 2, considering the perspective of the predictive approach to decision-making. In this perspective, the use of methods to structure deci‐ sion problems and suggest alternatives to choose from is an important issue and an efficient way to support the DSS design and development. Combined with the predictive approach, this process makes use of BI tools to provide do information to aid all the s of the process. Identification of criteria for data extraction from Big Data Business Intelligence tools Fig. 2. Integrated model of the decision-making process. It is noteworthy that some of these  are framed within the s of Simon’s model. In the  of intelligence, by making use of Big Data powered by internal and external data sources, organizations can use of BI strategies and tools to aid in identifying relevant information, and then the generation of decision opportunities occurs. The function of the design  is to provide a methodology to aid the choice of the alternatives based in what was defined in the problem structuring process during the intelligence . This design must also be incorporated into this methodology, as formal aspects related to the method or model that are defined according the problems identified during the problem structuring process. The development of DSS has made18 T. Poleto et al. the use of this model viable by allowing the decision-makers, through a friendly and easy-to-use interface, to perform a series of configurations. In the final  of choice, the decision-makers will use the results generated by DSS to complete the decision-making process with the choice of one, or a set of, alter‐ natives, that will then be implemented by the organization. All these processes produce new knowledge to be combined with previous knowl‐ edge about the do of the problem. This new knowledge will provide feedback to power the Big Data so that it can be used as necessary, thus fulfilling its role in the organizational learning process. Each element of the integrated model is described as follows: (a) Content acquisition through public and private organizational data sources: This is ly concerned with the collection, storage, and integration of relevant infor‐ mation necessary to produce a content item. In the course of this process, informa‐ tion is being pooled from internal or external sources for further processing. Big Data incorporates different types of sources, including text, audio, video, social networks, images, time forecasting, etc. Strictly, the  purpose of this element is none other than the data acquisition from Big Data to use in decision-making process. (b) Intelligence: The whole world is producing a  amount of data. Thus, this is relevant as Big Data obtains its value from three of the 4Vs: volume, variety, and velocity. In this , aggregated values from stored data have a fundamental role for the creation of opportunities and alternatives once the data are analyzed. More‐ over, in this context it is important to highlight the importance of data visualization. For example, in a spreadsheet is difficult to trends in data. However, the use of graphics and images improve the perception for the data analysis helping a faster recognition of trends or patters and improving the capacity of the data analyst to perform his work. Based on the visualization provided by the  that composes Big Data concept, corrective actions can be done in case of deviations and negative trends. Therefore, in the intelligence  the concept of Big Data should not be analyzed only with volume, but can improve the ability to view this data, filtering a large volume of data in different contexts of information. Visuali‐ zation techniques are now extremely important for the generation of value of the concept of Big Data. After all, Big Data is not a concept just about data, but we can extract insights and intelligence and visualization is the fundamental key to the decision-making process. The intelligence represents the capacity to aggregate value to acquired data in order to obtain relevant information, applicable in the organizational problem solving. These information should be capable of contextu‐ alize with internal and external phenomena of the organization, ensuring the other following  the necessary power of action to satisfactorily contribute to resolve the problem. (c) Opportunities and alternatives generation: This is the process of creating alterna‐ tives, which is not a trivial task. It starts with dataset analysis that enable decision- makers to obtain a global view of the process. Then, from the analyses performed through BI tools with Big Data content, decision-makers pro-actively create oppor‐ tunities and generate opportunities to solve the decision problem. This  alsoThe Roles of Big Data in the Decision-Support Process 19 works for the definition of the criteria, which the decision-makers will use to judge or evaluate each alternative. (d) DSS: With the opportunities identified and having the criteria and alternatives to evaluate, DSS may be implemented according a decision problem that will predict which method is the most adequate. DSS will act in helping decision-makers in obtaining an indication or a recommendation of alternatives to choose from that will be implemented to solve the problem. (e) Implementation of decision: After a choice is made, alternatives will be imple‐ mented in organizations to actively solve the identified problem. As a last element, the Organizational Learning says respect to all these processes’  generating important knowledge about the decision problem. This knowledge may be captured, registered, and stored in a knowledge repository to provide organiza‐ tional memory about the problem do and will be available for use at any time. The standard flow of this new knowledge, after the implementation of the chosen action, runs to private (or internal) data sources, e.g., a base of managerial practices. 6 Discussion Knowledge extracted adequately from Big Data aggregates the value that decision- makers use to a decision . This work provided theoretical evidence to corroborate the idea that the perspective of historical data combined with decision- makers’ knowledge and experience, formal problem structuring, and use of decision methods or models may the decision-making process more robust and more reli‐ able. Generally, companies use the descriptive approach to decisions, by performing an analysis based only on historical data. The focus solely on the past makes it difficult to concentrate on new strategies for the future. The proposition of the present work also considers this descriptive approach, but it recognizes the value of the predictive approach in order to provide recommendations to solve a decision problem, based on decision- makers’ knowledge and judgment, and information technology: Big Data, BI, and DSS. The Big Data study performed here started with the analysis of the data’s influence over the decision-making process by ensuring that decision-makers can discover oppor‐ tunities to act problem solving. The  contributions of the theoretical approach presented here are (i) develop a perspective that combines the decision-making process, Big Data, BI, DSS, and organ‐ izational learning and (ii) use the concept that Big Data works as a data provider over which may be applied BI techniques and tools may be applied ly in supporting the discovery of opportunities for a decision. Decision-makers, when preparing for making a decision, incorporate their knowl‐ edge and discernment along with an organizational learning process that will help them to create an organizational memory that provides knowledge generated through the process for later use. Thus, beyond technological toolsets and decision-making and methodologies, the process described here takes into account the subjective character‐ istics linked to the decision-makers’ perceptions, experiences, and personalities.20 T. Poleto et al. The use of Big Data provides to managers the possibility to explore both internal and external information, not only identifying a decision problem but also having as proposal the potential to increase de intelligence power within the decision-making process. 7 Conclusion The increasing amount of data that arrives at organizations accumulate through elec‐ tronic communication is amazing, in that not only has the volume of the data change, but also the variety of information collected in through several communication channels ranging from clicks on the Internet to the unstructured information from social media. In addition, the speed at which organizations can collect, analyze, and respond to infor‐ mation in different dimensions is increasing. Big Data has become a generic term, but in essence, it presents two challenges for organizations. First, business leaders must implement new technologies and then prepare for a potential revolution in the collection and measurement of information. Second, and most important, the organization as a whole must adapt to this new philosophy about how decisions are made by understanding the real value of Big Data. Organizations must understand the role of the Big Data associated with decision- making, with the emphasis on creating opportunities from these decisions, because we live in a world that is always connected, and where consumer preferences change every hour. Thus, analysts can check multiple communication channels simultaneously and trace certain profiles or decider behaviors. The  contribution of this work is to promote the integrated view of Big Data, BI and DSS inside the context of decision-making process, assisting managers to create new opportunities to resolve a specific problem. The crucial point is to look widely for new sources of data to help a decision. Furthermore, Big Data not only transforms the processes of management and technology but it also promotes changes in culture and learning in organizations. Ultimately, Big Data can be very useful if used adequately in the decision-making process, but just its use will not guide the decision itself and it will not generate alter‐ natives or predict the results. For this, the participation of decision-makers is essential, as their experience and tacit knowledge are necessary to aggregate value over informa‐ tion and the possible knowledge stored. From this initial study, where the idea of get an integrated view of all these  as decision-making tools, we can create a set of perspectives to apply in future researches, as example a detailed exploration focused on each  of the model. Other ideas: semantic exploration of Big Data applied to decision problems structuring, direct integration between Big Data and BI tools to fulfil organizational repositories  data to the information systems.  JULIE DEVOLL: This is Julie Devoll from Harvard Business Press. I’m here today with Tom Davenport, the President’s Distinguished Professor of Information Technology and Management at Babson College and author of the new book, Analytics at Work: Smarter Decisions, Better Results.  Tom, thank you for joining us today.  TOM DAVENPORT: My pleasure, Julie.  JULIE DEVOLL: Tom, let’s first get everybody up to speed on analytics. Can you give us a quick definition of what you mean when you talk about organizations competing on analytics?  TOM DAVENPORT: Well, by analytics we mean the systematic use of data and facts and typically, quantitative, but can be qualitative as well, analysis to make decisions. And this book is really about how any company can become more analytical in its decisions, more fact-based if you will.  My the first book on this topic, Competing on Analytics, was how companies can really get competitive advantage out of analytics. And we found that a number of organizations said, we like the idea of becoming more analytical, but we’re not sure that we want to make it our primary source of competitive advantage. So this book is how any company can do more of that kind of decision-making.  JULIE DEVOLL: Great. So let’s hone in a little on analytical leadership. What do you see analytical leaders doing that allows them to make better decisions?  TOM DAVENPORT: Well, it’s a variety of things. One is that they insist on decisions being made on the basis of facts. So you see these kinds of leaders pushing back when people say, I think we should develop this particular product, or we should make this change to our website, or whatever. The best companies will say, well, where’s the data to support that particular hypothesis?  One of the great examples of that is Google. There’s such a highly analytical culture at Google that you can’t really recommend any sort of change, a new product, or God knows, a change on the Search page without having substantial amounts of data that would suggest that it’s a good idea. Having tried it out on a small scale and how did that particular randomized experiment work? So it’s just establishing a culture that says we make decisions around data here. Maybe not all the time, maybe if we can’t get data, we obviously don’t let that paralyze us. But that’s a big factor.  And I think for leaders setting a personal example. If you’re making decisions from the gut yourself, as an individual, and then telling other people that they need to have data, then they are going to see that you’re not really that serious about it.  So we talk, for example, about two brothers, the McCann’s, Jim and Chris McCann, who run 1800flowers.com, and Jim, the CEO and founder, who was originally a social worker was originally not that analytical. His brother, Chris, much more so. And over time, Jim has come to see that he needs to ask for data as well, so that he can set that kind of personal example that others will believe in and emulate.  JULIE DEVOLL: So it sounds like companies need to manage their analytical talent as a strategic resource. How do you recommend HR departments go about actually finding these people, hiring them, and then retaining them once they’re brought on board?  TOM DAVENPORT: I think it is one of the key assets and because it’s limited in availability, there are only so many good, quantitative people around who are also good communicators that I think it will be a real strategic resource for the organizations that get those people at a relatively early stage and kind of lock them up. Obviously, nobody is totally locked up. But if you hire them and you provide them with challenging work, and my co-author Jeanne Harris did some research suggesting that the more centralized approaches to managing analysts, putting them in a center of excellence, or somehow managing them at the enterprise level, maybe it’s a centralized consulting group. More centralized approaches tended to create more engaged analysts who were, it would seem, more likely to hang around. They enjoyed their jobs, were more satisfied and engage in their work.  So I think analysts are kind of a special breed sometimes, and they don’t want to be acting as solo practitioners. They’d like to be able to share ideas with other people. They’d like to feel like there were a valued resource and hence, not only one of them in a department or a business unit. So maybe that explains why the more centralized approaches to managing and organizing analysts tended to be the more successful in terms of yielding happy analysts.  JULIE DEVOLL: So you mentioned Progressive Insurance. What are some other examples, or maybe one good example of a successful company that uses analytics to make better decisions?  TOM DAVENPORT: Well, there are lots of examples in the book. I mentioned 1-800 Flowers as well. We profile Carnival Cruises. But I probably center in on the companies that have some decisions that are common and repetitive, if you will, made a lot of time, so they have good data on them. Google is certainly a great example. We talk about Amazon a fair amount, which makes a lot of decisions around pricing and supply chain and promotions with analytics. Certainly their web page, Jeff Bezos once fired, supposedly, 10 out of 12 web designers because they did not use analytics in the design of the web pages that they proposed.  In retail, you have companies like Kroger that are very good now at targeting promotions. JC Penney very good at merchandising decisions. Best Buy uses analytics a lot for segmentation. We talk a fair amount about Best Buy.  Generally what you don’t find, Julie, is a lot of companies using analytics for the really big decisions. Things like a merger, or acquisition, or a really big new product development project. The one exception that we found and talk about in the book is Chevron, which basically, they have a requirement that if you’re making a decision that involves more than $100 million dollars, which is not a huge number for a big oil company like Chevron. But if you’re making a decision that’s going to involve that much capital, you have to follow a certain quantitative decision analysis approach. And they’ve been generally quite successful at the kinds of decisions that they made, and they go back and have a mandatory review of how did that decision work out. So I think overall, it works very well for them.  JULIE DEVOLL: Great. So I’ll put you on the spot a little bit here. What are some companies that you’re seeing out there that haven’t been successful, and maybe an analytical approach to their decision-making may have helped them?  TOM DAVENPORT: Well, I think, I mentioned some retailers that have been pretty successful. Certainly historically, the department stores, Macy’s, for example, has not been highly analytical. They’re trying to change and they’ve hired the same consulting firm that Kroger has been using to try to help them out. But I think probably led to some deterioration in their success over time. I think they thought maybe if we acquire every other department store in the land that will be enough. But it didn’t turn out to be.  I think industry-wise, probably health care is one of the least analytical industries that we have in the united States anyway, just because we don’t have good data.  Now, of course, the government is now encouraging through financial stimulus, basically every hospital and physician practice to get electronic medical records. That will create a vast amount of data, and then we’ll be able to analyze it and figure out which medical interventions work well and which ones don’t. But right now, it’s definitely an underachiever in terms of analytics.  I’d say retail, overall, has been an underachieving industry. In part because they just didn’t have the profit margins to devote to the analysts, paying the analysts to do that work. They relied in many cases, particularly in grocery, on the consumer products firms. But they’ve gotten a lot better. And companies like Walmart and JC Penney and so on have started to get a lot better at it. Target as well.  JULIE DEVOLL: So Tom, I’ll just conclude by asking, what do you see next in the field of analytics?  TOM DAVENPORT: One of the things we try to focus on in this book is creating a closer linkage between analytics and decision-making. Because after we wrote Competing on Analytics we found a number of analysts who would tell us, we did that marketing analysis, but nothing changed. We continued to make the same bad marketing decisions we’ve always made. Or the analytics were maybe telling us that some of these investments we made in financial services firms were not doing so well. Yet, we continue to go with those investments. So I think creating stronger ties between analytics and decision-making are a big thing from kind of a management standpoint.  From a technology standpoint, I think we’re going to see a whole new class of data emerging from sensors all over the place. Some people call them smart dust. They’re going to be so many sensors everywhere, they’re all going to generate data, and we have to analyze it to get some value. We’re going to see things like video analytics, so that we’ll know somebody in a retail environment. Are they actually looking at a product and deciding not to do it, or do they just not pay any attention at all?  For security purposes, we have all these video cameras now. We don’t have enough people to look at them all, so we’ll need analytics to deal with terrorism and intelligence issues and so on.  I think we need analytics for all the social media out there. To know, does it really make a difference if you tweet about something? Are you more likely to buy it? We have some very rough tools for sentiment analysis now, but hardly anybody even uses those. I think we’ll see a lot more analytics relative to the social media and social networks. So those are just a few of the things that are coming down the road I think.  Analytics help decision makers determine risk, weigh outcomes, and quantify costs and benefits associated with decisions. Learning Objective      Recognize the decision-making value of utilizing statistics and analytics to create accurate predictions  Key Points          Predictive and descriptive analytics are two methods of using data to inform and evaluate alternatives during decision making. They can also be used to explain performance outcomes.          encompass a variety of statistical techniques (such as modeling, machine learning, and data mining) that analyze current and historical facts to predictions about future events.         Descriptive analytics focus on developing new insights and understanding of business performance based on data and statistical methods; these analytics are then used to strategic decisions for the company.  Term      analytics      The use of skills, technologies, and practices to explore and investigate past performance, gain insight, and drive business decision making.          Analytics refer to the use of skills, technologies, and practices to explore and investigate past performance, gain insight, and drive business decision making. Predictive and descriptive analytics are two methods of using data and statistical methods to assess actual outcomes against target standards and s. These types of analysis can explain the relationship between factors that influence outcomes; they can also help prioritize improvement and other planning efforts. Companies can use their analytic capabilities to create advantages over competitors and better perform in the marketplace.           and Decision Making   encompass a variety of statistical techniques (such as modeling, machine learning, and data mining) that analyze current and historical facts to estimates about future events. Models capture relationships among many factors, allowing an assessment of risk or potential associated with a particular set of conditions. This helps to guide decision making for candidate transactions. Data mining draws on large numbers of records to patterns that can then be identified as opportunities or risks. For example, by analyzing grades for an entire class of first-year students, academic advisers can predict which students are most likely to struggle in the class.   help decision makers to predict the outcome(s) of a decision before it is implemented. Using these probabilities, decision makers can calculate the expected value of alternatives once risks and benefits are taken into account.  are particularly useful when there is a high degree of uncertainty. By carefully considering what is not known, decision makers can build confidence in the estimates that inform their choices. Forecasting consumer behavior in response to a new product or marketing initiative are examples of the use of . Descriptive Analytics and Decision Making  Descriptive analytics answer the questions, \"What happened and why did it happen?\" This approach seeks to understand past performances by using historical data to analyze the reasons behind past success or failure. Understanding cause and effect can help refine business and operational strategies. Most management reporting—such as sales, marketing, operations, and finance—uses this type of analysis. Descriptive analytics are used in quality management techniques and other methods of statistical process control. Analytics in the Modern Business World  Descriptive and  have increased ly in popularity due to advances in computing technology, techniques for data analysis, and mathematical modeling. Desktop tools can easily create reports and summaries of analytic results that help decision makers readily understand the findings and their implications. These tools create tables, charts, and graphs to present the data visually, which can help to clearly communicate the meaning of the data.   Data-driven decision management (DDDM) is an approach to business governance that values decisions that can be backed up with verifiable data. The success of the data-driven approach is reliant upon the quality of the data gathered and the effectiveness of its analysis and interpretation.   In the early days of computing, it usually took a specialist with a strong background in technology to mine data for information because it was necessary for that person to understand how databases and data warehouses worked. If a manager on the business side of an organization wanted to view data at a granular level, she had to reach out to the information technology department (IT) and request a report. Someone from the IT department would then create the report and schedule it to run on a periodic basis. Because the process was complex, ad hoc reports, also known as one-off reports, were discouraged.  Today, business intelligence tools often require very little, if any, support from the IT department. Business managers can customize dashboards to display the data they want to see and run custom reports on the fly. The changes in how data can be mined and visualized allows business executives who have no technology backgrounds to be able to work with analytics tools and make data-driven decisions.  Data-driven decision management is usually undertaken as a way to gain a competitive advantage. A study from the MIT Center for Digital Business found that organizations driven most by data-based decision making had 4% higher productivity rates and 6% higher profits. However, integrating massive amounts of information from different areas of the business and combining it to derive actionable data in real time can be easier said than done. Errors can creep into data analytics processes at any stage of the endeavor, and serious issues can result when they do.   Data-informed decision-making (DIDM) gives reference to the collection and analysis of data to guide decisions that improve success.[1] DIDM is used in education communities (where data is used with the  of helping students) but is also applicable to (and thus also used in) other fields in which data is used to inform decisions. While data based decision making is a more common term, data-informed decision-making is a preferable term since decisions should not be based solely on quantitative data.[1][2] Most educators have access to a data system for the purpose of analyzing student data.[3] These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system, making key package/display and content decisions) to improve the success of educators’ data-informed decision-making.[4] In Business, fostering and actively supporting DIDM in their firm and among their colleagues could be the  rôle of CIOs (Chief Information Officers) or CDOs (Chief Data Officers).[5]   Data-driven decision making (DDDM), applied to student achievement testing data, is a central focus of many school and district reform efforts, in part because of federal and state test-based accountability policies. This paper uses RAND research to show how schools and districts are analyzing achievement test results and other types of data to make decisions to improve student success. It examines DDDM policies and suggests future research in the field. A conceptual framework, adapted from the literature and used to organize the discussion, recognizes that multiple data types (input, outcome, process, and satisfaction data) can inform decisions, and that the presence of raw data does not ensure its effective use. Research questions addressed are: what types of data are administrators and teachers using, and how are they using them; what support is available to help with the use of the data; and what factors influence the use of data for decision making? RAND research suggests that most educators find data useful for informing aspects of their work and that they use data to improve teaching and learning. The first implication of this work is that DDDM does not guarantee effective decision making: having data does not mean that it will be used appropriately or lead to improvements. Second, practitioners and policymakers should promote the use of various data types collected at multiple points in time. Third, equal attention needs to be paid to analyzing data and taking action based on data. Capacity-building efforts may be needed to achieve this . Fourth, RAND research raises concerns about the consequences of high-stakes testing and excessive reliance on test data. Fifth, attaching stakes to data such as local progress tests can lead to the same negative practices that appear in high-stakes testing systems. Finally, policymakers seeking to promote educators’ data use should consider giving teachers flexibility to alter instruction based on data analyses. More research is needed on the effects of DDDM on instruction, student achievement, and other outcomes; how the focus on state test results affects the validity of those tests; and the quality of data being examined, the analyses educators are undertaking, and the decisions they are making. From customer service to social media to fintech to your inbox, predictive analytics are more and more driving what you see and when you see it. In fact, director of product management at Blue Yonder retail predictive applications provider Lars Trieloff has estimated that more than half of all apps have predictive components.  From our seat at the 2016  International Conference on Predictive APIs and Apps (PAPI) in Valencia Spain, we found some real-world applications of this out-of-this-world technology, taking it from Sci-Fi and academics to the board room. AI and Predictive Analytics  “If you don’t have examples, you don’t have machine learning,” said PAPIs chair and machine learning Ph.D. Louis Dorard. “Predictive analytics is about description, predictive, prescriptive, and finally automating decisions.”  Dorard offered an example: Look at the simple map below of blue and yellow dots. If you add in two more dots of unknown color, depending on where they fall in the groupings — based on the historical data of other blue and yellow dots — you can guess which one should be blue and which one should be yellow by which side of the invisible boundary you drew in the data.  predictive-analytics-example  You can make these predictions by satisfying three needs of predictive analytics:      Examples of inputs and outputs.     A sufficient amount of inputs and outputs.     Revision when similar inputs result in dissimilar outputs.  You can apply this dot logic to simplify more complex situations too, which can range from determining if an email is spam or ham to an autonomous car deciding who to hit in an extreme situation.  Predictive analytics is based on making generalizations and outputs based on certain inputs. Let’s continue to walk through some predictive analytics examples of how these inputs can bring outputs that affect your bottom line. Customer Service and Churn Detection  Pretty much every business has to worry about gaining and retaining new customers, which makes for one of the most compelling use cases to predictive analytics. Through the combination of customer data and revenue, a business can better score leads and also serve customers better. The input of product and price can be used to make smarter sales, and the input of customer data and product could make for better prices.  predictive-analytics-louis-dorard  But where predictive analytics can best help churn analysis is by identifying a churn indicator based on current and previous customers. Dorard explained the different levels of analytics as applied to churn:      Descriptive Analytics: Know churn rate against time.     Predictive Analytics: Know who is going to churn next.     Prescriptive Analytics: Decide what you should do about each customer that’s going to leave.  With prescriptive analytics, you start to apply strategies to reduce the possibility of churn — like switching customers to a different plan or providing them with more training on how to use your product or service.  However, you must factor the following variables into your machine learning results:      Customer presentation and context.     Churn prediction and action prediction     Uncertainty in predictions (or percentage of inaccuracy)     Revenue brought by customer and cost of actions     Constraints of frequency  From here you should be able to find the right level of accuracy and predictability to make sure that you are getting the desired results without missing clients or applying the wrong churn reduction strategy to the wrong customer. When done right, it should also let you prioritize who to woo back first, based on financial factors Dorard said. Filtering the Social Media Cacophony  There are countless use cases for predictive analytics in social media, but let’s stick with customer service for now. Amazon’s Alex Ingerman also spoke at the PAPIs conference in Valencia about his experience using the Amazon Machine Learning Public API, applying it to predictive customer service, specifically to finding important social media conversations via the Twitter API.  machine-learning-twitter-api  Why is listening to Twitter important to @awscloud and its more than half a million followers? Ingerman explained:      A customer could be reporting a possible service issue.     A customer is making a feature request — Ingerman says about 90 percent of Amazon Web Services product roadmap is driven by customers.     A customer is angry or unhappy.     A customer asks a very specific question.  So why would they need to use machine learning to automate this? “The social media stream is high-volume, and most of the messages—upwards of 80 to 90 percent of the @awscloud handle—are not customer service actionable.” Ingerman said. Most are not useful because that’s because they are:      jokes     self-promotional     a public conversation     thank you’s  That’s why Amazon built an end-to-end application to filter out the important customer service calls to action and reroute them from the social media team to the proper support departments.  Below are the steps Amazon took to use the Amazon platform to filter out these conversations. Note, while Amazon went through this process and have the full code sample publicly available on Github, Ingerman said this isn’t exactly how Amazon is filtering its social media — he couldn’t divulge the specifics — but it still serves as a good example of how to go about solving a business problem through predictive analytics and machine learning.      Formulate the business problem. Instantly find new tweets mentioning @awscloud, ingest and analyze each one to predict whether a CS agent should act on it     Establish the data flow.     Pick the machine learning strategy.     Get the data. Retrieve past tweets, which he says it’s easy to get the data, well-structured and clean.     Label the data. They labeled past tweets and discovered patterns connecting data points and labels. Ingerman suggests that you first try labeling by hand. “It’s fun for the first few dozen. Once you get past a hundred or so, it becomes a real problem,” he said. Amazon offers a marketplace for human intelligence so you can outsource this tedious machine learning step.     Align machine learning model with business requirements. Specifically, look out for a false positive, which tags as “actionable” but isn’t which could potentially waste a small amount of customer support time, and, more importantly, for a false negative, which is marked as “not actionable” but is which could have you risk losing a customer. In this situation, they set a threshold of how “certain” a model must be, accepting more false positives at the expense of fewer false negatives.  Ingerman says that this same strategy could be applied to Facebook, email, customer reviews and even scanned faxes. The only part of the predictive analytics flow shown below that changes is the model.  In the future, a company would try to then link these predictions to automatic results, like a knowledge base that can tweet responses to commonly asked questions. But where does the human fit in all this?  It’s not called machine autonomous learning—at least yet! This is because predictive analytics needs to be a part of a business strategy. Try using a Machine Learning Canvas before even starting to implement anything as a way to make sure you’re targeting the right problem, using the right technology and, as often is the case, making sure team members of different backgrounds aren’t misunderstanding each other.  machine-learning-canvas  Finally, Dorard reminds us: “Garbage in, garbage out.” Models are only as good as the data that’s going into them, so, no matter how much machine-based automation is happening, you need a good couple humans to check that everything’s ok.", "category": "Edison", "id": 122}
{"skillName": "DSDK01", "skillText": "A business process or business method is a collection of related, structured activities or tasks that produce a specific service or product (serve a particular goal) for a particular customer or customers. It may often be visualized as a flowchart of a sequence of activities with interleaving decision points or as a Process Matrix of a sequence of activities with relevance rules based on data in the process.  Contents      1 Overview     2 History         2.1 Adam Smith         2.2 Frederick Winslow Taylor         2.3 Peter Drucker         2.4 Other definitions     3 Related Concepts         3.1 Workflow         3.2 Business Process Re-engineering         3.3 Business Process Management (BPM)         3.4 Knowledge Management         3.5 Total Quality Management         3.6 Information Technology as an Enabler for Business Process Management     4 Importance of the process chain     5 Policies, processes and procedures     6 Manual / administrative vs. computer system-based internal controls     7 Information reports as an essential base for execution     8 Supporting theories and concepts         8.1 Span of control         8.2 Information management concepts     9 See also     10 References     11 Further reading  Overview  There are three types of business processes[citation needed]:      Management processes, the processes that govern the operation of a system. Typical management processes include \"corporate governance\" and \"strategic management\".     Operational processes, processes that constitute the core business and create the primary value stream. For example, taking orders from customers, and opening an account in a bank branch.     Supporting processes, which support the core processes. Examples include Health & Safety, accounting, recruitment, call center, technical support.  A business process begins with a mission objective and ends with achievement of the business objective. Process-oriented organizations break down the barriers of structural departments and try to avoid functional silos.  A complex business process may be decomposed into several sub-processes,[1] which have their own attributes, but also contribute to achieving the goal of the super-process. The analysis of business processes typically includes the mapping of processes and sub-processes down to activity/task level.[2][3]  Business processes are designed[4] to add value for the customer and should not include unnecessary activities. The outcome of a well designed business process is increased effectiveness (value for the customer) and increased efficiency (less use of resources).  Business Processes can be modeled through a large number of methods and techniques. For instance, the Business Process Modeling Notation is a Business Process Modeling technique that can be used for drawing business processes in a workflow. History Adam Smith  An important early (1776) description of processes was that of economist Adam Smith in his famous example of a pin factory. Inspired by an article in Diderot's Encyclopédie, Smith described the production of a pin in the following way:      ”One man draws out the wire, another straights it, a third cuts it, a fourth points it, a fifth grinds it at the top for receiving the head: to make the head requires two or three distinct operations: to put it on is a particular business, to whiten the pins is another ... and the important business of making a pin is, in this manner, divided into about eighteen distinct operations, which in some manufactories are all performed by distinct hands, though in others the same man will sometime perform two or three of them.”  Smith also first recognized how the output could be increased through the use of labor division. Previously, in a society where production was dominated by handcrafted goods, one man would perform all the activities required during the production process, while Smith described how the work was divided into a set of simple tasks, which would be performed by specialized workers. The result of labor division in Smith’s example resulted in productivity increasing by 24,000 percent (sic), i.e. that the same number of workers made 240 times as many pins as they had been producing before the introduction of labor division.  It is worth noting that Smith did not advocate labor division at any price and per se. The appropriate level of task division was defined through experimental design of the production process. In contrast to Smith's view which was limited to the same functional domain and comprised activities that are in direct sequence in the manufacturing process, today's process concept includes cross-functionality as an important characteristic. Following his ideas the division of labor was adopted widely, while the integration of tasks into a functional, or cross-functional, process was not considered as an alternative option until much later. Frederick Winslow Taylor  American engineer, Frederick Winslow Taylor greatly influenced and improved the quality of industrial processes in the early twentieth century. His Principles of Scientific Management focused on standardization of processes, systematic training and clearly defining the roles of management and employees. His methods were widely adopted in the United States, Russia and parts of Europe and led to further developments such as “time and motion study” and visual task optimization techniques, such as Gantt charts. Peter Drucker  In the latter part of the twentieth century, management guru Peter Drucker focused much of his work on simplification and decentralization of processes, which led to the concept of outsourcing. Other definitions  In the early 1990s, US corporations, and subsequently companies all over the world, started to adopt the concept of business process reengineering (BPR) in an attempt to re-achieve the competitiveness that they had lost during the previous decade.  Davenport (1993)[5] defines a (business) process as:      ”a structured, measured set of activities designed to produce a specific output for a particular customer or market. It implies a strong emphasis on how work is done within an organization, in contrast to a product focus’s emphasis on what. A process is thus a specific ordering of work activities across time and space, with a beginning and an end, and clearly defined inputs and outputs: a structure for action. ... Taking a process approach implies adopting the customer’s point of view. Processes are the structure by which an organization does what is necessary to produce value for its customers.”  This definition contains certain characteristics a process must possess. These characteristics are achieved by a focus on the business logic of the process (how work is done), instead of taking a product perspective (what is done). Following Davenport's definition of a process we can conclude that a process must have clearly defined boundaries, input and output, that it consists of smaller parts, activities, which are ordered in time and space, that there must be a receiver of the process outcome- a customer - and that the transformation taking place within the process must add customer value.  Hammer & Champy’s (1993)[6] definition can be considered as a subset of Davenport’s. They define a process as:      ”a collection of activities that takes one or more kinds of input and creates an output that is of value to the customer.”  As we can note, Hammer & Champy have a more transformation oriented perception, and put less emphasis on the structural component – process boundaries and the order of activities in time and space.  Rummler & Brache (1995)[7] use a definition that clearly encompasses a focus on the organization’s external customers, when stating that      ”a business process is a series of steps designed to produce a product or service. Most processes (...) are cross-functional, spanning the ‘white space’ between the boxes on the organization chart. Some processes result in a product or service that is received by an organization's external customer. We call these primary processes. Other processes produce products that are invisible to the external customer but essential to the effective management of the business. We call these support processes.”  The above definition distinguishes two types of processes, primary and support processes, depending on whether a process is directly involved in the creation of customer value, or concerned with the organization’s internal activities. In this sense, Rummler and Brache's definition follows Porter's value chain model, which also builds on a division of primary and secondary activities. According to Rummler and Brache, a typical characteristic of a successful process-based organization is the absence of secondary activities in the primary value flow that is created in the customer oriented primary processes. The characteristic of processes as spanning the white space on the organization chart indicates that processes are embedded in some form of organizational structure. Also, a process can be cross-functional, i.e. it ranges over several business functions.  Johansson et al. (1993).[8] define a process as:      ”a set of linked activities that take an input and transform it to create an output. Ideally, the transformation that occurs in the process should add value to the input and create an output that is more useful and effective to the recipient either upstream or downstream.”  This definition also emphasizes the constitution of links between activities and the transformation that takes place within the process. Johansson et al. also include the upstream part of the value chain as a possible recipient of the process output. Summarizing the four definitions above, we can compile the following list of characteristics for a business process:      Definability : It must have clearly defined boundaries, input and output.     Order : It must consist of activities that are ordered according to their position in time and space (a sequence).     Customer : There must be a recipient of the process' outcome, a customer.     Value-adding : The transformation taking place within the process must add value to the recipient, either upstream or downstream.     Embeddedness : A process cannot exist in itself, it must be embedded in an organizational structure.     Cross-functionality : A process regularly can, but not necessarily must, span several functions.  Frequently, identifying a process owner, (i.e., the person responsible for the continuous improvement of the process) is considered as a prerequisite. Sometimes the process owner is the same person who is performing the process. Related Concepts Workflow  Workflow is the movement of information or material from one activity or worksite to another. Workflow includes the procedures, people and tools involved in each step of a business process. A single workflow may either be sequential, with each step contingent upon completion of the previous one, or parallel, with multiple steps occurring simultaneously. Multiple combinations of single workflows may be connected to achieve a resulting overall process. Business Process Re-engineering  Business Process Re-engineering (BPR) was originally conceptualized by Hammer and Davenport as a means to improve organizational effectiveness and productivity. It consisted of starting from a blank slate and completely recreating major business processes as well as the use of information technology for significant performance improvement. The term unfortunately, became associated with corporate “downsizing” in the mid-1990s. Business Process Management (BPM)  Business Process Management also termed as BPM covers how we study, identify, change and monitor business processes to ensure they run smoothly and can be improved over time. It is a continuous evaluation of existing processes and identification of ways to improve upon it, resulting in a cycle of overall organizational improvement.[9] Knowledge Management  Knowledge Management is the definition of the knowledge that employees and systems use to perform their functions and maintaining it in a format that can be accessed by others. The Gartner Group definition states that \"Knowledge management is a discipline that promotes an integrated approach to identifying, capturing, evaluating, retrieving, and sharing all of an enterprise's information assets. These assets may include databases, documents, policies, procedures, and previously un-captured expertise and experience in individual workers.\" Total Quality Management  Total Quality Management (TQM) emerged in the early 1980s as organizations sought to improve the quality of their products and services. It was followed by the Six Sigma methodology in the mid-1980s, first introduced by Motorola. Six Sigma consists of statistical methods to improve business processes and thus reduce defects in outputs. The \"lean approach\" to quality management was introduced by the Toyota Motor Company in the 1990s and focused on customer needs and reduction of wastage. Information Technology as an Enabler for Business Process Management  Advances in information technology over the years, have changed business processes within and between business enterprises. In the 1960s, operating systems had limited functionality and any workflow management systems that were in use, were tailor made for the specific organization.  The 1970s-1980s saw the development of data-driven approaches, as data storage and retrieval technologies improved. Data modeling rather than process modeling was the starting point for building an information system. Business processes had to adapt to information technology because process modeling was neglected.  The shift towards process oriented management occurred in the 1990s. Enterprise resource planning software with workflow management components such as SAP, Baan, PeopleSoft, Oracle and JD Edwards emerged.  The world of e-business created a need to automate business processes across organizations, which in turn raised the need for standardized protocols and web services composition languages that can be understood across the industry. The Business Process Modeling Notation (BPMN) and Business Motivation Model (BMM) are widely used standards for business modeling. The Business Modeling and Integration Domain Task Force (BMI DTF) is a consortium of vendors and user companies that continues to work together to develop standards and specifications to promote collaboration and integration of people, systems, processes and information within and across enterprises.  The most recent trends in BPM are influenced by emergence of cloud technology, the prevalence of social media, mobile technology and development of analytical techniques. Cloud based technologies allow companies to purchase resources quickly and as required independent of their location. Social media, websites and smart phones are the newest channels through which organizations reach and support their customers. The abundance of customer data collected through these channels as well as through call center interactions, emails, voice calls, and customer surveys has led to a huge growth in data analytics which in turn is utilized for performance management and improving the ways in which the company services its customers. Importance of the process chain  Business processes comprise a set of sequential sub-processes or tasks with alternative paths, depending on certain conditions as applicable, performed to achieve a given objective or produce given outputs. Each process has one or more needed inputs. The inputs and outputs may be received from, or sent to other business processes, other organizational units, or internal or external stakeholders.  Business processes are designed to be operated by one or more business functional units, and emphasize the importance of the “process chain” rather than the individual units.  In general, the various tasks of a business process can be performed in one of two ways      manually and     by means of business data processing systems such as ERP systems.  Typically, some process tasks will be manual, while some will be computer-based, and these tasks may be sequenced in many ways. In other words, the data and information that are being handled through the process may pass through manual or computer tasks in any given order. Policies, processes and procedures  The above improvement areas are equally applicable to policies, processes and detailed procedures (sub-processes/tasks). There is a cascading effect of improvements made at a higher level on those made at a lower level.  For instance, if a recommendation to replace a given policy with a better one is made with proper justification and accepted in principle by business process owners, then corresponding changes in the consequent processes and procedures will follow naturally in order to enable implementation of the policies Manual / administrative vs. computer system-based internal controls  Internal controls can be built into manual / administrative process steps and / or computer system procedures.  It is advisable to build in as many system controls as possible, since these controls, being automatic, will always be exercised since they are built into the design of the business system software. For instance, an error message preventing an entry of a received raw material quantity exceeding the purchase order quantity by greater than the permissible tolerance percentage will always be displayed and will prevent the system user from entering such a quantity.  However, for various reasons such as practicality, the need to be “flexible” (whatever that may signify), lack of business domain knowledge and experience, difficulties in designing/writing software, cost of software development/modification, the incapability of a computerised system to provide controls, etc., all internal controls otherwise considered to be necessary are often not built into business systems and software.  In such a scenario, the manual, administrative process controls outside the computer system should be clearly documented, enforced and regularly exercised. For instance, while entering data to create a new record in a material system database’s item master table, the only internal control that the system can provide over the item description field is not to allow the user to leave the description blank – in other words, configure item description as a mandatory field. The system obviously cannot alert the user that the description is wrongly spelled, inappropriate, nonsensical, etc.  In the absence of such a system-based internal control, the item creation process must include a suitable administrative control through the detailed checking, by a responsible officer, of all fields entered for the new item, by comparing a print-out taken from the system with the item data entry sheet, and ensuring that any corrections in the item description (and other similar fields where no system control is possible) are promptly carried out.  Last but not least, the introduction of effective manual, administrative controls usually requires an overriding periodic check by a higher authority to ensure that such controls are exercised in the first place. Information reports as an essential base for execution  Business processes must include up-to-date and accurate reports to ensure effective action. An example of this is the availability of purchase order status reports for supplier delivery follow-up as described in the section on effectiveness above. There are numerous examples of this in every possible business process.  Another example from production is the process of analysis of line rejections occurring on the shop floor. This process should include systematic periodical analysis of rejections by reason, and present the results in a suitable information report that pinpoints the major reasons, and trends in these reasons, for management to take corrective actions to control rejections and keep them within acceptable limits. Such a process of analysis and summarisation of line rejection events is clearly superior to a process which merely inquires into each individual rejection as it occurs.  Business process owners and operatives should realise that process improvement often occurs with introduction of appropriate transaction, operational, highlight, exception or M.I.S. reports, provided these are consciously used for day-to-day or periodical decision-making. With this understanding would hopefully come the willingness to invest time and other resources in business process improvement by introduction of useful and relevant reporting systems. Supporting theories and concepts Span of control  The span of control is the number of subordinates a supervisor manages within a structural organization. Introducing a business process concept has a considerable impact on the structural elements of the organization and thus also on the span of control.  Large organizations that are not organized as markets need to be organized in smaller units - departments - which can be defined according to different principles. Information management concepts  Information Management and the organization design strategies being related to it, are a theoretical cornerstone of the business process concept. See also      Business analysis     Business Process Automation     Business Process Definition Metamodel     Business process improvement     Business process management     Business process mapping     Business process outsourcing     Information management  A business process is a collection of linked tasks which find their end in the delivery of a service or product to a client.   A business process has also been defined as a set of activities and tasks that, once completed, will accomplish an organizational goal.  The process must involve clearly defined inputs and a single output. These inputs are made up of all of the factors which contribute (either directly or indirectly) to the added value of a service or product. These factors can be categorized into management processes, operational processes and supporting processes.  Management processes govern the operation of a particular organization’s system of operation. Operational processes constitute the core business. Supporting processes such as human resources and accounting are put in place to support the core processes.  The definition of the term business process and the development of this definition since its conception by Adam Smith in 1776 has lead to such areas of study as Operations Development, Operations Management and to the development of various Business Management Systems.  These systems, in turn, have created an industry for BPM Software which seeks to automate process management by connecting various process actors via technology.  A process requires a series of actions to achieve a certain objective. BPM processes are continuous but also allow for adhoc action. Processes can be simple or complex based on number of steps, number of systems involved etc. They can be short or long running. Longer processes tend to have multiple dependencies and a greater documentation requirement.", "category": "Edison", "id": 123}
{"skillName": "DSDK04", "skillText": "Analytical skill is the ability to visualize, articulate, conceptualize or solve both complex and uncomplicated problems by making decisions that are sensible given the available information. Such skills include demonstration of the ability to apply logical thinking to breaking complex problems into their component parts[clarification needed].  In 1999, Richards J. Heuer Jr., explained that: \"Thinking analytically is a skill like carpentry or driving a car. It can be taught, it can be learned, and it can improve with practice. But like many other skills, such as riding a bike, it is not learned by sitting in a classroom and being told how to do it. Analysts learn by doing.\"[1]  To test for analytical skills one might be asked to look for inconsistencies in an advertisement, put a series of events in the proper order, or critically read an essay[citation needed]. Usually standardized tests and interviews include an analytical section that requires the examiner to use their logic to pick apart a problem and come up with a solution.  Although there is no question that analytical skills are essential, other skills are equally required[clarification needed]. For instance in systems analysis the systems analyst should focus on four sets of analytical skills:      systems thinking,     organizational knowledge,     problem identification, and     problem analyzing and solving.  MAke The MosT of Your AnAl YTicAl T AlenT There are many implementations of analytic CoEs out there. However, a large  percentage of them can be described as specialized shared-service organizations.  These organizations receive requests from the business community to apply analytics  to solve problems.  There is no question that these types of structures provide value to the organization,  but they may not be able to change the internal culture without having a much  closer connection and integration with the various business units where many better  decisions could be enabled. When used in an ad hoc way and without the right level  of executive sponsorship, these shared-service organizations are limited in their  ability to have more lasting effects on the way decisions are made and the quality of  those decisions.   Many of these shared-service organizations are set up to deal with requirements from  a project perspective. Each project has a scope, deliverable and start and end dates.  Because of this setup, it may be difficult to identify the root cause of data issues  such as data consistency, availability of master data, data quality and integration.  Consequently, those working on a project may not be able to make changes  to address the root cause of problems. Many data extraction and manipulation  tasks tend to be repeated. Another challenge can be where people report within  organizational structures. Continuous executive support and high internal visibility are  required to empower any analytic CoE to put the results of analytics to work and to  implement changes to facilitate future and more advanced applications of analytics. More Strategic and Effective Implementation of an  Analytic CoE The best implementations of analytic CoEs have these traits: •  Partnering with business stakeholders for ongoing success. •  High-level executive sponsorship. •  Sufficient prominence in the organizational hierarchy to have visibility and impact. •  A reputation for proven results, excellent work ethic and ability to deploy results  that affect decision making. Such strategic implementations will have the highest chance of promoting  widespread analytically driven decisions and surfacing new opportunities. All CoEs share many common components and characteristics.  The following diagram illustrates the key possible focus areas and functions for  analytic centers of excellence. It is important to point out that not all of these  functions need to be managed by the analytic CoE. However, addressing these  functions is critical to ensure that analytics is used in an efficient, repeatable and  effective manner within the organization. 4 14 MAke The MosT of Your AnAl YTicAl T AlenT The internal culture should effectively and continuously address the following key  areas: •  Communication of the organization’s business priorities and objectives. •  Alignments of internal groups, resources and information management and  analytical efforts around the changing organizational priorities and direction. •  Providing the incentives and mandate to foster collaboration and information  sharing between organization units on a consistent basis. •  Demonstrating the value of analytics and information in general as a corporate  asset. •  Rewarding individuals and groups for demonstrating effective applications and use  of analytics and information in the decision-making process. •  Facilitating acceptance of change by business units and resources. The analytic CoE will play a significant role in working very closely with the business  representatives to guide and facilitate the adoption of these key requirements. Organizational Benefits  A properly implemented analytic center of excellence will provide your organization  with the following key benefits: •  An effective way to use existing analytical skills. •  A structure and set of processes to develop and promote best practices. •  Support for ad hoc analytical requirements, but more importantly, the  establishment of the framework and foundation to implement repeatable and  more strategic analyses in an efficient way. •  A much-needed link and alignment between the business, analytical efforts and  the organization’s technology infrastructure. Alignment between the business,  analytical resources and IT is a critical requirement to enable effective use of  analytics. Analysts will be able to use their skills to apply effective analytics to  support decision making and organizational objectives. •  Support for analytical resources by allowing them to focus more on analytical  issues and less on data management and data quality tasks. •  The establishment of a solid, consistent communication and alignment between  the business community, especially the decision makers and the analytical  resources who conduct the analysis. This is the foundation needed to change the  internal culture to one that understands the value of analytics and uses a fact- based decision-making approach. MAke The MosT of Your AnAl YTicAl T AlenT 15 Approaches for Establishing an Analytic CoE The objective of analytic centers of excellence is focused on maximizing the  organizational benefit from the investment in data and analytics. The discussions  presented by this paper have highlighted the four critical organizational dimensions  that form the foundations necessary to gain the highest possible value from the  investment. The maturity, interaction and alignment of these four dimensions provide  essential requirements to identify the form, structure, vision and mandate of an  effective CoE in each organization. Analytic centers of excellence, and any other type  of centers of excellence, are not one-size-fits-all. Although there are common key  characteristics in all centers of excellence, the most effective structure and type will  have to be based on the nature and maturity of the four organizational dimensions  in each unique organization. The common characteristics of all centers of excellence  have been presented earlier in this paper. The following key areas must be considered as organizations think about this  valuable concept and about the approach to establishing an analytic CoE for their  own environment: •  Degree of centralization versus decentralization . Many organizational aspects  need to be considered to determine the right approach. Some of these factors  include the size of the organization, level of reach (global vs. local), the structure  of the business units and technical and IT resources, the existing culture and level  of collaboration between groups, the distribution of analytical resources, etc. •  Executive support . The support from an executive level is essential for  empowering the analytic CoE to produce accurate, repeatable and timely results  from applying analytics. Collaboration and discussion between groups are  necessary in many cases to collect the requirements and data to apply analytics  and, more importantly, to use the results in the decision-making process.  Many successful applications of analytics are promoted by the support of an  executive in a business unit that has a clear vision and need to use analytics.  Other implementations can have a broader scope to cover multiple business  units or even the entire enterprise. The common requirement is to have high-level  management support. •  Analytical skills . The availability of analytical resources, skill levels and  responsibilities are other critical factors. These analytical resources clearly are  needed to move forward. However, there are options for organizations to explore  if these resources are not available, if they are too few or if additional training is  needed. Some of these options include outsourcing some or all the analytical  work initially, or working with SAS and other partners in a collaborative way to  provide the initial resources while acquiring new resources or training existing staff  members. 16 MAke The MosT of Your AnAl YTicAl T AlenT •  Current data structure and quality . The availability of required data, the level of  data quality and the amount of data management that may be necessary to apply  analytics are also key factors. To effectively address these areas, other technical  resources and activities may be needed to facilitate and accelerate the use of  analytics. Some of these needs may be addressed by existing resources. There  are also options that include securing external resources to make progress and  tap into industry best practices. •  Level and scope of the required analytical work . There are many types  of analytic applications that can support the decision-making process and  strategy of organizations. Some of these analytical applications may be easier  to perform than others. Some may require a high level of processing power to  complete in a timely manner or specialized analytical skills. Another factor is how  aggressively the organization is planning to use analytics and its time frame.  Again, organizations may find it more cost-effective and faster to outsource some  of these types of analytics. SAS offers different types of outsourcing services that  can meet some of the most demanding analytical skills effectively.  The above list is not comprehensive, but it covers the essential areas. An assessment  of each one of these areas is recommended to identify an approach that balances the  requirements and characteristics of each of these areas. SAS offers several types of  services to assess the current environment of each organization and to develop the  most effective approach to promote analytics and assist organizations in establishing  their own analytic CoE.  The following approaches can be considered as organizations explore options and  assess the characteristics of their own environments: •  Outsourcing analytical projects . This may be the appropriate approach to  meet the need for a particular business unit, the entire organization or as an initial  phase to train existing staff – or an occasional way to tap into additional analytic  bandwidth and more effectively balance the workload. •  Outsourcing specific tasks . Depending on the requirements and the available  resources, certain analytical or even data management functions and steps may  be outsourced. In some cases, organizations may be outsourcing some of these  tasks. This process can be an initial phase, or it can be used on demand. •  Centralizing some or all functions . In some cases, centralizing some of  the analytical work or the data management work may be an effective way  to optimally use available resources. The IT team, or other members of the  organization, may be in the best position to take on the responsibilities of some  key functions. Existing information governance and policies may also be used  when appropriate to determine the best approach. MAke The MosT of Your AnAl YTicAl T AlenT 17 •  Decentralizing . Depending on the need, the executive support and the readiness  of internal groups, decentralizing specific analytical tasks may be the best  approach to ensure that analytical work is closely aligned with the business  requirements. This will require the availability and alignment of sufficient analytical  resources in business units. From a best-practice standpoint, some information  management tasks may still need to be centralize Big data analytics is a trending practice that many companies are adopting. Before jumping in and buying big data tools, though, organizations should first get to know the landscape. The analytics process, including the deployment and use of big data analytics tools, can help companies improve operational efficiency, drive new revenue and gain competitive advantages over business rivals. But there are different types of analytics applications to consider. For example, descriptive analytics focuses on describing something that has already happened, as well as suggesting its root causes. Descriptive analytics, which remains the lion's share of the analysis performed, typically hinges on basic querying, reporting and visualization of historical data.  Alternatively, more complex predictive and prescriptive modeling can help companies anticipate business opportunities and make decisions that affect profits in areas such as targeting marketing campaigns, reducing customer churn and avoiding equipment failures. With predictive analytics, historical data sets are mined for patterns indicative of future situations and behaviors, while prescriptive analytics subsumes the results of predictive analytics to suggest actions that will best take advantage of the predicted scenarios.  In many environments, the processing and data storage demands of advanced analytics applications have limited their adoption -- but those barriers are beginning to fall. The growing availability of big data platforms and big data analytics tools has enabled environments in which predictive and prescriptive analytics applications can scale to handle massive data volumes originating from a wide variety of sources. What does big data analytics mean?  In essence, big data analytics tools are software products that support predictive and prescriptive analytics applications running on big data computing platforms -- typically, parallel processing systems based on clusters of commodity servers, scalable distributed storage and technologies such as Hadoop and NoSQL databases. The tools are designed to enable users to rapidly analyze large amounts of data, often within a real-time window.  In addition, big data analytics tools provide the framework for using data mining techniques to analyze data, discover patterns, propose analytical models to recognize and react to identified patterns, and then enhance the performance of business processes by embedding the analytical models within the corresponding operational applications. For example, massive amounts of shipping delivery data, streaming traffic data, streaming weather data and historical vendor performance data can be analyzed to devise a model for optimal selection of shipping subcontractors within geographic regions to limit the risks of late delivery or damaged goods. Editor's note  This article is part of a series on big data analytics tools. This first article looks at the benefits and uses of big data analytics tools in the enterprise; the second article examines the different use cases for big data analytics tools; part three offers insight into procuring and deploying big data analytics tools; and the last article examines the top big data analytics in the market.  Big data analytics tools can ingest a wide variety of data types: structured data with defined and consistent fields, such as transaction data stored in relational databases; semi-structured data, such as Web server or mobile application log files; and unstructured data, encompassing things like text files, documents, emails, text messages and social media posts. Powering analytics: Inside big data and advanced analytics tools  A Google search for big data analytics yields a long list of vendors. However, many of these vendors provide big data platforms and tools that support the analytics process -- for example, data integration, data preparation and other types of data management software. We focus on tools that meet the following criteria:      They provide the analyst with advanced analytics algorithms and models.     They're engineered to run on big data platforms such as Hadoop or specialty high-performance analytics systems.     They're easily adaptable to use structured and unstructured data from multiple sources.     Their performance is capable of scaling as more data is incorporated into analytical models.     Their analytical models can be or already are integrated with data visualization and presentation tools.     They can easily be integrated with other technologies.  In addition, the tools must incorporate essential characteristics and include integrated algorithms and methods supporting the typical suite of data mining techniques, including (but not limited to):      Clustering and segmentation, which divides a large collection of entities into smaller groups that exhibit some (potentially unanticipated) similarities. An example is analyzing a collection of customers to differentiate smaller segments for targeted marketing.     Classification, which is a process of organizing data into predefined classes based on attributes that are either pre-selected by an analyst or identified as a result of a clustering model. An example is using the segmentation model to determine into which segment a new customer would be categorized.     Regression, which is used to discover relationships among a dependent variable and one or more independent variables, and helps determine how the dependent variable's values change in relation to the independent variable values. An example is using geographic location, mean income, average summer temperature and square footage to predict the future value of a property.     Association and item set mining, which looks for statistically relevant relationships among variables in a large data set. For example, this could help direct call-center representatives to offer specific incentives based on the caller's customer segment, duration of relationship and type of complaint.     Similarity and correlation, which is used to inform undirected clustering algorithms. Similarity-scoring algorithms can be used to determine the similarity of entities placed in a candidate cluster.     Neural networks, which are used in undirected analysis for machine learning based on adaptive weighting and approximation.  This is just a subset of the types of analyses used for predictive and prescriptive analytics. In addition, different vendors are likely to provide a variety of algorithms supporting each of the different methods. The advanced analytics market  The market for advanced analytics tools has evolved over time, and the types of tools that are available vary in degree of maturity and, consequently, in capability and ease of use. For example, there are tools with relatively long histories from some mega-vendors like IBM, Oracle and SAS. Other large vendors have acquired companies whose tools have a more recent history, such as those provided by Microsoft, Dell, Teradata and SAP.  A number of smaller companies provide big data analytics products, including Angoss, Predixion, Alteryx, Alpine Data Labs, Pentaho, KNIME and RapidMiner. In some cases, companies have developed their own suite of algorithms. Others have adapted the open source statistical R language and provide predictive and prescriptive modeling capabilities using R's features, or use the software from the open source Weka project.  A third category of products are those available as open source technologies. Examples include the previously mentioned R language, the Mahout software distribution that's part of the Hadoop stack, and Weka.  In some of these cases (particularly with the mega-vendors), the big data analytics tools are incorporated into larger big data enterprise suites. In others, the tools are sold as standalone products. In the latter case, it's the customer's job to integrate with the big data platform being deployed. Most of the tools provide a visual interface to guide the analytics processes (data mining/discovery analysis, evaluation and scoring of models, integration with operational environments), and in most cases, the vendors provide guidance and services to get the customer up and running. Who uses big data and advanced analytics tools?  While some individuals in the organization are looking to explore and devise new predictive models, others look to embed these models within their business processes, and still others will want to understand the overall impact that these tools will have on the business. In other words, organizations that are adopting big data analytics need to accommodate a variety of user types, such as:      The data scientist, who likely performs more complex analyses involving more complex data types and is familiar with how underlying models are designed and implemented to assess inherent dependencies or biases.     The business analyst, who is likely a more casual user looking to use the tools for proactive data discovery or visualization of existing information, as well as some predictive analytics.     The business manager, who is looking to understand the models and conclusions.     IT developers, who support all the prior categories of users.  All of these roles would typically work together in the model development lifecycle. The data scientist subjects a swath of big data sets to the undirected analyses provided, and looks for any patterns that would be of business interest. After engaging the business analyst to review how the models work and evaluate how each of those discovered models or patterns could potentially positively affect the business, the business manager and IT teams are brought in to embed or integrate the models into business processes or devise new processes around the models.  From a market perspective, though, it's interesting to consider the types of businesses that are embracing big data analytics. Many of the early users of big data technologies were Internet companies (e.g., Google, Yahoo, Facebook, LinkedIn and Netflix) or analytics services providers. Each of these companies relied on operational and analytical applications requiring fast-flowing streams of data to ingest, process, analyze, and then feed the results back to continuously improve performance.  As appetites for data expand among companies in more mainstream industries, big data analytics has found a place in a more general corporate population. In the past, the cost factors for a large-scale analytics platform would have limited the adoption to only the very largest businesses. However, the availability of utility-style hosted big data platforms (such as those available via Amazon Web Services) and the ability to instantiate big data platforms such as Hadoop on-premises without a large investment have reduced the barrier to entry. In addition, open data sets and accessibility to fire hose data feeds from social media channels provide the raw material for larger-scale data analyses when blended with internal data sets.  Larger businesses may still opt for high-end big data analytics tools, but lower-cost alternatives deployed on cost-effective platforms enable small and medium-size businesses to evaluate and launch big data analytics programs and achieve the desired business improvement results.  Now that we've examined the different types of tools and their uses, the next step is to determine how these tools could benefit your company. By taking a look at the various use cases for big data analytics, you will begin to see where a general big data analytics capability can be leveraged for creating and enhancing value.", "category": "Edison", "id": 124}
{"skillName": "DSDK05", "skillText": "Customer analytics, also called customer data analytics, is the systematic examination of a company's customer information to identify, attract and retain the most profitable customers.  customer analytics (customer data analytics)  Sponsored News      Three Use Cases for Interactive Data Discovery and Predictive Analytics     –SAS Institute Inc.     4 Ways to Leverage Information Governance for Improved Outcomes     –Symantec     See More  Vendor Resources      Big Data BlackOut: Are Utilities Powering Up Their Data Analytics?     –ComputerWeekly.com     MIT Sloan Management Review: The Analytics Talent Dividend     –SAS  Customer analytics, also called customer data analytics, is the systematic examination of a company's customer information to identify, attract and retain the most profitable customers.  Download this free guide Download our Guide: Create an Analytics Success Story  Learn how to gain executive approval and drive operational, cultural changes within your organization.  Customer analytics is often managed by an interdisciplinary group made up of business owners from different departments within the company, including marketing, sales, customer service, IT and business analysts. In order to be effective, the group must first agree upon which business metrics they need to achieve a single view of the customer experience. Multiple instances of customer relationship management (CRM) applications, disparate enterprise resource planning (ERP) systems and poor customer data integration (CDI) can leave group members with a fragmented view of the customer.   The goal of customer analytics is to create a single, accurate view of the customer for the group to work with and make decisions about how best to acquire and retain customers, identify high-value customers and proactively interact with them.  Failure to have enough accurate data can make any insight derived from analysis wildly inaccurate. Customer analytics is a process by which data from customer behavior is used to help make key business decisions via market segmentation and predictive analytics. This information is used by businesses for direct marketing, site selection, and customer relationship management. Marketing provides services in order to satisfy customers. With that in mind, the productive system is considered from its beginning at the production level, to the end of the cycle at the consumer. Customer analytics plays a very important role in the prediction of customer behavior today.[1]  Contents      1 Uses     2 Predicting customer behaviour     3 Data mining     4 See also     5 References     6 Further readling     7 External links  Uses  Retail  Although until recently over 90% of retailers had limited visibility on their customers[citation needed], with increasing investments in loyalty programs, customer tracking solutions and market research, this industry started increasing use of customer analytics in decisions ranging from product, promotion, price and distribution management.[citation needed] The most obvious use of customer analytics in retail today is the development of personalized communications and offers and/or different marketing programs by segment.[citation needed] Additional reasons set forth by Bain & Co. include: prioritizing product development efforts, designing distribution strategies and determining product pricing.[2] Demographic, lifestyle, preference, loyalty data, behavior, shopper value and predictive behavior data points are key to the success of customer analytics.[citation needed]  Finance  Banks, insurance companies and pension funds make use of customer analytics in understanding customer lifetime value, identifying below-zero customers which are estimated to be around 30% of customer base, increasing cross-sales, managing customer attrition as well as migrating customers to lower cost channels in a targeted manner.  Community  Municipalities utilize customer analytics in an effort to lure retailers to their cities. Using psychographic variables, communities can be segmented based on attributes like personality, values, interests, and lifestyle. Using this information, communities can approach retailers that match their community’s profile.  Customer relationship management  Analytical Customer Relationship Management, commonly abbreviated as CRM, enables measurement of and prediction from customer data to provide a 360° view of the client. Predicting customer behaviour  Forecasting buying habits and lifestyle preferences is a process of data mining and analysis. This information consists of many aspects like credit card purchases, magazine subscriptions, loyalty card membership, surveys, and voter registration. Using these categories, profiles can be created for any organization’s most profitable customers. When many of these potential customers are aggregated in a single area it indicates a fertile location for the business to situate. Using a drive time analysis, it is also possible to predict how far a given customer will drive to a particular location. Combining these sources of information, a dollar value can be placed on each household within a trade area detailing the likelihood that household will be worth to a company. Through customer analytics, companies can make decisions with confidence because every decision is based on facts and objective Data. Data mining  There are two types of categories of data mining. Predictive models use previous customer interactions to predict future events while segmentation techniques are used to place customers with similar behaviors and attributes into distinct groups. This grouping can help marketers to optimize their campaign management and targeting processes. See also      Buyer decision processes     Business analytics     Data warehouse     Psychographics     Mattersight Corporation", "category": "Edison", "id": 125}
{"skillName": "DSENG01", "skillText": "Engineering is the application of mathematics, empirical evidence and scientific, economic, social, and practical knowledge in order to invent, innovate, design, build, maintain, research, and improve structures, machines, tools, systems, components, materials, processes and organizations.  The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.  The term Engineering is derived from the Latin ingenium, meaning \"cleverness\" and ingeniare, meaning \"to contrive, devise\".  Contents      1 Definition     2 History         2.1 Ancient era         2.2 Renaissance era         2.3 Modern era     3 Main branches of engineering     4 Practice     5 Methodology         5.1 Problem solving         5.2 Computer use     6 Social context     7 Relationships with other disciplines         7.1 Science         7.2 Medicine and biology         7.3 Art         7.4 Business Engineering and Engineering Management         7.5 Other fields     8 See also     9 References     10 Further reading     11 External links  Definition  The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET)[1] has defined \"engineering\" as:      The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation or safety to life and property.[2][3]  History Main article: History of engineering Relief map of the Citadel of Lille, designed in 1668 by Vauban, the foremost military engineer of his age.  Engineering has existed since ancient times as humans devised fundamental inventions such as the wedge, lever, wheel, and pulley. Each of these inventions is essentially consistent with the modern definition of engineering.  The term engineering is derived from the word engineer, which itself dates back to 1390, when an engine'er (literally, one who operates an engine) originally referred to \"a constructor of military engines.\"[4] In this context, now obsolete, an \"engine\" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.  The word \"engine\" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning \"innate quality, especially mental power, hence a clever invention.\"[5]  Later, as the design of civilian structures such as bridges and buildings matured as a technical discipline, the term civil engineering[3] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the older discipline of military engineering. Ancient era The Ancient Romans built aqueducts to bring a steady supply of clean fresh water to cities and towns in the empire.  The Pharos of Alexandria, the pyramids in Egypt, the Hanging Gardens of Babylon, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuacán and the cities and pyramids of the Mayan, Inca and Aztec Empires, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur and Indian Temples, among many others, stand as a testament to the ingenuity and skill of the ancient civil and military engineers.  The earliest civil engineer known by name is Imhotep.[3] As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC.[6]  Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer,[7][8] and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.[9]  Chinese, Greek and Roman armies employed complex military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C.,[10] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed. Renaissance era  William Gilbert is considered to be the first electrical engineer with his 1600 publication of De Magnete. He coined the term \"electricity\".[11]  The first steam engine was built in 1698 by Thomas Savery.[12] The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.  With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering the fields then known as the mechanic arts became incorporated into engineering. Modern era The International Space Station represents a modern engineering challenge from many disciplines.  The inventions of Thomas Newcomen and the Scottish engineer James Watt gave rise to modern mechanical engineering. The development of specialized machines and machine tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.[3] Structural engineers investigating NASA's Mars-bound spacecraft, the Phoenix Mars Lander  John Smeaton was the first self-proclaimed civil engineer, and is often regarded as the \"father\" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton's Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain \"hydraulicity\" in lime; work which led ultimately to the invention of Portland cement.  The United States census of 1850 listed the occupation of \"engineer\" for the first time with a count of 2,000.[13] There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890 there were 6,000 engineers in civil, mining, mechanical and electrical.[14]  There was no chair of applied mechanism and applied mechanics established at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[15]  The early stages of electrical engineering included the experiments of Alessandro Volta in the 1800s, the experiments of Michael Faraday, Georg Ohm and others and the invention of the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[3] Chemical engineering developed in the late nineteenth century.[3] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[3] The role of the chemical engineer was the design of these chemical plants and processes.[3] The Falkirk Wheel in Scotland  Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[16]  The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[17]  Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I . Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.  In 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage. Main branches of engineering \tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2013) (Learn how and when to remove this template message) Main article: List of engineering branches For a topical guide to this subject, see Outline of engineering#Branches of engineering. The design of a modern auditorium involves many branches of engineering, including acoustics, architecture and civil engineering. Hoover Dam  Engineering is a broad discipline which is often broken down into several sub-disciplines. These disciplines concern themselves with differing areas of engineering work. Although initially an engineer will usually be trained in a specific discipline, throughout an engineer's career the engineer may become multi-disciplined, having worked in several of the outlined areas. Engineering is often characterized as having four main branches:[18][19][20]      Chemical engineering – The application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as petroleum refining, microfabrication, fermentation, and biomolecule production.     Civil engineering – The design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply and treatment etc.), bridges, dams, and buildings.     Electrical engineering – The design, study and manufacture of various electrical and electronic systems, such as electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, controls, and electronics.     Mechanical engineering – The design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, and mechatronics.  Beyond these four, a number of other branches are recognized. Historically, naval engineering and mining engineering were major branches. Other engineering fields sometimes included as major branches[citation needed] are manufacturing engineering, acoustical engineering, corrosion engineering, Instrumentation and control, aerospace, automotive, computer, electronic, petroleum, systems, audio, software, architectural, agricultural, biosystems, biomedical,[21] geological, textile, industrial, materials,[22] and nuclear[23] engineering. These and other branches of engineering are represented in the 36 professional engineering institutions the UK Engineering Council.  New specialties sometimes combine with the traditional fields and form new branches – for example Earth Systems Engineering and Management involves a wide range of subject areas including anthropology, engineering studies, environmental science, ethics and philosophy. A new or emerging area of application will commonly be defined temporarily as a permutation or subset of existing disciplines; there is often gray area as to when a given sub-field warrants classification as a new \"branch.\" One key indicator of such emergence is when major universities start establishing departments and programs in the new field.  For each of these fields there exists considerable overlap, especially in the areas of the application of fundamental sciences to their disciplines such as physics, chemistry, and mathematics. Practice  One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur or European Engineer, Designated Engineering Representative. In the UK many trades are called \"Engineer\" including gas, telephone, photocopy, maintenance, plumber-heating, drainage, sanitary, auto mechanic, TV, Refrigerator, electrician, washing machine, TV antenna installer (satellite) and many others. Methodology \tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2013) (Learn how and when to remove this template message) Design of a turbine requires collaboration of engineers from many fields, as the system involves mechanical, electro-magnetic and chemical processes. The blades, rotor and stator as well as the steam cycle all need to be carefully designed and optimized.  Engineers apply mathematics and sciences such as physics to find suitable solutions to problems or to make improvements to the status quo. More than ever, engineers are now required to have knowledge of relevant sciences for their design projects. As a result, they keep on learning new material throughout their career.  If multiple options exist, engineers weigh different design choices on their merits and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to produce a successful result. It is usually not enough to build a technically successful product; it must also meet further requirements.  Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.  A general methodology and epistemology of engineering can be inferred from the historical case studies and comments provided by Walter Vincenti.[24] Though Vincenti's case studies are from the domain of aeronautical engineering, his conclusions can be transferred into many other branches of engineering, too.  According to Billy Vaughn Koen, the \"engineering method is the use of heuristics to cause the best change in a poorly understood situation within the available resources.\" Koen argues that the definition of what makes one an engineer should not be based on what he produces, but rather how he goes about it.[25] Problem solving \tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (August 2013) (Learn how and when to remove this template message) A drawing for a booster engine for steam locomotives. Engineering is applied to design, with emphasis on function and the utilization of mathematics and science.  Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem allows them to analyze it (sometimes definitively), and to test potential solutions.  Usually multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of \"low-level\" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.  Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.  Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.  The study of failed products is known as forensic engineering, and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure. Computer use A computer simulation of high velocity air flow around a Space Shuttle orbiter during re-entry. Solutions to the flow require modelling of the combined effects of fluid flow and the heat equations.  As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.  One of the most widely used design tools in the profession is computer-aided design (CAD) software like CATIA, Autodesk Inventor, DSS SolidWorks or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.  These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[26]  There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and AEC software for civil engineering.  In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[27] Social context \tThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (July 2010) (Learn how and when to remove this template message) Robotic Kismet can produce a range of facial expressions.  The engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects. Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are pro bono engineering and open-design engineering.  By its very nature engineering has interconnections with society, culture and human behavior. Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety. Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large.  Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.  Engineering is a key driver of innovation and human development.[28] Sub-Saharan Africa in particular has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid.[citation needed] The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[29] Radar, GPS, lidar, ... are all combined to provide proper navigation and obstacle avoidance (vehicle developed for 2007 DARPA Urban Challenge)  All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:      Engineers Without Borders     Engineers Against Poverty     Registered Engineers for Disaster Relief     Engineers for a Sustainable World     Engineering for Change     Engineering Ministries International[30]  Engineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring. This problem is very prominent in the UK where engineering has a poor image and low status.[31] There are many negative economic and political issues that this can cause, as well as ethical issues[32] It is widely agreed that the engineering profession faces an \"image crisis\",[33] rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and other western economies. Relationships with other disciplines Science      Scientists study the world as it is; engineers create the world that has never been.     — Theodore von Kármán[34][35][36]  Engineers, Scientists and Technicians works on target positioner inside National Ignition Facility (NIF) target chamber.  There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]  Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely \"engineering scientists\".[citation needed]  In the book What Engineers Know and How They Know It,[37] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner. Christopher Cassidy of NASA works on the Capillary Flow Experiment aboard the International Space Station.  Examples are the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[citation needed]  As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:      Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.[38]  Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[citation needed] Medicine and biology Leonardo da Vinci, seen here in a self-portrait, has been described as the epitome of the artist/engineer.[39] He is also known for his studies on human anatomy and physiology.  The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology. Genetically engineered mice expressing green fluorescent protein, which glows green under blue light. The central mouse is wild-type.  Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[40][41] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.  Conversely, some engineering disciplines view the human body as a biological machine worth studying, and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[42][43]  Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.  Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[44]  The heart for example functions much like a pump,[45] the skeleton is like a linked structure with levers,[46] the brain produces electrical signals etc.[47] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.  Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[44] Art  There are connections between engineering and art;[48] they are direct in some fields, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering); and indirect in others.[48][49][50][51]  The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[52] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[53] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[49][54]  Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[39][55] Business Engineering and Engineering Management  Business Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management is a specialized field of management concerned with the engineering sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to the engineering sector. This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives. Other fields  In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term. See also Part of a series on Science Formal      Logic Mathematics      Mathematical logic     Mathematical statistics     Theoretical computer science  Physical Chemistry      Acid-base Analytical Environmental Inorganic Nuclear Organic Physical Solid-state Supramolecular Sustainable (\"green\") Theoretical      Astrochemistry Biochemistry Crystallography Food chemistry Geochemistry Materials science Molecular physics Photochemistry Radiochemistry Stereochemistry Surface science  Physics      Classical Modern Applied      Experimental Theoretical      Computational      Atomic Condensed matter      Mechanics         (classical continuum fluid solid)      Molecular Nuclear Particle Plasma Quantum field theory Quantum mechanics (introduction)      Special relativity General relativity Rheology String theory Thermodynamics  Earth sciences      Climatology Ecology Edaphology Environmental science Geodesy Geography (physical) Geology Geomorphology Geophysics Glaciology Hydrology Limnology Meteorology Oceanography Paleoclimatology Paleoecology Palynology Pedology Volcanology  Space science      Astronomy Astrophysics Cosmology Galactic astronomy Planetary geology Planetary science Stellar astronomy  Life Biology      Anatomy Anthropology Astrobiology Biochemistry Biogeography Biological engineering Biophysics Behavioral neuroscience Biotechnology Botany Cell biology Conservation biology Cryobiology Developmental biology Ecology Ethnobiology Ethology Evolutionary biology (introduction) Genetics (introduction) Gerontology Immunology Limnology Marine biology Microbiology Molecular biology Neuroscience Paleontology Parasitology Physiology Radiobiology Soil biology Sociobiology Systematics Toxicology Zoology  Social      Anthropology Archeology Criminology Demography Economics Geography (human) History International relations Law Linguistics Pedagogy Political science Psychology Science education Sociology  Applied Engineering      Aerospace Agricultural Biological Biomedical Chemical Civil Computer science / engineering      Electrical Fire protection Genetic Industrial Mechanical Military Mining Nuclear Operations research Robotics Software Web  Healthcare      Medicine Veterinary Dentistry Midwifery Epidemiology Pharmacy Nursing  Interdisciplinary      Applied physics Artificial intelligence Bioethics Bioinformatics Biomedical engineering Biostatistics Cognitive science Complex systems Computational linguistics Cultural studies Cybernetics Environmental science Environmental social science Environmental studies Ethnic studies Evolutionary psychology      Forestry Library science      Mathematical / theoretical biology      Mathematical physics Military science Network science Neural engineering Neuroscience Science studies Scientific modelling Semiotics Sociobiology Statistics Systems science Urban planning Web science      Philosophy History      Basic research Citizen science Fringe science Protoscience Pseudoscience Freedom Policy Funding Method Technoscience      Outline Portal Category", "category": "Edison", "id": 126}
{"skillName": "DSENG03", "skillText": "And the end result in business is directly linked to the quality of the decisions made at each point along the way.  So not surprisingly, decision-making is a universally important competence in business. Some decisions clearly have a greater impact on the business than others, but the underlying skill is the same: The difference is in the scope and depth of the process you go through to reach your decision.  One reason why decision-making can be so problematic is that the most critical decisions tend to have to be made in the least amount of time. You feel pressured and anxious. The time pressure means taking shortcuts, jumping to conclusions, or relying heavily on instinct to guide your way.  In your organization, you've probably heard of someone who made it all the way to VP by relying on his gut to make decisions. At the other extreme is the guy who simply can't make a decision because he analyses the situation to death. The bottom line is, you have to make decisions, and you have to make good decisions. Poor decisions are bad for business. Worse still, one poor decision can lead to others, and so the impact can be compounded and lead to more and more problems down the line.  Thankfully, decision-making is a skill set that can be learned and improved on. Somewhere between instinct and over-analysis is a logical and practical approach to decision-making that doesn't require endless investigation, but helps you weigh up the options and impacts.  One such approach is called the Kepner-Tregoe Matrix. It provides an efficient, systematic framework for gathering, organizing and evaluating decision making information. The approach was developed by Charles H. Kepner and Benjamin B. Tregoe in the 1960's and they first wrote about it in the business classic, The Rational Manager (1965). The approach is well-respected and used by many of the world's top organizations including NASA and General Motors. The Kepner-Tregoe Approach  The Kepner-Tregoe approach is based on the premise that the end goal of any decision is to make the \"best possible\" choice. This is a critical distinction: The goal is not to make the perfect choice, or the choice that has no defects. So the decision maker must accept some risk. And an important feature of the Kepner-Tregoe Matrix is to help evaluate and mitigate the risks of your decision.  The Kepner-Tregoe Matrix approach guides you through the process of setting objectives, exploring and prioritizing alternatives, exploring the strengths and weaknesses of the top alternatives, and of choosing the final \"best\" alternative. It then prompts you to generate ways to control the potential problems that will crop up as a consequence of your decision.  This type of detailed problem and risk analysis helps you to make an unbiased decision. By skipping this analysis and relying on gut instinct, your evaluation will be influenced by your preconceived beliefs and prior experience – it's simply human nature. The structure of the Kepner-Tregoe approach limits these conscious and unconscious biases as much as possible.  The Kepner-Tregoe Matrix comprises four basic steps:      Situation Appraisal – identify concerns and outline the priorities.     Problem Analysis – describe the exact problem or issue by identifying and evaluating the causes.     Decision Analysis – identify and evaluate alternatives by performing a risk analysis for each and then make a final decision.     Potential Problem Analysis – evaluate the final decision for risk and identify the contingencies and preventive actions necessary to minimize that risk.  Going through each stage of this process will help you come to the \"best possible choice\", given your knowledge and understanding of the issues that bear on the decision.     Identify the decision to be made: After realizing that a decision must be made, you then go through an internal process of trying to clearly define the nature of the decision you must make.     Gather relevant information: Most decisions require collecting pertinent information. Some information must be sought from within yourself through a process of self-assessment, while other information must be sought from outside books, people and a variety of other sources.     Identify alternatives: Through the process of collecting information you will probably identify several possible paths of action, or alternatives. In this step of the decision-making process, you will list all possible and desirable alternatives.     Weigh evidence: In this step, you draw on your information and emotions to imagine what it would be like if you carried out each of the alternatives to the end. You must evaluate whether the need identified in Step 1 would be helped or solved through the use of each alternative.     Choose among alternatives: Once you have weighed all the evidence, you are ready to select the choice that seems to be best suited to you.     Take action: You now take some positive action, which begins to implement the alternative you chose.     Review decision and consequences: In the last step you experience the results of your decision and evaluate whether or not it has \"solved\" the need you identified in Step 1. If it has, you may stay with this decision for some period of time. If the decision has not resolved the identified need, you may repeat certain steps of the process in order to make a new decision.  Decision-making tools and techniques  While the basic principles might be the same, there are dozens of different techniques and tools that can be used when trying to make a decision. Among some of the more popular options, which often use graphs, models or charts, are:      Decision matrix: A decision matrix is used to evaluate all the options of a decision. When using the matrix, create a table with all of the options in the first column and all of the factors that affect the decision in the first row. Users then score each option and weigh which factors are of more importance.  A final score is then tallied to reveal which option is the best.     T-Chart: This chart is used when weighing the plusses and minuses of the options. It ensures that all the positives and negatives are taken into consideration when making a decision.     Decision tree: This is a graph or model that involves contemplating each option and the outcomes of each. Statistical analysis is also conducted with this technique.     Multivoting: This is used when multiple people are involved in making a decision. It helps whittle down a large list options to a smaller one to the eventual final decision.     Pareto analysis: This is a technique used when a large number of decisions need to be made. This helps in prioritizing which ones should be made first by determining which decisions will have the greatest overall impact.     Cost-benefit: This technique is used when weighing the financial ramifications of each possible alternative as a way to come to a final decision that makes the most sense from an economic perspective.     Conjoint analysis: This is a method used by business leaders to determine consumer preferences when making decisions.  Decision-making mistakes  When making decisions, especially in the workplace, it can be easy to fall into several traps that lead to the wrong decision. The U.S. Small Business Administration warns business leaders to be careful of numerous mistakes that are often made during the decision-making process. Several of the mistakes revolve around turning to outside help when making a decision. The SBA advises leaders to not rely too much on expert information when evaluating their choices.   \"Oftentimes, people have a tendency to place too much emphasis on what experts say,\" the SBA writes on its website. \"Remember, experts are only human and have their own set of biases and prejudices just like the rest of us.\"  Rather than trusting their own gut feelings when trying to make a decision, the SBA said business leaders often both over and underestimate the value of the information they receive from others. Instead of letting others sway your belief one way or another, they said leaders should keep the opinions of others in perspective.  In the end, they said not listening to your own feelings or gut reactions can be one of the biggest mistakes that can be made during the decision-making process.  \"Our society teaches us to ignore these feelings, but by tuning into your intuition, you will find that you will make much better decisions in the long run,\" the SBA writes.  Among some of the other common mistakes that leaders often make, according to Jeff Miller, the director of corporate training and development for human resources and business performance solutions provider Insperity, are not taking enough time to make a decision by rushing to a conclusion.  In addition, Miller said procrastinating over actually making a decision is also a huge misstep.  \"As well as being uncomfortable or time-consuming, avoiding decisions can also hurt your reputation,\" Miller wrote on the Insperity website. \"Your staff may perceive it as a lack of care for their well-being, which can create a lack of respect.\"     Decision analysis (DA) is the discipline comprising the philosophy, theory, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision, for prescribing a recommended course of action by applying the maximum expected utility action axiom to a well-formed representation of the decision, and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker and other stakeholders.  Contents      1 History and methodology     2 Controversy     3 See also     4 References     5 Further reading     6 External links  History and methodology  Graphical representation of decision analysis problems commonly use influence diagrams and decision trees. Both of these tools represent the alternatives available to the decision maker, the uncertainty they face, and evaluation measures representing how well they achieve their objectives in the final outcome. Uncertainties are represented through probabilities. The decision maker's attitude to risk is represented by utility functions and their attitude to trade-offs between conflicting objectives can be made using multi-attribute value functions or multi-attribute utility functions (if there is risk involved). In some cases, utility functions can be replaced by the probability of achieving uncertain aspiration levels. Decision analysis advocates choosing that decision whose consequences have the maximum expected utility (or which maximize the probability of achieving the uncertain aspiration level). Such decision analytic methods are used in a wide variety of fields, including business (planning, marketing, and negotiation), environmental remediation, health care research and management, energy exploration, litigation and dispute resolution, etc.  Decision analysis is used by major corporations to make multibillion-dollar capital investments. In 2010, Chevron won the Decision Analysis Society Practice Award  for its use of decision analysis in all major decisions. In a video  detailing Chevron's use of decision analysis, Chevron Vice Chairman George Kirkland notes that \"decision analysis is a part of how Chevron does business for a simple, but powerful, reason: it works.\" Controversy  Decision analysis is designed to be a prescriptive approach to decision making, especially decision-making under uncertainties. The distinction between prescriptive and descriptive perspectives of decision-making is important, because the first answers the question \"how should I make my decisions\" (even though people don't act this way necessarily), while the second targets to explain how people make decisions (even though in many cases the way they make the decisions are not perfect).  Decision researchers studying how individuals research decisions have found that decision analysis is rarely used.[1] High-stakes decisions, made under time pressure, are not well described by decision analysis.[2] Some decision analysts, in turn,[3] argue that their approach is prescriptive, providing a prescription of what actions to take based on sound logic, rather than a descriptive approach, describing the flaws in the way people do make decisions. Critics cite the phenomenon of paralysis by analysis as one possible consequence of over-reliance on decision analysis in organizations.  Studies have demonstrated the utility of decision analysis in creating decision-making algorithms that are superior to \"unaided intuition\".[4][5]  The term \"decision analytic\" has often been reserved for decisions that do not appear to lend themselves to mathematical optimization methods. Methods like applied information economics, however, attempt to apply more rigorous quantitative methods even to these types of decisions. See also      Choice     Decision analysis cycle     Decision engineering     Decision making software     Decision model     Decision quality     Decision support     Decision theory     Influence diagram     Management science     Micromort     Multiple-criteria decision analysis (MCDA)     Optimal decision     Stochastic dominance    Decision-making software (DMS) is used to help individuals and organizations with their decision-making processes, typically resulting in ranking, sorting or choosing from among alternatives.  An early example of DMS was described in 1973.[1][2] Prior to the advent of the World Wide Web, most DMS was spreadsheet-based,[2] with the first web-based DMS appearing in the mid-1990s.[3] Nowadays, at least 20 DMS products (mostly web-based) are available.[4][5][6]  Though DMS exists for the various stages of structuring and solving decision problems – from brain-storming problems to representing decision-maker preferences and reaching decisions – most DMS focuses on choosing from among a group of alternatives characterized on multiple criteria or attributes.[4]  Contents      1 Purpose     2 Methods and features         2.1 Decision-making methods         2.2 Software features         2.3 Comparison of decision-making software     3 See also     4 References  Purpose  DMS is a tool that is intended to support the analysis involved in decision-making processes, not to replace it.[5] \"DMS should be used to support the process, not as the driving or dominating force.\"[7] DMS frees users \"from the technical implementation details [of the decision-making method employed – discussed in the next section], allowing them to focus on the fundamental value judgements\".[7] Nonetheless, DMS should not be employed blindly. \"Before using a software, it is necessary to have a sound knowledge of the adopted methodology and of the decision problem at hand.\"[8] Methods and features Decision-making methods  Most decision-making processes supported by DMS are based on decision analysis, most commonly multi-criteria decision making (MCDM). MCDM involves evaluating and combining alternatives' characteristics on two or more criteria or attributes in order to rank, sort or choose from among the alternatives.[9]  DMS employs a variety of MCDM methods;[7] popular examples include (and see the table below):      Aggregated Indices Randomization Method (AIRM)     Analytic Hierarchy Process (AHP)     Analytic network process (ANP, an extension of AHP)     Elimination and Choice Expressing Reality (ELECTRE)     Measuring Attractiveness by a Categorical Based Evaluation Technique (MACBETH) [10]     Multi-attribute global inference of quality (MAGIQ)     Potentially All Pairwise RanKings of all possible Alternatives (PAPRIKA)     Preference Ranking Organization Method for Enrichment Evaluation (PROMETHEE)     The Evidential reasoning approach for MCDM under hybrid uncertainty  Naturally, there are significant differences between these methods[7][9] and, accordingly, the DMS implementing them. Such differences include:          The extent to which the decision problem is broken into a hierarchy of sub-problems;         Whether or not pairwise comparisons of alternatives and/or criteria are used to elicit decision-makers' preferences;         The use of interval scale or ratio scale measurements of decision-makers' preferences;         The number of criteria included;         The number of alternatives evaluated, ranging from a few (finite) to infinite;         The extent to which numerical scores are used to value and/or rank alternatives;         The extent to which incomplete rankings (relative to complete rankings) of alternatives are produced;         The extent to which uncertainty is modeled and analyzed.  Software features  In addition to helping decision-makers to rank, sort or choose from among alternatives, DMS products often include a variety of additional features and tools;[3][4] examples include:      Time analysis and time optimization     Sensitivity analysis and fuzzy logic calculations     Risk aversion measurement     Group evaluation (teamwork)     Graphic or visual presentation tools  Comparison of decision-making software  Notable software includes the following. Software \tSupported MCDA Methods \tPairwise Comparison \tSensitivity Analysis \tGroup Evaluation \tWeb-based 1000Minds \tPAPRIKA \tYes \tYes \tYes \tYes \t[5] Ahoona \tWSM, Utility \tNo \tNo \tYes \tYes \t[11] Altova MetaTeam \tWSM \tNo \tNo \tYes \tYes \t[citation needed] Analytica \t\tNo \tYes \tNo \tYes \t[5] Criterium DecisionPlus \tAHP, SMART \tYes \tYes \tNo \tNo \t[citation needed] D-Sight \tPROMETHEE, UTILITY \tYes \tYes \tYes \tYes \t[5] DecideIT \tMAUT \tYes \tYes \tYes \tYes \t[5] Decision Lens \tAHP, ANP \tYes \tYes \tYes \tYes \t[citation needed] Super Decisions \tAHP, Analytic Network Process \tYes \tYes \tNo \tYes \t[12][12] Expert Choice \tAHP \tYes \tYes \tYes \tYes \t[5] Hiview3 \t\tNo \tYes \tYes \tNo \t[5] Intelligent Decision System \tEvidential Reasoning Approach, Bayesian Inference, Dempster–Shafer theory, Utility \tYes \tYes \tYes \tAvailable on request \t[5] Logical Decisions \tAHP \tYes \tYes \tYes \tNo \t[5] Loomio \t ? \t ? \t ? \t ? \tYes \t M-MACBETH \tMACBETH \tYes \tYes \tYes \tNo \t[10][13] PriEsT \tAHP \tYes \tYes \tNo \tNo \t[14] WISED \tMACBETH \tYes \tYes \tYes \tYes \t[10][15] See also      iconSoftware portal       Decision engineering     Decision support system     Project management software     List of concept- and mind-mapping software     Strategic planning software    Decision engineering (more recently called Decision Intelligence by The Decision Intelligence Institute International[1] and companies like Quantellia[2]) is a framework that unifies a number of best practices for organizational decision making. It is based on the recognition that, in many organizations, decision making could be improved if a more structured approach were used. Decision engineering seeks to overcome a decision making \"complexity ceiling\", which is characterized by a mismatch between the sophistication of organizational decision making practices and the complexity of situations in which those decisions must be made. As such, it seeks to solve some of the issues identified around complexity theory and organizations. In this sense, decision engineering represents a practical application of the field of complex systems, which helps organizations to navigate the complex systems in which they find themselves. Decision engineering can also be thought of as a framework that brings advanced analytics techniques to the desktop of the non-expert decision maker, as well as incorporating, and then extending, inductive reasoning and machine learning techniques to overcome the problems articulated in Black swan theory.[citation needed]  Decision engineering proponents[3] believe that many organizations continue to make poor decisions.[4][5] In response, decision engineering seeks to unify a number of decision making best practices, described in more detail below.  Decision engineering builds on the insight that it is possible to design the decision itself, using principles previously used for designing more tangible objects like bridges and buildings.[6]  The use of a visual design language representing decisions is an important element of decision engineering, since it provides an intuitive common language readily understood by all decision participants. A visual metaphor[7] improves the ability to reason about complex systems[8] as well as to enhance collaboration.  In addition to visual decision design, there are other two aspects of engineering disciplines that aid mass adoption. These are: 1) the creation of a shared language of design elements and 2) the use of a common methodology or process, as illustrated in the diagram above.  Contents      1 Motivation     2 Transferring engineering principles     3 Bringing numerical methods to the desktop     4 Origins     5 Visual decision design         5.1 Explicit representation of intangibles     6 See also     7 Notes     8 Bibliography     9 External links     10 References  Motivation  The need for a unified methodology of decision making is driven by a number of factors that organizations face as they make difficult decisions in a complex internal and external environment.  Recognition of the broad-based inability of current methods to solve decision making issues in practice comes from several sources, including government sources and industries such as telecommunications, media, the automotive industry, and pharmaceuticals.  Examples:      The outcomes of decisions are becoming more complex, going well beyond next quarter's revenues or other tangible outcomes to multiple goals that must be satisfied together, some of which are often intangible:      The car is becoming an expression of identity, values, and personal control in ways that move far beyond traditional segmentation and branding. For example, fuel efficiency will be only one consideration for a socially responsible vehicle (SRV). What percent of the parts are recyclable? What is the vehicle's total carbon footprint? Are there child labor inputs? Toxic paints, glues, or plastics? How transparent is the supply chain? Is the seller accountable for recycling? What methods are used? Are fair labor practices employed?     — Shoshana Zuboff, The GM Solution: Life Boats, Not Life Support.      Business Week, November 18, 2008      Global increase in complexity:      We live in a dynamic world in which the pace, scope, and complexity of change are increasing. The continued march of      globalization, the growing number of independent actors, and advancing technology have increased global connectivity, interdependence and complexity, creating greater uncertainties, systemic risk and a less predictable future. These changes     have led to reduced warning times and compressed decision cycles.     — Director of National Intelligence, Vision 2015: A Globally Networked and Integrated Intelligence Enterprise      Also see this Vision 2015 summary  Transferring engineering principles  Unlike other decision making tools and methodologies, decision engineering seeks to bring to bear a number of engineering practices to the process of creating a decision. These include requirements analysis, specification, scenario planning, quality assurance, security, and the use of design principles as described above. During the decision execution phase, outputs produced during the design phase can be used in a number of ways; monitoring approaches like business dashboards and assumption based planning are used to track the outcome of a decision and to trigger replanning as appropriate. One view of how some of these elements combine is shown in the diagram at the start of this article.  Decision engineering has the potential to improve the quality of decisions made, the ability to make them more quickly, the ability to align organizational resources more effectively around a change in decisions, and lowers the risks associated with decisions. Furthermore, a designed decision can be reused and modified as new information is obtained. [9] Bringing numerical methods to the desktop  Although many elements of decision engineering, such as Sensitivity analysis and analytics are mature disciplines, they are not in wide use by decision makers.[10] Decision engineering seeks to create a visual language that serves to facilitate communication between them and quantitative experts, allowing broader utilization of these and other numerical and technical approaches.  In particular, dependency links in a decision model represent cause-and-effect (as in a causal loop diagram), data flow (as in a data flow diagram), or other relationships. As an example, one link might represent the connection between \"mean time to repair a problem with telephone service\" and \"customer satisfaction\", where a short repair time would presumably raise customer satisfaction. The functional form of these dependencies can be determined by a number of approaches. Numerical approaches, which analyze data to determine these functions, include machine learning and analytics algorithms (including artificial neural networks), as well as more traditional regression analysis. Results from operations research and many other quantitative approaches have a similar role to play.  When data is not available (or is too noisy, uncertain, or incomplete), these dependency links can take on the form of rules as might be found in an expert system or rule-based system, and so can be obtained through knowledge engineering.  In this way, a decision model represents a mechanism for combining multiple relationships, as well as symbolic and subsymbolic reasoning, into a complete solution to determining the outcome of a practical decision. Origins \tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2012) (Learn how and when to remove this template message)  It is interesting to note that, despite decades of development of decision support system and methodologies (like decision analysis), these are still less popular than spreadsheets as primary tools for decision making. Decision engineering seeks to bridge this gap, creating a critical mass of users of a common methodology and language for the core entities included in a decision, such as assumptions, external values, facts, data, and conclusions. If a pattern from previous industries holds, such a methodology will also facilitate technology adoption, by clarifying common maturity models and road maps that can be shared from one organization to another.  The decision engineering approach is multidisciplinary, unifying findings on cognitive bias and decision making, situational awareness, critical and creative thinking, collaboration and organizational design, with engineering technologies.  Decision engineering is considered an improvement upon current organizational decision making practices, which include the use of spreadsheets (difficult to QA, hard to collaborate and discuss), text (sequential in nature, so is not a good fit for how information flows through a decision structure), and verbal argument. The movement from these largely informal structures to one in which a decision is documented in a well understood, visual language, echoes the creation of common blueprint methodologies in construction, with promise of similar benefits.  Decision engineering is both a very new and also a very old discipline. Many of its elements—such as the language of assessing assumptions, using logic to support an argument, the necessity of critical thinking to evaluate a decision, and understanding the impacts of bias—are ancient. Yet the realization that these elements can form a coherent whole that provides significant benefits to organizations by focusing on a common methodology is relatively new. Visual decision design  Because it makes visible the otherwise invisible reasoning structures used in complex decisions, the design aspect of decision engineering draws from other conceptual representation technologies like mind mapping, conceptual graphs, and semantic networks.  The basic idea is that a visual metaphor enhances intuitive thinking, inductive reasoning, and pattern recognition—important cognitive skills usually less accessible in a verbal or text discussion. A business decision map can be seen as one approach to a formal decision language to support decision engineering. See, e.g., Waring, 2010.[11] Explicit representation of intangibles  Decision engineering recognizes that many aspects of decision making are based on intangible elements, including opportunity costs, employee morale, intellectual capital, brand recognition and other forms of business value that are not captured in traditional quantitative or financial models. Value network analysis—most notably Value network maps  —are therefore relevant here. See also      Amos Tversky     Daniel Kahneman     The Black Swan (Taleb book)    A decision support system (DSS) is a computer-based information system that supports business or organizational decision-making activities. DSSs serve the management, operations, and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. Unstructured and Semi-Structured decision problems. Decision support systems can be either fully computerized, human-powered or a combination of both.  While academics have perceived DSS as a tool to support decision making process, DSS users see DSS as a tool to facilitate organizational processes.[1] Some authors have extended the definition of DSS to include any system that might support decision making.[2] Sprague (1980) defines DSS by its characteristics:      DSS tends to be aimed at the less well structured, underspecified problem that upper level managers typically face;     DSS attempts to combine the use of models or analytic techniques with traditional data access and retrieval functions;     DSS specifically focuses on features which make them easy to use by noncomputer people in an interactive mode; and     DSS emphasizes flexibility and adaptability to accommodate changes in the environment and the decision making approach of the user.  DSSs include knowledge-based systems. A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions.  Typical information that a decision support application might gather and present includes:      inventories of information assets (including legacy and relational data sources, cubes, data warehouses, and data marts),     comparative sales figures between one period and the next,     projected revenue figures based on product sales assumptions.  DSSs are often contrasted with more automated decision-making systems known as Decision Management Systems.[3]  Contents      1 History     2 Taxonomies     3 Components     4 Development frameworks     5 Classification     6 Applications     7 See also     8 References     9 Further reading  History  The concept of decision support has evolved from two main areas of research: The theoretical studies of organizational decision making done at the Carnegie Institute of Technology during the late 1950s and early 1960s, and the technical work on Technology in the 1960s.[4] DSS became an area of research of its own in the middle of the 1970s, before gaining in intensity during the 1980s. In the middle and late 1980s, executive information systems (EIS), group decision support systems (GDSS), and organizational decision support systems (ODSS) evolved from the single user and model-oriented DSS.  According to Sol (1987)[5] the definition and scope of DSS has been migrating over the years. In the 1970s DSS was described as \"a computer-based system to aid decision making\". In the late 1970s the DSS movement started focusing on \"interactive computer-based systems which help decision-makers utilize data bases and models to solve ill-structured problems\". In the 1980s DSS should provide systems \"using suitable and available technology to improve effectiveness of managerial and professional activities\", and towards the end of 1980s DSS faced a new challenge towards the design of intelligent workstations.[5]  In 1987, Texas Instruments completed development of the Gate Assignment Display System (GADS) for United Airlines. This decision support system is credited with significantly reducing travel delays by aiding the management of ground operations at various airports, beginning with O'Hare International Airport in Chicago and Stapleton Airport in Denver Colorado.[6] Beginning in about 1990, data warehousing and on-line analytical processing (OLAP) began broadening the realm of DSS. As the turn of the millennium approached, new Web-based analytical applications were introduced.  The advent of more and better reporting technologies has seen DSS start to emerge as a critical component of management design. Examples of this can be seen in the intense amount of discussion of DSS in the education environment.  DSS also have a weak connection to the user interface paradigm of hypertext. Both the University of Vermont PROMIS system (for medical decision making) and the Carnegie Mellon ZOG/KMS system (for military and business decision making) were decision support systems which also were major breakthroughs in user interface research. Furthermore, although hypertext researchers have generally been concerned with information overload, certain researchers, notably Douglas Engelbart, have been focused on decision makers in particular. Taxonomies  Using the relationship with the user as the criterion, Haettenschwiler[7] differentiates passive, active, and cooperative DSS. A passive DSS is a system that aids the process of decision making, but that cannot bring out explicit decision suggestions or solutions. An active DSS can bring out such decision suggestions or solutions. A cooperative DSS allows the decision maker (or its advisor) to modify, complete, or refine the decision suggestions provided by the system, before sending them back to the system for validation. The system again improves, completes, and refines the suggestions of the decision maker and sends them back to them for validation. The whole process then starts again, until a consolidated solution is generated.  Another taxonomy for DSS has been created by Daniel Power. Using the mode of assistance as the criterion, Power differentiates communication-driven DSS, data-driven DSS, document-driven DSS, knowledge-driven DSS, and model-driven DSS.[8]      A communication-driven DSS supports more than one person working on a shared task; examples include integrated tools like Google Docs or Groove[9]     A data-driven DSS or data-oriented DSS emphasizes access to and manipulation of a time series of internal company data and, sometimes, external data.     A document-driven DSS manages, retrieves, and manipulates unstructured information in a variety of electronic formats.     A knowledge-driven DSS provides specialized problem-solving expertise stored as facts, rules, procedures, or in similar structures.[8]     A model-driven DSS emphasizes access to and manipulation of a statistical, financial, optimization, or simulation model. Model-driven DSS use data and parameters provided by users to assist decision makers in analyzing a situation; they are not necessarily data-intensive. Dicodess is an example of an open source model-driven DSS generator.[10]  Using scope as the criterion, Power[11] differentiates enterprise-wide DSS and desktop DSS. An enterprise-wide DSS is linked to large data warehouses and serves many managers in the company. A desktop, single-user DSS is a small system that runs on an individual manager's PC. Components Design of a drought mitigation decision support system  Three fundamental components of a DSS architecture are:[7][8][12][13][14]      the database (or knowledge base),     the model (i.e., the decision context and user criteria)     the user interface.  The users themselves are also important components of the architecture.[7][14] Development frameworks  DSS systems are not entirely different from other systems and require a structured approach. Such a framework includes people, technology, and the development approach.[12]  The Early Framework of Decision Support System consists of four phases:  Intelligence Searching for conditions that call for decision.  Design Developing and analyzing possible alternative actions of solution.  Choice Selecting a course of action among those.  Implementation Adopting the selected course of action in decision situation.  DSS technology levels (of hardware and software) may include:      The actual application that will be used by the user. This is the part of the application that allows the decision maker to make decisions in a particular problem area. The user can act upon that particular problem.     Generator contains Hardware/software environment that allows people to easily develop specific DSS applications. This level makes use of case tools or systems such as Crystal, Analytica and iThink.     Tools include lower level hardware/software. DSS generators including special languages, function libraries and linking modules  An iterative developmental approach allows for the DSS to be changed and redesigned at various intervals. Once the system is designed, it will need to be tested and revised where necessary for the desired outcome. Classification  There are several ways to classify DSS applications. Not every DSS fits neatly into one of the categories, but may be a mix of two or more architectures.  Holsapple and Whinston[15] classify DSS into the following six frameworks: text-oriented DSS, database-oriented DSS, spreadsheet-oriented DSS, solver-oriented DSS, rule-oriented DSS, and compound DSS.  A compound DSS is the most popular classification for a DSS. It is a hybrid system that includes two or more of the five basic structures described by Holsapple and Whinston.[15]  The support given by DSS can be separated into three distinct, interrelated categories:[16] Personal Support, Group Support, and Organizational Support.  DSS components may be classified as:      Inputs: Factors, numbers, and characteristics to analyze     User Knowledge and Expertise: Inputs requiring manual analysis by the user     Outputs: Transformed data from which DSS \"decisions\" are generated     Decisions: Results generated by the DSS based on user criteria  DSSs which perform selected cognitive decision-making functions and are based on artificial intelligence or intelligent agents technologies are called Intelligent Decision Support Systems (IDSS)[17]  The nascent field of Decision engineering treats the decision itself as an engineered object, and applies engineering principles such as Design and Quality assurance to an explicit representation of the elements that make up a decision. Applications  As mentioned above, there are theoretical possibilities of building such systems in any knowledge domain.  One is the clinical decision support system for medical diagnosis. There are four stages in the evolution of clinical decision support system (CDSS). The primitive version is standalone which does not support integration. The second generation of CDSS supports integration with other medical systems. The third generation is standard-based while the fourth is service model-based.[18]  Other examples include a bank loan officer verifying the credit of a loan applicant or an engineering firm that has bids on several projects and wants to know if they can be competitive with their costs.  DSS is extensively used in business and management. Executive dashboard and other business performance software allow faster decision making, identification of negative trends, and better allocation of business resources. Due to DSS all the information from any organization is represented in the form of charts, graphs i.e. in a summarized way, which helps the management to take strategic decision. For example, one of the DSS applications is the management and development of complex anti-terrorism systems.[19]  A growing area of DSS application, concepts, principles, and techniques is in agricultural production, marketing for sustainable development. For example, the DSSAT4 package,[20][21] developed through financial support of USAID during the 80s and 90s, has allowed rapid assessment of several agricultural production systems around the world to facilitate decision-making at the farm and policy levels. There are, however, many constraints to the successful adoption on DSS in agriculture.[22]  DSS are also prevalent in forest management where the long planning horizon and the spatial dimension of planning problems demands specific requirements. All aspects of Forest management, from log transportation, harvest scheduling to sustainability and ecosystem protection have been addressed by modern DSSs. In this context the consideration of single or multiple management objectives related to the provision of goods and services that traded or non-traded and often subject to resource constraints and decision problems. The Community of Practice of Forest Management Decision Support Systems provides a large repository on knowledge about the construction and use of forest Decision Support Systems.[23]  A specific example concerns the Canadian National Railway system, which tests its equipment on a regular basis using a decision support system. A problem faced by any railroad is worn-out or defective rails, which can result in hundreds of derailments per year. Under a DSS, CN managed to decrease the incidence of derailments at the same time other companies were experiencing an increase. See also Recommender systems Concepts      Collective intelligence Relevance Star ratings Long tail   Methods and challenges      Cold start Collaborative filtering Dimensionality reduction Implicit data collection Item-item collaborative filtering Preference elicitation Similarity search   Implementations      Collaborative search engine Content discovery platform Decision support system Music Genome Project Product finder   Research      GroupLens Research MovieLens Netflix Prize       v t e   \tWikimedia Commons has media related to Decision support systems.      Clinical decision support system     Spatial decision support system     Land Allocation Decision Support System     Decision engineering     Decision-making software     Decision theory     Enterprise Decision Management     Expert system     Judge–advisor system     Morphological analysis (problem-solving)     Online deliberation     Predictive analytics     Self service software     Tableau Public: Tableau democratizes visualization in an elegantly simple and intuitive tool. It is exceptionally powerful in business because it communicates insights through data visualization. Although great alternatives exist, Tableau Public's million row limit provides a great playground for personal use and the free trial is more than long enough to get you hooked. In the analytics process, Tableau's visuals allow you to quickly investigate a hypothesis, sanity check your gut, and just go explore the data before embarking on a treacherous statistical journey.  OpenRefine: Formerly GoogleRefine, OpenRefine is a data cleaning software that allows you to get everything ready for analysis. What do I mean by that? Well, let's look at an example. Recently, I was cleaning up a database that included chemical names and noticed that rows had different spellings, capitalization, spaces, etc that made it very difficult for a computer to process. Fortunately, OpenRefine contains a number of clustering algorithms (groups together similar entries) and makes quick work of an otherwise messy problem. **Tip- Increase Java Heap Space to run large files (Google the tip for exact instructions!)  KNIME: KNIME allows you to manipulate, analyze, and modeling data in an incredibly intuitive way through visual programming. Essentially, rather than writing blocks of code, you drop nodes onto a canvas and drag connection points between activities. More importantly, KNIME can be extended to run R, python, text mining, chemistry data, etc, which gives you the option to dabble in the more advanced code driven analysis. **TIP- Use \"File Reader\" instead of CSV reader for CSV files. Strange quirk of the software.  RapidMiner: Much like KNIME, RapidMiner operates through visual programming and is capable of manipulating, analyzing and modeling data. Most recently, RapidMiner won KDnuggets software poll, demonstrating that data science does not need to be a counter-intuitive coding endeavor.  Google Fusion Tables: Meet Google Spreadsheets cooler, larger, and much nerdier cousin. Google Fusion tables is an incredible tool for data analysis, large data-set visualization, and mapping. Not surprisingly, Google's incredible mapping software plays a big role in pushing this tool onto the list. Take for instance this map, which I made to look at oil production platforms in the Gulf of Mexico. With just a quick upload, Google Fusion tables recognized the latitude and longitude data and got to work.  NodeXL: NodeXL is a visualization and analysis software of networks and relationships. Think of the giant friendship maps you see that represent linkedin or Facebook connections. NodeXL takes that a step further by providing exact calculations. If you're looking for something a little less advanced, check out the node graph on Google Fusion Tables, or for a little more visualization try out Gephi.  Import.io: Web scraping and pulling information off of websites used to be something reserved for the nerds. Now with Import.io, everyone can harvest data from websites and forums. Simply highlight what you want and in a matter of minutes Import.io walks you through and \"learns\" what you are looking for. From there, Import.io will dig, scrape, and pull data for you to analyze or export.  Google Search Operators: Google is an undeniably powerful resource and search operators just take it a step up. Operators essentially allow you to quickly filter Google results to get to the most useful and relevant information. For instance, say you're looking for a Data science report published this year from ABC Consulting. If we presume that the report will be in PDF we can search      \"Date Science Report\" site:ABCConsulting.com Filetype:PDF   then underneath the search bar, use the \"Search Tools\" to limit the results to the past year. The operators can be even more useful for discovering new information or market research.  Solver: Solver is an optimization and linear programming tool in excel that allows you to set constraints (Don't spend more than this many dollars, be completed in that many days, etc). Although advanced optimization may be better suited for another program (such as R's optim package), Solver will make quick work of a wide range of problems.  WolframAlpha: Wolfram Alpha's search engine is one of the web's hidden gems and helps to power Apple's Siri. Beyond snarky remarks, Wolfram Alpha is the nerdy Google, provides detailed responses to technical searches and makes quick work of calculus homework. For business users, it presents information charts and graphs, and is excellent for high level pricing history, commodity information, and topic overviews.", "category": "Edison", "id": 127}
{"skillName": "DSDA05", "skillText": "Big Data Analytics platforms Big Data tools Hadoop, Spark Distributed computing tools a plus Spark, MapReduce, Hadoop, Hive Real time and streaming analytics systems Flume, Kafka, Storm Hadoop Ecosystem/platform Spotfire Azure Data Analytics platforms HDInsight, APS and PDW Amazon Data Analytics platform Kinesis, EMR Other cloud based Data Analytics platforms HortonWorks, Vertica LexisNexis HPCC System hadoop spark YARN HDFS MapReduce Apache Flink Apache Storm Apache Samza Apache Kafka Cloudera software framework big data framework User centered approaches are well known in the visualization community (although not always implemented) [D'Amico et al. 2005, Munzner et al. 2009]. Jointly developing the visualizations themselves, however, is rather rare. As we have very good experience with co-creative techniques in design and innovation, we wanted to apply them to the domain of data visualization as well. For example, we tried to experiment with data sets during a day-long workshop with a larger group of stakeholders (a session we called the “data picnic” because everyone brought his/her data and tools). Visualization  For this paper, we focused on a pixel oriented technique [Keim 2000] to fullfill requirements such as visualization of raw data or a chronological view of data to preserve the course of events. We stack graphical representations for various parameters of a log line (such as IP, user name, request or message) so that we get small columns for each log line. Lining up these stacks produces a dense visual representation with distinct patterns. This is why we call it the Pixel Carpet. Other subgroups of our research group took different approaches that can be found at other places in this blog. Snapshot of the Pixel Carpet interface. Each \"multi pixel\" represents one log line, as it a appears at the bottom of the screen.Snapshot of the Pixel Carpet interface. Each “multi pixel” represents one log line, as it a appears at the bottom of the screen. Data and Code  Our data sources included an ssh log (~13.000 lines, unpublished for privacy reasons) and an Apache (web server) access log (~145.000 lines, unpublished), and ~4.500 lines (raw data available, including countries from ip2geo .csv | .json ).  We implemented our ideas in a demonstrator in plain HTML/JavaScript (demo online – caution, will heavily stress your CPU). It helped us iterate quickly and evaluate the idea at various stages, also with new stakeholders. While the code achieves what we need, we are also aware that computing performance is rather bad. If you want to take a look or even improve it, you can find it on github.  To bring it closer to a productive tool, we would turn the Pixel Carpet into a plugin for state-of-the-art data processing engines such as ElasticSearch/Kibana or splunk (scriptable with d3.js since version 6). Time Series Visualizations – An overview by Kim Albrecht\ton October 17, 2013, 1 comment  “Time-series — sets of values changing over time” A Tour Through the Visualization Zoo http://hci.stanford.edu/jheer/files/zoo/  This description of the word “Time-Series” is very close to the explanation in Oxfords dictionary which adds that the word comes from a statistic background and often the intervals are equal within the time-series. http://www.oxforddictionaries.com/definition/english/time-series?q=time-series  Within our research project we are mainly interested in the visualization part within the vast field of statistics. In the book “The Visual Display of Quantitative Information” Edward Tufte defines time-series visualizations as:  “With one dimension marching along to the regular rhythm of seconds, minutes, hours, days, weeks, months, years, centuries, or millennia, the natural ordering of the time scale gives this design a strength and efficiency of interpretation found in no other graphic arrangement.” Edward R. Tufte The Visual Display of Quantitative Information p. 28  Classical datasets of time series visualizations are temperature, wind, condensation (or any other kind of weather measurement), stock data, population change, electricity usage etc. the field is so vast that Tufte writes that in a study that analysed graphics between 1974 and 1980 75% of the graphics where time-series visualizations. Obviously more than 30 years later the field has changed but time-series still seams to be an important part within the area.  In my opinion most Security Network Data doesn’t provide information with changing values over time initially. For example Flow Data is structured through nodes and edges with additional information. These single incidents in time don’t hold the same characteristics as usual time-series datasets where one value changes. But on a certain level of abstraction (for example by counting incidents within set timeframes) or by combining time-series with other methods like network visualizations this kind of graphics could be very helpful for us.  This article first summarises a few classical time-series examples and than looks at recent developments in the field.  The first time-series visualization was designed in the tenth or possibly eleventh century. It shows the changing positions of the planets with the time on the x-axis.  As we will see the use of the x-axis is still the most common form of presenting time-series graphics. Nathan Yau gives an overview of the most common forms of time-series visualizations in his book “data points” which are in his opinion bar graphs, line charts, dot plots & dot-bar graphs. All of this charts are actually similar in what they do. The only difference is the graphical representation of the data. While all of them use the time dimension on the x-axis, Nathan Yau gives two examples for different representation methods. Radial plots, which are similar to line charts, just circular and calendar heat maps.  Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky from Stanford University are giving a different overview of time-series visualizations in their article “A Tour Through the Visualization Zoo”. Their overview starts with index charts, which is an interactive line chart. Index Chart  Stacked Graphs. Which are Area Charts that are stacked on top of each other. They are also called stream graphs. What makes them special is the fact that we get a visual summation of all time-series values.  The controversy around stacked graphs is very big. Alberto Cairo, graphics director at El Mundo Online wrote in a blog article that stacked graphs are “one of the worst graphics the New York Times have published – ever!” on the other hand the publisher of the first paper on stacked graphs wrote: “simplifying the user’s task of tracking individual themes through time by providing a continuous ‘flow’ from one time point to the next”. Furthermore, “we believe this metaphor is familiar and easy to understand and that it requires little cognitive effort to interpret the visualization” both points seam valid to me the cognitive effort needed in some contemporary visualizations is so high that it becomes hard to understand them without putting a lot of effort into them. Stacked Graphs are very simple to understand for the complexity they hold but the information output that can be generated from them is questionable. Andy Kirk from visualisingdata.com credits both sides very fairly in his blog article about the graphs with these comments:  “… a streamgraph is a fantastic solution to displaying large data sets to a mass audience.”  “The main problem facing static streamgraphs lies in the difficulty of reading data points formed by uncommon shapes.”  Tools: D3, Processing  Paper: ThemeRiver: Visualizing Theme Changes over Time,  Stacked Graphs – Geometry & Aesthetics  Example: The Ebb and Flow of Movies, How Different Groups Spend Their Day, Trace (this one is about visualizing wireless networks)   Stacked Graph  Small Multiples are multiple time-series graphs (what kind these graphs are is another question, in this case, area charts) arranged within a grid. Small multiples are more use full to understand different datasets on its own and not as a summary apposed to the stacked graphs. Small Multiples  The last example from the article are horizon graphs. These are actual also area charts which are mirrored and separated by occupacity. This is especially interesting in combination with small multiples because the “data density” is much higher than which classic area charts which leads to more information in a smaller space. An important factor when we are dealing with big datasets. Horizon Graph  There is some interesting research about the usefulness of horizon graphs that I recommend: Tool, Paper, Article    The list of graphics from the Stanford Group are much more contemporary than the examples from Nathan Yau, but still all of these examples use the same mechanism to visualize time-series data by using one axis as a dimension for time. This now more than 1.000 years old way to visualize time is helpful and very common but might not always be the best choice. As we know from scatter-plot visualizations our two space dimensions within a graphic are maybe the most powerful ones for pattern recognition and time might not be the main factor to identify these patterns. So what other ways are there to use time as a dimension within a visualization a part from space?  Animation: At least since Hans Roslings famous TED talks the usage of animation for displaying time is common and it seams to be the most obvious way to visualize time very literal though time. But the technique needs to be used with caution. Tamara Munzners visualization principles give a great insight on page 59 why visualizing time with animation is dangerous:  Principle: external cognition vs. internal memory      easy to compare by moving eyes between side-by-side views –harder to compare visible item to memory of what you saw  Implications for animation      great for choreographed storytelling     great for transitions between two states     poor for many states with changes everywhere  There is also a paper about the topic which gives more insights into the problem.  Small multiples: I already mentioned small multiples above but as I raised before the idea behind small multiples is more of a frame for visualizations than an actual kind of visualization. Like this we can also use each multiple as a timeframe. A beautiful example of small multiples with time as a dimension comes from the NYTimes Graphics department.  Binning time in bubbles: The idea here is to use bubble charts where the time dimension gets binned by minutes, days, years etc. into one bubble and compared to each other. In the Nasdaq 100 Index example each year is represented by one bubble.  Scatterplots: Scatterplots where time is displayed as connected points against two variables. This is similar to the animation idea. But in this case the animated dots leave behind a path behind. Also here the NYTimes has a good example. Data visualization or data visualisation is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data, meaning \"information that has been abstracted in some schematic form, including attributes or variables for the units of information\".[1]  A primary goal of data visualization is to communicate information clearly and efficiently via statistical graphics, plots and information graphics. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message.[2] Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.  Data visualization is both an art and a science. It is viewed as a branch of descriptive statistics by some, but also as a grounded theory development tool by others. The rate at which data is generated has increased. Data created by internet activity and an expanding number of sensors in the environment, such as satellites, are referred to as \"Big Data\". Processing, analyzing and communicating this data present a variety of ethical and analytical challenges for data visualization. The field of data science and practitioners called data scientists have emerged to help address this challenge.[3]  Contents      1 Overview     2 Characteristics of effective graphical displays     3 Quantitative messages     4 Visual perception and data visualization         4.1 Human perception/cognition and data visualization     5 History of Data Visualization     6 Terminology     7 Examples of diagrams used for data visualization     8 Other perspectives     9 Data presentation architecture         9.1 Objectives         9.2 Scope         9.3 Related fields     10 See also         10.1 People (Historical)             10.1.1 People (active today)     11 References     12 Further reading     13 External links  Overview Data visualization is one of the steps in analyzing data and presenting it to users.  Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in data analysis or data science. According to Friedman (2008) the \"main goal of data visualization is to communicate information clearly and effectively through graphical means. It doesn't mean that data visualization needs to look boring to be functional or extremely sophisticated to look beautiful. To convey ideas effectively, both aesthetic form and functionality need to go hand in hand, providing insights into a rather sparse and complex data set by communicating its key-aspects in a more intuitive way. Yet designers often fail to achieve a balance between form and function, creating gorgeous data visualizations which fail to serve their main purpose — to communicate information\".[4]  Indeed, Fernanda Viegas and Martin M. Wattenberg have suggested that an ideal visualization should not only communicate clearly, but stimulate viewer engagement and attention.[5]  Not limited to the communication of an information, a well-crafted data visualization is also a way to a better understanding of the data (in a data-driven research perspective),[6] as it helps uncover trends, realize insights, explore sources, and tell stories.[7]  Data visualization is closely related to information graphics, information visualization, scientific visualization, exploratory data analysis and statistical graphics. In the new millennium, data visualization has become an active area of research, teaching and development. According to Post et al. (2002), it has united scientific and information visualization.[8] Characteristics of effective graphical displays Charles Joseph Minard's 1869 diagram of Napoleon's March - an early example of an information graphic.  The greatest value of a picture is when it forces us to notice what we never expected to see. John Tukey[9]  Professor Edward Tufte explained that users of information displays are executing particular analytical tasks such as making comparisons or determining causality. The design principle of the information graphic should support the analytical task, showing the comparison or causality.[10]  In his 1983 book The Visual Display of Quantitative Information, Edward Tufte defines 'graphical displays' and principles for effective graphical display in the following passage: \"Excellence in statistical graphics consists of complex ideas communicated with clarity, precision and efficiency. Graphical displays should:      show the data     induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else     avoid distorting what the data has to say     present many numbers in a small space     make large data sets coherent     encourage the eye to compare different pieces of data     reveal the data at several levels of detail, from a broad overview to the fine structure     serve a reasonably clear purpose: description, exploration, tabulation or decoration     be closely integrated with the statistical and verbal descriptions of a data set.  Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations.\"[11]  For example, the Minard diagram shows the losses suffered by Napoleon's army in the 1812–1813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, direction of movement, and temperature. The line width illustrates a comparison (size of the army at points in time) while the temperature axis suggests a cause of the change in army size. This multivariate display on a two dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility. Tufte wrote in 1983 that: \"It may well be the best statistical graphic ever drawn.\"[11]  Not applying these principles may result in misleading graphs, which distort the message or support an erroneous conclusion. According to Tufte, chartjunk refers to extraneous interior decoration of the graphic that does not enhance the message, or gratuitous three dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of \"administrative debris.\" The ratio of \"data to ink\" should be maximized, erasing non-data ink where feasible.[11]  The Congressional Budget Office summarized several best practices for graphical displays in a June 2014 presentation. These included: a) Knowing your audience; b) Designing graphics that can stand alone outside the context of the report; and c) Designing graphics that communicate the key messages in the report.[12] Quantitative messages A time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time. A scatterplot illustrating negative correlation between two variables (inflation and unemployment) measured at points in time.  Author Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message:      Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.     Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.     Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.     Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.     Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.     Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.     Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.     Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.[2][13]  Analysts reviewing a set of data may consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis. Visual perception and data visualization  A human can distinguish differences in line length, shape orientation, and color (hue) readily without significant processing effort; these are referred to as \"pre-attentive attributes.\" For example, it may require significant time and effort (\"attentive processing\") to identify the number of times the digit \"5\" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.[14]  Effective graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).[14] Human perception/cognition and data visualization  There is a human side to data visualization. With the \"studying [of] human perception and cognition ...\" we are better able to understand the target of the data which we display.[15] Cognition refers to processes in human beings like perception, attention, learning, memory, thought, concept formation, reading, and problem solving.[16] The basis of data visualization evolved because as a picture is worth a thousand words, data displayed graphically allows for an easier comprehension of the information. Proper visualization provides a different approach to show potential connections, relationships, etc. which are not as obvious in non-visualized quantitative data. Visualization becomes a means of data exploration. Human brain neurons involve multiple functions but 2/3 of the brain's neurons are dedicated to vision.[17] With a well-developed sense of sight, analysis of data can be made on data, whether that data is quantitative or qualitative. Effective visualization follows from understanding the processes of human perception and being able to apply this to intuitive visualizations is important. Understanding how humans see and organize the world is critical to effectively communicating data to the reader. This leads to more intuitive designs. History of Data Visualization  There is a history of data visualization: beginning in the 2nd century C.E. with data arrangement into columns and rows and evolving to the initial quantitative representations in the 17th century.[15] According to the Interaction Design Foundation, French philosopher and mathematician René Descartes laid the ground work for Scotsman William Playfair. Descartes developed a two-dimensional coordinate system for displaying values, which in the late 18th century Playfair saw potential for graphical communication of quantitative data.[15] In the second half of the 20th century, Jacques Bertin used quantitative graphs to represent information \"intuitively, clearly, accurately, and efficiently\".[15] John Tukey and more notably Edward Tufte pushed the bounds of data visualization. Tukey with his new statistical approach: exploratory data analysis and Tufte with his book \"The Visual Display of Quantitative Information\", the path was paved for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand drawn visualizations and evolving into more technical applications – including interactive designs leading to software visualization.[18] Programs like SAS, SOFA, R, Minitab, and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility. Terminology  Data visualization involves specific terminology, some of which is derived from statistics. For example, author Stephen Few defines two types of data, which are used in combination to support a meaningful analysis or visualization:      Categorical: Text labels describing the nature of the data, such as \"Name\" or \"Age\". This term also covers qualitative (non-numerical) data.     Quantitative: Numerical measures, such as \"25\" to represent the age in years.  Two primary types of information displays are tables and graphs.      A table contains quantitative data organized into rows and columns with categorical labels. It is primarily used to look up specific values. In the example above, the table might have categorical column labels representing the name (a qualitative variable) and age (a quantitative variable), with each row of data representing one person (the sampled experimental unit or category subdivision).     A graph is primarily used to show relationships among data and portrays values encoded as visual objects (e.g., lines, bars, or points). Numerical values are displayed within an area delineated by one or more axes. These axes provide scales (quantitative and categorical) used to label and assign values to the visual objects. Many graphs are also referred to as charts.[19]  KPI Library has developed the \"Periodic Table of Visualization Methods ,\" an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound.[20] Examples of diagrams used for data visualization \tName \tVisual Dimensions \tExample Usages Bar chart of tips by day of week \tBar Chart      length/count     category     (color)        Comparison of values, such as sales performance for several persons or businesses in a single time period. For a single variable measured over time (trend) a line chart is preferable.  Histogram of housing prices \tHistogram      bin limits     count/length     (color)        Determining frequency of annual stock market percentage returns within particular ranges (bins) such as 0-10%, 11-20%, etc. The height of the bar represents the number of observations (years) with a return % in the range represented by the bin.  Basic scatterplot of two variables \tScatter plot      x position     y position     (symbol/glyph)     (color)     (size)        Determining the relationship (e.g., correlation) between unemployment (x) and inflation (y) for multiple time periods.  Scatter Plot \tScatter plot (3D)      position x     position y     position z     color   Network Analysis \tNetwork      nodes size     nodes color     ties thickness     ties color     spatialization        Finding clusters in the network (e.g. grouping Facebook friends into different clusters).     Determining the most influential nodes in the network (e.g. A company wants to target a small group of people on Twitter for a marketing campaign).  Streamgraph \tStreamgraph      width     color     time (flow)  Treemap \tTreemap      size     color        disk space by location / file type  Gantt Chart \tGantt chart      color     time (flow)        schedule / progress, e.g. in project planning  Heat Map \tHeat Map      row     column     cluster     color        Analyzing risk, with green, yellow and red representing low, medium, and high risk, respectively.  Other perspectives  There are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008) presented it. In this way Friendly (2008) presumes two main parts of data visualization: statistical graphics, and thematic cartography.[1] In this line the \"Data Visualization: Modern Approaches\" (2007) article gives an overview of seven subjects of data visualization:[21]      Articles & resources     Displaying connections     Displaying data     Displaying news     Displaying websites     Mind maps     Tools and services  All these subjects are closely related to graphic design and information representation.  On the other hand, from a computer science perspective, Frits H. Post (2002) categorized the field into a number of sub-fields:[8]  [22]      Information visualization     Interaction techniques and architectures     Modelling techniques     Multiresolution methods     Visualization algorithms and techniques     Volume visualization  Data presentation architecture A data visualization from social media  Data presentation architecture (DPA) is a skill-set that seeks to identify, locate, manipulate, format and present data in such a way as to optimally communicate meaning and proper knowledge.  Historically, the term data presentation architecture is attributed to Kelly Lautt:[23] \"Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of Business Intelligence. Data presentation architecture weds the science of numbers, data and statistics in discovering valuable information from data and making it usable, relevant and actionable with the arts of data visualization, communications, organizational psychology and change management in order to provide business intelligence solutions with the data scope, delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen (which is data visualization). Data visualization skills are one element of DPA.\" Objectives  DPA has two main objectives:      To use data to provide knowledge in the most efficient manner possible (minimize noise, complexity, and unnecessary data or detail given each audience's needs and roles)     To use data to provide knowledge in the most effective manner possible (provide relevant, timely and complete data to each audience member in a clear and understandable manner that conveys important meaning, is actionable and can affect understanding, behavior and decisions)  Scope  With the above objectives in mind, the actual work of data presentation architecture consists of:      Creating effective delivery mechanisms for each audience member depending on their role, tasks, locations and access to technology     Defining important meaning (relevant knowledge) that is needed by each audience member in each context     Determining the required periodicity of data updates (the currency of the data)     Determining the right timing for data presentation (when and how often the user needs to see the data)     Finding the right data (subject area, historical reach, breadth, level of detail, etc.)     Utilizing appropriate analysis, grouping, visualization, and other presentation formats  Related fields  DPA work shares commonalities with several other fields, including:      Business analysis in determining business goals, collecting requirements, mapping processes.     Business process improvement in that its goal is to improve and streamline actions and decisions in furtherance of business goals     Data visualization in that it uses well-established theories of visualization to add or highlight meaning or importance in data presentation.     Graphic or user design: As the term DPA is used, it falls just short of design in that it does not consider such detail as colour palates, styling, branding and other aesthetic concerns, unless these design elements are specifically required or beneficial for communication of meaning, impact, severity or other information of business value. For example:         choosing locations for various data presentation elements on a presentation page (such as in a company portal, in a report or on a web page) in order to convey hierarchy, priority, importance or a rational progression for the user is part of the DPA skill-set.         choosing to provide a specific colour in graphical elements that represent data of specific meaning or concern is part of the DPA skill-set     Information architecture, but information architecture's focus is on unstructured data and therefore excludes both analysis (in the statistical/data sense) and direct transformation of the actual content (data, for DPA) into new entities and combinations.     Solution architecture in determining the optimal detailed solution, including the scope of data to include, given the business goals     Statistical analysis or data analysis in that it creates information and knowledge out of data  See also      Analytics     Balanced scorecard     Business analysis     Business intelligence     Data analysis     Data profiling     Data warehouse     Exploratory data analysis     Infographic     Information architecture     Information design     Information visualization     Interaction design     Interaction techniques     Scientific visualization     Software visualization     Statistical analysis     Statistical graphics     Visual analytics  People (Historical)      Charles Joseph Minard     John Tukey     John Snow     Otto Neurath     Florence Nightingale     William Playfair  People (active today)      Alberto Cairo     Edward Tufte     Ola Rosling - Rosling developed the scatter-plot graphing tool used on Gapminder.org     .     Hans Rosling     Aaron Koblin     Manuel Lima     Max Roser - Roser is an economist at the University of Oxford and author of the online data visualisation publication Our World In Data.     Moritz Stefaner     Ben Shneiderman     Fernanda Viégas     Martin M. Wattenberg     Mona Chalabi - Data journalist at FiveThirtyEight. Previously at the Guardian, the Bank of England, and the Economist Intelligence Unit.     George Furnas     Branko Milanovic     Mike Bostock - Bostock is one of the key developers of the Javascript library d3.js.     Adrien Segal - Oakland, CA based artist known for her sculptures based on tidal and snow data.  Bigdata Platforms and Bigdata Analytics Software focuses on providing efficient analytics for extremely large datasets. These analytics helps the organisations to gain insight, by turning data into high quality information, providing deeper insights about the business situation. This enables the business to take advantage of the digital universe. IBM Bigdata Analytics, HP Bigdata , SAP Bigdata Analytics, Microsoft Bigdata, Oracle Bigdata Analytics, Talend Open Studio, Teradata Bigdata Analytics, SAS Big data, Dell Bigdata Analytics, HPCC System Big data, Palantir Bigdata, Pivotal Bigdata, Google BigQuery, Pentaho Big Data Analytics, Amazon Web Service, Cloudera Enterprise Bigdata, Hortonworks Data Platform, FICO Bigdata Analytics, Cisco Bigdata, Splunk Bigdata Analytics, Fusion-io Bigdata, Intel Bigdata, Mu Sigma Bigdata, MicroStrategy Bigdata , Opera Solutions Bigdata, Redhat Bigdata, Informatica Bigdata, MarkLogic Bigdata, Vmware Bigdata, Syncsort Bigdata, SGI Bigdata, MongoDB , Guavus Bigdata, Alteryx Bigdata, 1010data Advanced Analytics, Actian Analytics Platform, MapR, Tableau Software bigdata, QlikView Bigdata, Attivio’s Bigdata, DataStax Bigdata, Gooddata, Google Bigdata, Datameer, CSC Big Data Platform, Flytxt, Amdocs, Cisco Bigdata, Platfora and GE Bigdata are some of the Big data Analytics Platforms and Software in no particular order. 1. IBM Bigdata Analytics  IBM Bigdata Analytics solution portfolio  InfoSphere Streams , InfoSphere BigInsights , IBM Watson Explorer , IBM PureData powered by Netezza technology , DB2 with BLU Acceleration , IBM Smart Analytics System , InfoSphere Information Server and InfoSphere Master Data Management.2. HP Bigdata  HP’s Bigdata Analytics solution  HP HAVEn and HP Vertica. HP HAVEn is a platform comprised of software, services, and hardware. Big Data of any type either structured and unstructured can be analyzed to lead to powerful strategic insights. HP Vertica Dragline let organizations store their data in a cost effective manner, and provide capabilities to explore it quickly using SQL based tools  3. SAP Bigdata Analytics  SAP Bigdata Analytics platform  In Memory Platform called, SAP HANA, and SAP IQ, which is a column oriented, grid based, massively parallel processing database. There is also SAP HANA platform and Apache Hadoop solution available together. Bigdata Analytics solutions include the Predictive Analytics and Text Analytics solutions.     4. Microsoft Bigdata  Microsoft Azure is an open and flexible cloud platform which enables to quickly build, deploy and manage applications across a global network of Microsoft-managed datacenters. The applications can be  using any language, tool or framework and can integrated with other public cloud applications in the IT environment.    5. Oracle Bigdata Analytics  Oracle Bigdata Analytics solutions include Oracle Big Data Appliance, Oracle Exadata Database Machine and Oracle Exalytics In-Memory Machine. These are engineered Systems which are pre-integrated to reduce the cost and complexity of IT infrastructures. The database include Oracle Database, Oracle NoSQL Database, MySQL and MySQL Cluster, Oracle Event Processing, Oracle NoSQL Database and Oracle Coherence, Oracle Endeca Information  and in database analytics.     6. Talend Open Studio  Talend Open Studio is a versatile set of open source products for developing, testing, deploying and administrating data management and application integration projects. Talend delivers the only unified platform that makes data management and application integration easier by providing a unified environment for managing the entire lifecycle across enterprise boundaries. Talend’s products dramatically lower the adoption barrier for businesses wanting powerful packaged solutions to operational challenges like data cleansing, master data management, and enterprise service bus deployment. Leveraging and extending leading Apache technologies, Talend’s open source ESB and open source SOA solutions help organizations to build flexible, high-performance enterprise architectures that integrate and service-enable distributed applications.   7. Teradata Bigdata Analytics  Teradata has  a simple architecture called, the Unified Data Architecture in Bigdata Analytics. The Teradata Aster  Platform ease the  of crucial business insights from all data types. With its powerful analytic applications coupled with minimal time and effort requirements, it provides the  insights needed for sophisticated companies today.  8. SAS Bigdata Analytics  SAS Bigdata Analytics solution portfolio  Credit Scoring for SAS Enterprise Miner, SAS High-Performance Data Mining, SAS Model Manager, SAS Scoring Accelerator, SAS Text Miner and SAS Visual Statistics.  9. Dell Bigdata Analytics  Dell Bigdata Analytics  Kitenga Analytics Suite, Boomi AtomSphere and SharePlex Connector for Hadoop. Kitenga Analytics Suite provides you with integrated information modeling and visualization capabilities in a big data search and business analytics platform.  10. HPCC Systems Big data  HPCC Systems is an Open-source platform for Big Data analysis. The Data Refinery engine called Thor, clean, link, transform and analyze Big Data. Thor supports ETL (Extraction, Transformation and Loading) functions like ingesting unstructured/structured data out, data profiling, data hygiene, and data linking out of the box. The Data Delivery engine (Roxie) provides highly concurrent and low latency real time query capability. The Thor processed data can be accessed by a large number of users concurrently in real time fashion using the Roxie. The programming language, Enterprise Control Language (ECL), is used to program both the data processing jobs on Thor and the queries on Roxie.  HPCC Systems is an Open-source platform for Big Data analysis.  HPCC Systems is an Open-source platform for Big Data analysis. 11. Palantir Bigdata  Palantir Bigdata solution  Palantir Gotham to integrate, manage, secure, and analyze all of the enterprise data and Palantir Metropolis to ntegrate, enrich, model, and analyze any kind of quantitative data.  12. Pivotal Bigdata  Pivotal Big Data solutions help to discover insight from all data to build applications that serve customers in the context to store, manage, and deliver value from , massive data sets using the most disruptive set of enterprise data products such as MPP and column store databases, in-memory data processing, and Hadoop.  13. Google BigQuery  Google BigQuery is a web service that enables companies to analyze massive datasets using Google’s infrastructure . This can analyze up to billions of rows in seconds. It is scalable and easy to use with the the familiar SQL query language. BigQuery lets developers and businesses tap into powerful data analytics on demand against multi-terabyte datasets in seconds.  14. Pentaho Big Data Analytics  Pentaho Big Data Analytics s a comprehensive and unified solution that supports the entire big data lifecycle. Regardless of the data source, within a single platform the solution provides visual big data analytics tools to extract and prepare the data plus the visualizations and analytics. The Open, standards based architecture, make it easy to integrate with or extend existing infrastructure.  15. Amazon Web Service  Amazon Web Services provides cloud based analytics services to help you process and analyze any volume of data, whether your need is for managed Hadoop clusters, real-time streaming data, petabyte scale data warehousing, or orchestration.  16. Cloudera Enterprise Bigdata  Cloudera Enterprise  CDH, the open source Hadoop-based platform, as well as advanced system management and data management tools plus dedicated support and community advocacy .  17. Hortonworks Data Platform  HDP is a platform for multi-workload data processing across an array of processing methods – from batch through interactive to real-time – all supported with solutions for governance, integration, security and operations.  18. FICO Bigdata Analytics  FICO s comprehensive Big Data Analytics software solutions, Predictive Analytics and Business Intelligence tools  FICO Data Orchestrator, FICO Decision Management Platform, FICO Decision Optimizer, FICO Model Builder, FICO Model Central Solution, FICO Predictive Analytics and FICO Solution Stack.  19. Cisco Bigdata  Cisco UCS Common Platform Architecture (CPA) for big data  computing, storage, connectivity, and unified management capabilities. Unique to this architecture are transparent, simplified data and management integration with an enterprise application ecosystem.  20. Splunk Bigdata Analytics  Splunk s a portfolio of Bigdata Analytics software such as Hunk: Splunk Analytics for Hadoop, NoSQL Data Stores, Splunk Hadoop Connect, Hadoop Management and Splunk DB Connect.  21. Fusion-io Bigdata  Fusion-io solutions eliminate the random workload performance deficiencies common to MongoDB, Cassandra and NoSQL databases, such as HBASE, while reducing the operational overhead of their conventional scale out architectures. Fusion based solutions deliver predictable and consistently high performance across the entire database, resulting in a more efficient overall system that can require fewer nodes, less DRAM, and use less energy for power and cooling.  22. Intel Bigdata  Intel portfolio  technology products such as Intel Xeon processors, 10 Gigabit server adapters, SSDs, and the Intel Distribution improve performance for big data projects.  23. Mu Sigma Bigdata  Mu Sigma’s platforms for Data Sciences include muXo, muHPC and muText. muXo is an advanced decision optimization engine  to solve complex business problems. It provides a suite of constantly evolving, cutting-edge meta-heuristic algorithms. muHPC is a suite of popular statistical algorithms, integrated in the form of R packages, for Big Data analysis. Written in MapReduce, muHPCTM algorithms leverage the power of parallel computation. Mu Sigma’s text mining engine enables knowledge  from unstructured and semi-structured data .  24. MicroStrategy Bigdata  MicroStrategy Bigdata solution called PRIME, which is deployed on the Cloud, provides visualization and dashboarding engine with an innovative massively parallel in-memory data store. This architecture allows companies to rapidly build and deploy powerful information-driven apps that deliver analytics to hundreds of thousands of users in a fraction of the time and cost of other approaches.  25. Opera Solutions Bigdata  Opera Solutions Bigdata solution Vektor Big Data analytics and Signal-processing platform integrates Big Data flows from both inside and outside the enterprise; provides the technology to identify, extract, and store Signals; and supports deployment of all Signal Apps.  26. Redhat Bigdata  Majority of big data implementations run on Linux. Red Hat Enterprise Linux is a leading platform for big data deployments. Red Hat Enterprise Linux excels in distributed architectures and  features that  critical big data needs. Managing tremendous data volumes and intensive analytic processing requires an infrastructure  for high performance, reliability, fine-grained resource management, and scale-out storage.  27. Informatica Bigdata  Informatica PowerCenter Big Data Edition provides a safe, efficient way to integrate all types of data on Hadoop at any scale without having to learn Hadoop.  28. MarkLogic Bigdata  MarkLogic Bigdata solution the Enterprise NoSQL database, brings all the features into one unified system: a document-centric, schema-agnostic, structure-aware, clustered, transactional, secure, database server with -in search and a full suite of application services.  29. Vmware Bigdata  vSphere is a robust, high-performance virtualization layer that abstracts server hardware resources and makes them shareable by multiple virtual machines. Runs Hadoop workloads on vSphere to achieve higher utilization, reliability and agility.  30. Syncsort Bigdata  Syncsort Hadoop Solutions helps on the challenges of collecting, processing and integrating data in Hadoop. It remove barriers for wider Hadoop adoption: connect, develop, deploy, re-use, and accelerate. No programming or tuning are required.  31. SGI Bigdata  SGI InfiniteData Cluster s the compute platform for Hadoop Solutions with cluster installations now reaching tens of thousands of nodes.SGI UV s the solution with the industry’s most powerful shared memory platform to find hidden data relationships or perform real-time analysis.  32. MongoDB  MongoDB is the leading NoSQL database, empowering businesses to be more agile and scalable. Fortune 500 companies and startups alike are using MongoDB to create new types of applications, improve customer experience, accelerate time to market and reduce costs.  33. Guavus Bigdata  The Guavus Reflex platform is capable of creating actionable information from widely distributed, high volume data streams in near real-time. Reflex uses highly optimized computational algorithms and machine learning to distill actionable insights from very large datasets.  34. Alteryx Bigdata  Alteryx Bigdata solution access, integration, and cleaning of sources of data as varied as Hadoop ( Cloudera & MapR) or NoSQL (MongoDB) and Excel or Teradata with predictive and spatial tools, combined in a simple, workflow design environment.  35. 1010data Advanced Analytics  The 1010data analytics platform  advanced, -in analytic functions such as Statistics (distribution analysis, correlation, variance),Predictive modeling and forecasting (linear and multivariate regression, logistic regression), Machine learning (clustering analysis, Markov chains for Monte Carlo simulations, principal component analysis). These functions are integrated directly into the system, so they run incredibly quickly on large volumes of data  36. Actian Analytics Platform  Actian Analytics Platform deliver the next generation analytics in three editions- Extreme Performance Edition, Hadoop SQL Edition, Cloud Edition.Extreme Performance Edition accelerates the analytics value chain from connecting to massive amounts of raw big data all the way to delivering actionable business value from sophisticated analytics. Hadoop SQL Edition accelerates Hadoop and makes it enterprise-grade by providing high-performance data enrichment, visual design and SQL analytics on Hadoop without the need for MapReduce skills. Cloud Edition integrates cloud and on-premises applications while providing robust data quality and other data services.  37. MapR  The MapR Distribution for Apache Hadoop provides organizations with an enterprise grade distributed data platform to reliably store and process big data. MapR packages a broad set of Apache open source ecosystem projects enabling batch, interactive, or real time applications.  38. Tableau Software bigdata  Tableau Software bigdata solutions connect to any data, anytime and anywhere, regardless of its size and complexity or mix of unstructured and structured data with the technologies like Google BigQuery and a variety of Hadoop flavors.  39. QlikView Bigdata  QlikView s two approaches to handling Big Data, both deliver the same great user experience. Either with QlikView’s 100% In-Memory Architecture or QlikView Direct , which is a hybrid approach that leverages both in-memory data and data that is dynamically queried from an external source.  40. Attivio’s Bigdata  Attivio’s Active Intelligence Engine combines Big Data and Big Content,  Hadoop. Universal indexing and automatic ad hoc JOIN of all information matching a given query, without costly data modeling and with full security. There is also Advanced text analytics that adds context and signals from human-generated information sources and support for business intelligence/data visualization tools .  41. DataStax Bigdata  DataStax Enterprise (DSE), which is  on Apache Cassandra, delivers what Internet Enterprises need to compete in today. With in-memory computing capabilities, enterprise-level security,  and powerful integrated analytics and enterprise search, visual management, and expert support, DataStax Enterprise is the leading distributed database choice for online applications that require  performance with no downtime.  42. Gooddata  The GoodData Platform is a portfolio of tools, APIs and frameworks, which makes the key components of a BI solution to collect, store, combine, analyze, and visualize. These were  to exist in the cloud and be delivered as an end-to-end service.  GoodData  GoodData 43. Google Bigdata  Google Cloud Platform surfaces the same analytical engines invented and used by Google for nearly two decades to help unearth insight in your business and operational environment. Google Cloud Platform leads the industry in the ability to let you analyze data at the scale of the entire web, with the familiarity of SQL and in a fully managed, serverless architecture where backend infrastructure is fully handled on your behalf.The big data analytics products are able to scale automatically while you focus only on the business insight you want to uncover.  44. Datameer  Datameer Professional, is a SaaS big data analytics platform targeted for department specific deployments. Datameer ing features leading Hadoop cloud providers Altiscale and Bigstep. Datameer simplifies the big data analytics environment into a single application on top of the powerful Hadoop platform.  45. CSC Big Data Platform  CSC Big Data Platform as a Service (BDPaaS) helps enterprises leap past these hurdles and get value from their data much more quickly. With BDPaaS, enterprises can rapidly develop, secure and deploy next-generation big data and analytics applications with a centralized, subscription-based platform that uses leading analytics tools, infrastructure and software.  46. Flytxt Big Data Analytics platform  Flytxt’s Big Data Analytics platform is  to integrate  Data, Big Data for deriving deeper actionable insights. It follows a hybrid architecture combining scale out clusters running Hadoop with traditional RDBMS as a metadata store and an in-memory database for Real time transactional data processing.  47. Amdocs Insight Big Data Analytics Platform  Amdocs Insight Big Data Analytics Platform supports a wide variety of Amdocs analytical applications and data services to facilitate new revenues, drive business efficiency and enhance the customer experience.  48. Cisco Bigdata  Cisco provide integrated infrastructures and analytics to support our big data partner ecosystem. Cisco UCS Integrated Infrastructure for Big Data architecture provides a secure and scalable infrastructure. Cisco is bringing the computing and analytics to the data to take advantage of the valuable insight that it reveals.  49. Platfora   on Hadoop, Spark and native cloud APIs, Platfora’s technology helps it fit in just about anywhere  your existing analytics ecosystem, hardware and BI tools.  50. GE Bigdata  The Industrial Internet co ordinate multiple industrial applications to work intelligently in order to optimize entire operational environments.  big-data-analytics  Data analysis is nothing new. Even before computers were used, information gained in the course of business or other activities was reviewed with the aim of making those processes more efficient and more profitable. These were, of course, comparatively small-scale undertakings given the limitations posed by resources and manpower; analysis had to be manual and was slow by modern standards, but it was still worthwhile. Opinion polling, for example, has been carried out since early in the 19th century, almost 200 years ago. The first national survey took place in 1916 and involved the publication Literary Digest sending out millions of postcards and counting the returns. As a result, they correctly predicted Woodrow Wilson’s election as president.  Since then, volumes of data have grown exponentially. The advent of the internet and er computing has meant that huge quantities of information can now be harvested and used to optimise business processes. The problem is that conventional methods were simply not suited to crunching through all the numbers and making sense of them. The amount of information is phenomenal, and within that information lies insights that can be extremely beneficial. Once patterns are identified, they can be used to adjust business practices, create targeted campaigns and discard ones that are not effective. However, as well as large amounts of storage, it takes specialised software to be able to make sense of all this data in a useful way.  ‘Big Data’ is the emerging discipline of capturing, storing, processing, analysing and visualising these huge quantities of information. The data sets may start at a few terabytes and run to many petabytes – far more than traditional data analysis packages can handle. In 2012 Gartner defined it as, ‘high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight  and process optimization.’ This ‘3V’ classification has been  on since (particularly with the addition of veracity), such that Big Data is often described in terms of the following characteristics:          Volume. Terabytes or petabytes of data are analysed. An estimated 2.5 quintillion bytes of data (2.5 trillion gigabytes) are created every day, an amount which will only rise in the future. However, the size of the dataset is not the only variable that characterises Big Data.         Variety. The dataset may contain many different forms of data – not simply a large amount of the same type. The profusion of different kinds of mobile device and the variety of content consumed on them on a wide range of platforms, for example, means that companies can harvest data from an enormous array of sources, each telling them a different part of the same picture.         Velocity. Data may change on a constant basis. For example, modern cars may have 100 or so different sensors that continually monitor different aspects of performance. Markets change on a moment-to-moment scale. Data is highly fluid, and snapshots are not always enough.         Veracity. The data acquired may not all be accurate, or much of it may be uncertain or provisional in nature. Data quality is unreliable, especially when there is so much of it. Any system of analysis must take this into account.  In addition to the 4V characteristics, there are also two others to deal with:      Variability. Data capture and volume may be inconsistent, not just inaccurate, so varying quantities and qualities of data will be acquired at different times.     Together, these factors mean that managing the data can be an extremely complex process, since there are many data sources with differing types and formats of data, but these need to be correlated and made sense of if they are to be useful.  big-data-will-drive-the-next-phase-of-innovation-in-mobile-computing  Big Data companies  Due to the nature of Big Data, specialist companies have grown up around it in order to manage the volumes and complexity of information involved.  ibm-bigdata-mobile-header  IBM Big Data Analytics  Like many other big data companies, IBM builds its ings on Hadoop – so it’s , affordable and open source. It allows businesses to capture, manage and analyse structured and unstructured data with its BigInsights product. This is also available on the cloud (BigInsights on Cloud) to give the benefits of outsourcing storage and processing, providing Hadoop as a service. InfoSphere Streams is  to enable capture and analysis of data in realtime for Internet-of-Things applications. IBM’s analytics enable powerful collating and visualisation of data with excellent flexibility for storage and management. You can also find plenty of downloadable documentation and white papers on their site.  hp4750-540x334  HP Big Data  Another well-known name in IT, HP brings a wealth of experience to big data. As well as ing their own platform, they run workshops to assess organisations’ needs. Then, ‘when you’re ready to transform your infrastructure, HP can help you develop an IT architecture that provides the capacity to manage the volume, velocity, variety, voracity, and value of your data.’ The platform itself is based on Hadoop. HP look to add value beyond providing the software alone, and will consult with you to help you craft a strategy to help you make the most of the big data you collect – and how to go about it most efficiently.  bdpmicrobd  Microsoft  Microsoft’s big data solutions run on Hadoop and can be used either in the cloud or natively on Windows. Business users can use Hadoop to gain insights into their data using standard tools  Excel or Office 365. It can be integrated with core databases to analyse both structured and unstructured data and create sophisticated 3D visualisations. Polybase is incorporated so users can then easily query and combine relational and non-relational data with the same techniques required for SQL Server. Microsoft’s solution enables you to analyse Hadoop data from within Excel, adding new functionality to a familiar software package.  30680129.cms  Intel Big Data  Recognising that making the most of big data means changing your information architecture, Intel takes the approach of enabling enterprise to create a more flexible, open and distributed environment, whilst their big data platform is based on Apache’s Hadoop. They take a thorough approach that does not assume they know what your needs are, but presents a walkthrough to determine how best to help achieve your objectives. Intel’s own industry-standard hardware is at your disposal to optimise the performance of your big data project, ing speed, scalability and a cost-effective approach according to your organisation’s requirements.  amazonwebservices-100014921-orig  Amazon Web Services  Amazon is a huge name in providing web hosting and other services, and the benefits of using them are unparalleled economies of scale and uptime. Amazon tend to  a basic framework for customers to use, without providing much in the way of customer support. This means they are the ideal choice if you know exactly what you are doing and want to save money. Amazon supports products like Hadoop, Pig, Hive and Spark, enabling you to build your own solution on their platform and create your own big data stack. There are plenty of tutorials, video demos and guides to get you started as quickly and easily as possible.  dell-software-logo  Dell Big Data Analytics  Another well known and globally-established company, this time in the hardware space, Dell s its own big data package. Their solution  an automated facility to load and continuously replicate changes from an Oracle database to a Hadoop cluster to support big data analytics projects, thereby simplifying Oracle and Hadoop data integration. Data can be integrated in near real-time, from a wide range of data stores and applications, and from both on- and off-premises sources. Techniques such as natural language processing, machine learning and sentiment analysis are made accessible through straightforward search and powerful visualisation to enable users to learn relationships between different data streams and leverage these for their businesses.  Teradata-Logo-620x265  Teradata  Teradata call their big data product a ‘data warehouse system’, which stores and manages data. The different server nodes share nothing, having their own memory and processing power, and each new node increases storage capacity. The database sits over these and the workload is shared among them. The company started taking an interest in big data in 2010, adding analytics for text documents,  unstructured data and semi-structured data (e.g. word processor documents and spreadsheets). They also work with unstructured data gathered from online interactions.  bigquery_0  Google BigQuery  Google is the big daddy of internet search: the outright market leader with the vast majority of search traffic to its name. No other search engine comes close, so perhaps it’s not surprising that Google should  an analytics package to crunch through the phenomenal amount of data it produces in the course of its day-to-day work for millions of businesses around the world. It already hosts the hugely popular Google Analytics, but BigQuery is  for a different order of magnitude of data. It puts Google’s impressive infrastructure at your disposal, allowing you to analyse massive datasets in the cloud with , SQL-like queries – analysing multi-terabyte datasets in just seconds. Being Google it’s also very scalable and straightforward to use.  vmware-logo  VMware Big Data  VMware is well-known in the world of best cloud storage and IaaS. Their big data solutions use their established vSphere product to virtualise Hadoop whilst maintaining excellent performance.  and elastic scaling is possible due to an approach that separates out storage from computing, keeping data safe and persistent, enabling greater efficiency and flexibility. Essentially this is a sophisticated and safe approach to Hadoop-as-a-service, which utilises many of VMware’s strengths to deliver a big data platform reliably and in a cost-effective way.  red-hat-1-large  Redhat  As might be expected, Redhat take an open source approach to big data, believing that changing workloads and technologies require an open approach. They take a modular approach so that the building blocks of their platform work interoperably with other elements of your data centre. Building blocks include Platform-as-a-Service (PaaS), so you can develop apps er, process data in real time, and easily integrate systems; Infrastructure-as-a-Service (IaaS), to enable deployment and management of service providers, tools, and components of IT architecture across platforms and technology stacks in a consistent, unified way; Middleware, integration and automation, to streamline data sources and interaction; and Storage, of the most appropriate kind for the task in hand.  it_photo_102724  Tableau Software  Tableau s significant flexibility over how you work with data. Using Tableau’s own servers and Desktop visualisation with your existing big data storage makes it a versatile and powerful system. There are two options: connecting to your data live, or bringing it into memory for  response queries. Memory management means all laptop/PC memory is used, down to the hard disk, to maintain speed and performance, even at large scale. Tableau supports more than 30 databases and formats, and is easy to connect to and manage. Multi-million row tables can be visually analysed directly on the database itself, extremely quickly.  informatica-cloud  Informatica Big Data  Another provider that builds its platform on Hadoop, Informatica has several options that make life easy by giving you access to the functionality and allow you to integrate all types of data efficiently without having to learn Hadoop itself. Informatica Big Data Edition uses a visual development environment to save time and improve accessibility (Informatica claims this makes it approximately five times er than hand-coding a solution). This also has the advantage of not needing to hire dedicated Hadoop experts, since there are more than 100,000 Informatica experts worldwide. This makes for a fantastically versatile solution that is still simple enough to be used without intensive training.  splunk-logo  Splunk  Splunk collects and analyses machine data as it comes in. Realtime alerts are used to spot trends and identify patterns as they occur. It’s extremely easy to deploy and use, and highly scalable: ‘from a single server to multiple datacenters.’ There is also a strong emphasis on security, with role-based access controls and auditability. Splunk is  for Hadoop and NoSQL data stores to enable analysis and visualisation of unstructured data. There’s also a community forum and online support centre, should you need assistance getting set up or figuring out how things work.  datastax_logo_blue  DataStax Big Data  DataStax big data solution is  on Apache Cassandra, an open source and enterprise-ready platform that is commercially supported. It is used by a number of the world’s most innovative and best-known companies, such as Netflix and eBay. Their chief product, DataStax Enterprise, leverages Cassandra’s properties to give vast scalability, continuous availability and strong security. The combination of commercial software and open source platform means that it’s  and low-cost compared to many other options on the market. It’s also relatively easy to run. DataStax boast that their product ‘enables you to perform real-time transactions with Cassandra, analytics with Apache Hadoop and enterprise search with Apache Solr, in a single, smartly integrated big data platform that works across multiple datacenters and the cloud.  MongoDB_Logo_Full  MongoDB  ‘Mongo’ comes from ‘humongous’ and takes a different approach to normal, using JSON-like documents instead of table-based relational database structures. This allows it to integrate certain types of data er and more easily. Is it free and open-source software, released under a combination of the GNU Affero General Public License and the Apache License. Mongo has been adopted by a number of well-known and very large websites, such as Craigslist, eBay and the New York Times. Mongo’s analytics are  to scale and are  into the operational database, meaning you have access to them in realtime.  gooddata_vertical_black-1  Gooddata  Gooddata is an all-in-one cloud analytics platform. They have a wide range of customers,  HP and Nestle. Operating fully in the cloud, Gooddata manage hosting, data and technology, meaning that the customer is able to focus completely on the analytics. They are recognised as industry leaders, with a number of awards to their name,  from Gartner. There’s an emphasis on usability, with interactive dashboards that facilitate collaboration by team-members as well as visual data , so that teams can move quickly on insights gained. The responsive UI is  to be easy to use on any device or platform,  mobile devices.  qlikview_logo_large  QlikView  QlikView s two big data solutions, enabling users to switch between them as the require. Their In-Memory architecture uses a patented data engine to compress data by a factor of 10, so that up to 2 TB can be stored on a 256 GB RAM server. This s exceptional performance, and other features further enhance response rates and make exploring very large data sets extremely . This is used by many of Qlik’s customers to analyse volumes of data stored in data warehouses or Hadoop clusters. This hybrid approach means big data can be made accessible to users without knowledge of programming. It also allows a highly focused and granular view of data when required.  Attivio-Small-PR-trim  Attivio  Attivio’s Active Intelligence Engine (AIE) brings together a number of separate capabilities – business intelligence, enterprise search, business analytics, data warehousing and process automation – to produce comprehensive information, presented in a user-friendly way. AIE puts together both structured and unstructured data into one index to be searched, collated and analysed; regular search queries and SQL can be used and a wide range of queries are therefore possible, from broad to highly focused. It can be integrated with a large number of data sources by giving it access with other software applications. It uses proprietary, patented technology, unlike many of its open-source-based rivals.  img15  1010data Advanced Analytics  1010data s a complete suite of products, enabling companies to engage with the data they harvest in their everyday business. Data is analysed on the same platform on which it is stored, minimising delays from moving data. This enables  responses to changing market information and an agile approach that reacts in near-realtime. There is ‘immediate, direct, unfettered access to all relevant data, even voluminous, granular, raw data’. 1010’s platform can be implemented on the cloud, so that anyone with the correct access rights can use it from anywhere in the world. The company s an ‘Analytical Platform as a Service’ (APaaS) approach that gives enterprise-grade cloud security, reliability, and interoperability, along with cost-effective, on-demand performance and storage scalability.  Actian-logo  Actian  Actian’s Vortex is  on Apache Hadoop, an open source framework written in Java for distributed storage and processing of very large data sets. This means that Actian’s big data solutions will always be open themselves, so that customers are not locked into a proprietary platform. They claim their software is , despite the large size of the datasets they deal with. Whilst Hadoop is complex, Actian’s platform is far more straightforward to use, making it enterprise ready and emphasising security and scalability. It gives full SQL support to your data. Actian is used by thousands of big-name customers worldwide,  Nikon, China Telecom and GE Transportation.  Conclusion  Big data isn’t just an emerging phenomenon. It’s already here and being used by major companies to drive their business forwards. Traditional analytics packages simply aren’t capable of dealing with the quantity, variety and changeability of data that can now be harvested from diverse sources – machine sensors, text documents, structured and unstructured data, social media and more. When these are combined and analysed as a whole, new patterns emerge. The right big data package will allow enterprises to track these trends in real time, spotting them as they occur and enabling businesses to leverage the insights provided.  However, not all big data platforms and software are alike. As ever, which you decide on will depend on a number of factors. These include not just the nature of the data you are working with, but organisational budgets, infrastructure and the skillset of your team, amongst other things. Some solutions are  to be used off-the-peg, providing powerful visualisations and connecting easily to your data stores. Others are intended to be more flexible but should only be used by those with coding expertise. You should also think to the future, and the long-term implications of being tied to your platform of choice – particularly in terms of open-source vs proprietary software.  Big data refers to massive, heterogeneous, and often unstructured digital content that is difficult to process using traditional data management tools and techniques. The term encompasses Published by the IEEE Computer Society the complexity and variety of data and data types, real-time data collection and processing needs, and the value that can be obtained by smart analytics. Advanced data mining techniques and associated tools can help extract information from large, complex datasets that is useful in making informed decisions in many business and scientific applications  tax payment collection, market sales, social studies, biosciences, and high- energy physics. Combining big data analytics and knowledge techniques with scalable computing systems will produce new insights in a shorter time. Although few cloud-based analytics platforms are available today, current research work anticipates that they will become common within a few years. Some current solutions are based on open 0018-9162/13/$31.00 © 2013 IEEEsource systems such as Apache Hadoop and SciDB, while others are proprietary solutions provided by companies such as Google, IBM, EMC, BigML, Splunk Storm, Kognitio, and InsightsOne. As more such platforms emerge, researchers will port increasingly powerful data mining programming tools and strategies to the cloud to exploit complex and flexible software models such as the distributed workflow paradigm. The growing use of service-oriented computing could accelerate this trend (http://tinyurl.com/d26o2j5). DATA ANALYTICS SERVICE MODELS Developers and researchers can adopt the software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS) models to implement big data analytics solutions in the cloud. The SaaS model s complete big data analytics applications to end users, who can exploit the cloud’s scalability in both data storage and processing power to execute analysis on large or complex datasets. The PaaS model provides data analytics programming suites and environments in which data mining developers can design scalable analytics services and applications. Researchers can exploit the IaaS model to compose a set of virtualized hardware and software resources for running data analysis frameworks or applications. Column Contributions W e welcome short articles (1,500 to 2,000 words) for publication in the Cloud Cover column that  the questions outlined in Computer’s January 2013 issue (S. Murugesan, As Table 1 shows, developers can implement big data analytics services within each of these three models: •\t data analytics software as a ser- vice—provides a well-defined data mining algorithm or ready- to-use knowledge  tool as an Internet service to end users, who can access it directly through a Web browser; •\t data analytics platform as a ser- vice—provides a supporting platform that developers can use to build their own data ana- lytics applications or extend existing ones without concern about the underlying infrastruc- ture or distributed computing issues; and •\t data analytics infrastructure as a service—provides a set of virtualized resources that devel- opers can use as a computing infrastructure to run data mining applications or to imple- ment data analytics systems from scratch. End users whose goal is to per- form complex data analysis can “Cloud Computing: The New Normal?,” pp. 77-79). Submit your ideas for advancing the technology or share your experiences in harnessing the cloud at cloudcover@computer.org. apply the recently implemented Data Mining Cloud Framework (http:// tinyurl.com/c4b4f5k) as a high-level PaaS programming environment and create a set of SaaS suites for big data analytics. With this approach, users need not be concerned about cloud platform or application pro- gramming details. BIG DATA ANALYTICS WORKFLOWS Developers can use workflows, which consist of complex graphs of many concurrent tasks, to  the complexity of scientific and business applications. This approach supports data analytics design by providing a paradigm that encompasses all the steps of data analytics, from data access and filtering to data mining and sharing produced knowledge. Workflow-based data mining frameworks that run on cloud platforms and use a service- oriented approach  a flexible programming model, distributed task interoperability, and execution scalability that reduces data analytics completion time. Application developers can design Table 1. Cloud-based data analytics services. Cloud service model Features Users Data analytics software as a service A single and complete data mining application or task ( data sources) ed as a service End users, analytics managers, data analysts Data analytics platform as a service A data analysis suite or framework for programming or developing high-level applications, hiding the cloud infra- structure and data storage Data mining application developers, data scientists Data analytics infrastructure as a service A set of virtualized resources provided to a programmer or data mining researcher for developing, configuring, and running data analysis frameworks or applications Data mining programmers, data management developers, data mining researchers  MAY 2013 99Clo ud C ov er Figure 1. Data analysis workflow application  using the Data Mining Cloud Framework’s graphical programming interface. data analysis tasks, scientific computation methods, and complex simulation techniques as workflows that integrate single Web services and execute them concurrently on virtual machines in the cloud. Figure 1 shows a data analysis workflow application  using the Data Mining Cloud Framework’s graphical programming interface recently developed in our laboratory (http://tinyurl.com/crnork2). Data sources and tools such as data mining algorithms, filters, and data splitters are connected through direct edges that define specific dependency relationships among them. When creating an edge between two nodes, the system automatically attaches a label to it that represents the relationship between them. To ease workflow composition and allow users to monitor its execution, each resource icon has an associated tag—the checkmarks in Figure 1—representing the status of a corresponding resource. The experimental results of a set of studies using the framework to analyze genomics, network intrusion, and bioinformatics data demonstrated its effectiveness, as well as the linear scalability achieved through concurrent execution of the workflow tasks on a pool of virtual servers (http://tinyurl. com/c4b4f5k). Current research focuses on the workflow composition interface, with the aim of extending supported design patterns such as conditional branches and iterations and evaluating its functionality and Cloud Computing Special Technical Community T he CS Cloud Computing Special Technical Community (CS CCSTC) focuses on cloud activities across the IEEE Computer Society, involving both CS members and nonmembers. The STC’s work is complementary \t100 computer to the IEEE Cloud Computing Initiative (IEEE CCI), a three-year project to promote cloud efforts across IEEE. For details or to join the STC, visit www.computer.org/cc. performance during the design and execution of complex data mining workflows on large datasets in the cloud. RESEARCH RECOMMENDATIONS Cloud-based data analytics requires high-level, easy-to-use design tools for programming large applications dealing with huge, distributed data sources. This necessitates further research and development in several key areas. •\t Programming abstracts for big data analytics. Big data analyt- ics programming tools require novel complex abstract struc- tures. The MapReduce model is often used on clusters and clouds, but more research is needed to develop scalable higher-level models and tools. •\t Data and tool interoperability and openness. Interoperability is a main issue in large-scale applications that use resources such as data and computing nodes. Standard formats and models are needed to support interoperability and ease co- operation among teams usingdifferent data formats and tools. •\t Integration of big data analytics frameworks. The service- oriented paradigm allows run- ning large-scale distributed workflows on heterogeneous platforms along with software components developed using different programming lan- guages or tools. The Web and cloud services paradigms can help manage worldwide integra- tion of multiple data analytics frameworks. •\t Data provenance and annota- tion mechanisms. Provenance is captured as a set of dependencies between elements that researchers can use to interpret data and provide reproducible analysis. Research is needed to develop innovative techniques for visualizing and mining provenance data. These solutions, together with others ing data privacy and security concerns, will promote cloud-based data analytics in large companies, and eventually will benefit users such as independent research teams, start-ups, and small enterprises that aren’t deeply skilled in cloud programming and management. Advancing the cloud from a computation and data management infrastructure to a pervasive and scalable data analytics platform", "category": "Edison", "id": 128}
{"skillName": "DSDK03", "skillText": "Strategic financial management [1] refers to study of finance with a long term view considering the strategic goals of the enterprise. Financial management is nowadays increasingly referred to as \"Strategic Financial Management\" so as to give it an increased frame of reference.  To understand what strategic financial management is about, we must first understand what is meant by the term \"Strategic\". Which is something that is done as part of a plan that is meant to achieve a particular purpose.  Therefore, Strategic Financial Management are those aspect of the overall plan of the organisation that concerns financial managers. This includes different parts of the business plan, for example marketing and sales plan, production plan, personnel plan, capital expenditure, etc. These all have financial implications for the financial managers of an organisation.  The objective of the Financial Management is the maximisation of shareholders wealth. To satisfy this objective a company requires a \"long term course of action\" and this is where strategy fits in.  Contents      1 Strategic planning         1.1 Component of a financial strategy     2 Role of a financial manager     3 Decision making         3.1 Investment decisions             3.1.1 Long term assets - Capital Budgeting investment decisions             3.1.2 Short term assets investment decisions             3.1.3 Profitability management             3.1.4 Evaluation         3.2 Financing decisions             3.2.1 Decision making         3.3 Liquidity/ Working capital Decisions             3.3.1 Receivables management             3.3.2 Inventory management             3.3.3 Cash management         3.4 Dividend decisions     4 Strategic Financial Management tasks/ services provided     5 References  Strategic planning  Strategic planning is an organisation’s process to outlining and defining its strategy, direction it is going. This led to decision making and allocation of resources inline with this strategy. Some techniques used in strategic planning includes: SWOT analysis, PEST analysis, STEER analysis. Often it is a plan for one year but more typically 3 to 5 years if a longer term view is taken. Component of a financial strategy  When making a financial strategy, financial managers need to include the following basic elements. More elements could be added, depending on the size and industry of the project.  Startup cost: For new business ventures and those started by existing companies. Could include new fabricating equipment costs, new packaging costs, marketing plan.  Competitive analysis: analysis on how the competition will affect your revenues.  Ongoing costs: Includes labour, materials, equipment maintenance, shipping and facilities costs. Needs to be broken down into monthly numbers and subtracted from the revenue forecast (see below).  Revenue forecast: over the length of the project, to determine how much will be available to pay the ongoing cost and if the project will be profitable. Role of a financial manager  Broadly speaking, financial managers have to have decisions regarding 4 main topics within a company. Those are as follow:      Investment decisions - Regarding the long and short term investment decisions. For example: the most appropriate level and mix of assets a company should hold.     Financing decisions - concerns the optimal levels of each financing source - E.g. Debt - Equity ratio.     Liquidity decisions - Involves the current assets and liabilities of the company - one function is to maintain cash reserves.     Dividend decisions - Disbursement of dividend to shareholders and retained earnings.  Decision making  Each decisions made by financial managers must be strategic sound and not only have benefits financially (e.g. Increasing value on the Discounted Cash Flow Analysis) but must also consider uncertain, unquantifiable factors which could be strategically beneficial.  To explain this further, a proposal could have a negative impact from the Discounted Cash Flow analysis, but if it is strategically beneficial to the company this decision will be accepted by the financial managers over a decision which has a positive impact on the Discounted Cash Flow analysis but is not strategically beneficial. Investment decisions  For a financial manager in an organisation this will be mainly regarding the selection of assets which funds from the firm will be invested in. These assets will be acquired if they are proven to be strategically sound and assets are classified into 2 classifications:      Long term assets - also known as Capital Budgeting for financial managers.     Short term assets/current assets.     Profitability Management.  Long term assets - Capital Budgeting investment decisions  Financial managers in this field must select assets or an investment proposals which provides a beneficial course of action, that will most likely come in the future and over the lifetime of the project. This is one of the most crucial financial decisions for a firm. Short term assets investment decisions  Important for short term survival of the organisation; thus prerequisite for long term success; mainly concerning the management of current assets that’s held on the company’s balance sheet. Profitability management  As a more minor role under this section; it comes under investment decisions because revenue generated will be from investments and divestments. Evaluation  Under each of the above headings: financial managers have to use the following financial figures as part of the evaluation process to determine if a proposal should be accepted. Payback period with NPV (Net Present Value), IRR (internal rate of return) and DCF (Discounted Cash Flow). Financing decisions  For a financial managers, they have to decide the financing mix, capital structure or leverage of a firm. Which is the use of a combination of equity, debt or hybrid securities to fund a firm's activities, or new venture. Decision making  Financial manager often uses the Theory of capital structure to determine the ratio between equity and debt which should be used in a financing round for a company.  The basis of the theory is that debt capital used beyond the point of minimum weighted average cost of capital will cause devaluation and unnecessary leverage for the company.  The equation of working out the average cost of capital can be found on the right.  Where:  Re = cost of equity Weighted Average Cost Of Capital.[2]  Rd = cost of debt  E = market value of the firm's equity  D = market value of the firm's debt  V = E + D  E/V = percentage of financing that is equity  D/V = percentage of financing that is debt  Tc = corporate tax rate Liquidity/ Working capital Decisions  The role of a financial manager often includes making sure the firm is liquid – the firm is able to finance itself in the short run, without running out of cash. They also have to make the firm’s decision in investing into current assets: which can generally be defined as the assets which can be converted into cash within one accounting year, which includes cash, short term securities debtors, etc.  The main indicator to be used here is the net working capital: which is the difference between current assets and current liabilities. Being able to be positive and negative, indicating the companies current financial position and the health of the balance sheet.  This can be further split into: Receivables management  Which includes investment in receivables that is the volume of credit sales, and collection period. Credit policy which includes credit standards, credit terms and collection efforts. Inventory management  Which are stocks of manufactured products and the material that make up the product, which includes raw materials, work-in-process, finished goods, stores and spares (supplies). For a retail business, for example, this will be a major component of their current assets. Cash management  Concerned with the management of cash flow in and out of the firm, within the firm, and cash balances held by the firm at a point of time by financing deficit or investing surplus cash. Dividend decisions  Financial managers often have to influence the dividend to 2 outcomes: The ratio as which this is distributed is called the dividend-pay out ratio.      Distribute to the shareholder in the form of dividends     Retain in the business itself  This is largely dependent on the preference of the shareholders and the investment opportunities available within the firm. But also on the theory that there must be a balance between the pay out to satisfy shareholders for them to continue to invest in the company. But the company will also need to retain profits to be reinvested so more profits can be made for the future. This is also beneficial to the shareholders for growth in the value of shares and for increased dividends paid out in the future. This infers that it is important for management and shareholders to agree to a balanced ratio which both sides can benefit from, in the long term. Although this is often an exception for shareholders who only wish to hold for the short term dividend gain. Corporate finance  Panorama clip3.jpg Working capital  Cash conversion cycle Return on capital Economic value added Just in time Economic order quantity Discounts and allowances Factoring (finance) Capital budgeting  Capital investment decisions The investment decision The financing decision Sections  Managerial finance Financial accounting Management accounting Mergers and acquisitions Balance sheet analysis Business plan Corporate action Finance series  Financial market Financial market participants Corporate finance Personal finance Public finance Banks and Banking Financial regulation This box:      view talk edit   Strategic Financial Management tasks/ services provided      Derivatives     Asset Pricing[3]     Investment Banking     M&A and other corporate restructuring  Strategic management involves the formulation and implementation of the major goals and initiatives taken by a company's top management on behalf of owners, based on consideration of resources and an assessment of the internal and external environments in which the organization competes.[1]  Strategic management provides overall direction to the enterprise and involves specifying the organization's objectives, developing policies and plans designed to achieve these objectives, and then allocating resources to implement the plans. Academics and practicing managers have developed numerous models and frameworks to assist in strategic decision making in the context of complex environments and competitive dynamics.[2] Strategic management is not static in nature; the models often include a feedback loop to monitor execution and inform the next round of planning.[3][4][5]  Michael Porter identifies three principles underlying strategy: creating a \"unique and valuable [market] position\", making trade-offs by choosing \"what not to do\", and creating \"fit\" by aligning company activities with one another to support the chosen strategy.[6] Dr. Vladimir Kvint defines strategy as \"a system of finding, formulating, and developing a doctrine that will ensure long-term success if followed faithfully.\"[7]  Corporate strategy involves answering a key question from a portfolio perspective: \"What business should we be in?\" Business strategy involves answering the question: \"How shall we compete in this business?\"[8] In management theory and practice, a further distinction is often made between strategic management and operational management. Operational management is concerned primarily with improving efficiency and controlling costs within the boundaries set by the organization's strategy.  Contents      1 Definition         1.1 Formulation         1.2 Implementation         1.3 Many definitions of strategy     2 Historical development         2.1 Origins         2.2 Change in focus from production to marketing         2.3 Nature of strategy     3 Concepts and frameworks         3.1 SWOT Analysis         3.2 Experience curve         3.3 Corporate strategy and portfolio theory         3.4 Competitive advantage         3.5 Industry structure and profitability         3.6 Generic competitive strategies         3.7 Value chain         3.8 Core competence         3.9 Theory of the business     4 Strategic thinking     5 Strategic planning         5.1 Environmental analysis         5.2 Scenario planning         5.3 Measuring and controlling implementation         5.4 Evaluation     6 Limitations     7 Strategic themes         7.1 Self-service         7.2 Globalization and the virtual firm         7.3 Internet and information availability     8 Strategy as learning     9 Strategy as adapting to change     10 Strategy as operational excellence         10.1 Quality         10.2 Reengineering     11 Other perspectives on strategy         11.1 Strategy as problem solving         11.2 Creative vs analytic approaches         11.3 Non-strategic management         11.4 Strategy as marketing         11.5 Information- and technology-driven strategy         11.6 Maturity of planning process         11.7 PIMS study     12 Other influences on business strategy         12.1 Military strategy     13 Traits of successful companies     14 See also     15 Further reading     16 References     17 External links  Definition Strategic management processes and activities  Strategic management involves the formulation and implementation of the major goals and initiatives taken by a company's top management on behalf of owners, based on consideration of resources and an assessment of the internal and external environments in which the organization competes.[1] Strategy is defined as \"the determination of the basic long-term goals of an enterprise, and the adoption of courses of action and the allocation of resources necessary for carrying out these goals.\"[9] Strategies are established to set direction, focus effort, define or clarify the organization, and provide consistency or guidance in response to the environment.[10]  Strategic management involves the related concepts of strategic planning and strategic thinking. Strategic planning is analytical in nature and refers to formalized procedures to produce the data and analyses used as inputs for strategic thinking, which synthesizes the data resulting in the strategy. Strategic planning may also refer to control mechanisms used to implement the strategy once it is determined. In other words, strategic planning happens around the strategic thinking or strategy making activity.[11]  Strategic management is often described as involving two major processes: formulation and implementation of strategy. While described sequentially below, in practice the two processes are iterative and each provides input for the other.[11] Formulation  Formulation of strategy involves analyzing the environment in which the organization operates, then making a series of strategic decisions about how the organization will compete. Formulation ends with a series of goals or objectives and measures for the organization to pursue. Environmental analysis includes the:      Remote external environment, including the political, economic, social, technological, legal and environmental landscape (PESTLE);     Industry environment, such as the competitive behavior of rival organizations, the bargaining power of buyers/customers and suppliers, threats from new entrants to the industry, and the ability of buyers to substitute products (Porter's 5 forces); and     Internal environment, regarding the strengths and weaknesses of the organization's resources (i.e., its people, processes and IT systems).[11]  Strategic decisions are based on insight from the environmental assessment and are responses to strategic questions about how the organization will compete, such as:      What is the organization's business?     Who is the target customer for the organization's products and services?     Where are the customers and how do they buy? What is considered \"value\" to the customer?     Which businesses, products and services should be included or excluded from the portfolio of offerings?     What is the geographic scope of the business?     What differentiates the company from its competitors in the eyes of customers and other stakeholders?     Which skills and capabilities should be developed within the firm?     What are the important opportunities and risks for the organization?     How can the firm grow, through both its base business and new business?     How can the firm generate more value for investors?[11][12]  The answers to these and many other strategic questions result in the organization's strategy and a series of specific short-term and long-term goals or objectives and related measures.[11] Implementation  The second major process of strategic management is implementation, which involves decisions regarding how the organization's resources (i.e., people, process and IT systems) will be aligned and mobilized towards the objectives. Implementation results in how the organization's resources are structured (such as by product or service or geography), leadership arrangements, communication, incentives, and monitoring mechanisms to track progress towards objectives, among others.[11]  Running the day-to-day operations of the business is often referred to as \"operations management\" or specific terms for key departments or functions, such as \"logistics management\" or \"marketing management,\" which take over once strategic management decisions are implemented.[11] Many definitions of strategy  Strategy has been practiced whenever an advantage was gained by planning the sequence and timing of the deployment of resources while simultaneously taking into account the probable capabilities and behavior of competition. Bruce Henderson[13]  In 1988, Henry Mintzberg described the many different definitions and perspectives on strategy reflected in both academic research and in practice.[14][15] He examined the strategic process and concluded it was much more fluid and unpredictable than people had thought. Because of this, he could not point to one process that could be called strategic planning. Instead Mintzberg concludes that there are five types of strategies:      Strategy as plan – a directed course of action to achieve an intended set of goals; similar to the strategic planning concept;     Strategy as pattern – a consistent pattern of past behavior, with a strategy realized over time rather than planned or intended. Where the realized pattern was different from the intent, he referred to the strategy as emergent;     Strategy as position – locating brands, products, or companies within the market, based on the conceptual framework of consumers or other stakeholders; a strategy determined primarily by factors outside the firm;     Strategy as ploy – a specific maneuver intended to outwit a competitor; and     Strategy as perspective – executing strategy based on a \"theory of the business\" or natural extension of the mindset or ideological perspective of the organization.  In 1998, Mintzberg developed these five types of management strategy into 10 “schools of thought” and grouped them into three categories. The first group is normative. It consists of the schools of informal design and conception, the formal planning, and analytical positioning. The second group, consisting of six schools, is more concerned with how strategic management is actually done, rather than prescribing optimal plans or positions. The six schools are entrepreneurial, visionary, cognitive, learning/adaptive/emergent, negotiation, corporate culture and business environment. The third and final group consists of one school, the configuration or transformation school, a hybrid of the other schools organized into stages, organizational life cycles, or “episodes”.[16]  Michael Porter defined strategy in 1980 as the \"...broad formula for how a business is going to compete, what its goals should be, and what policies will be needed to carry out those goals\" and the \"...combination of the ends (goals) for which the firm is striving and the means (policies) by which it is seeking to get there.\" He continued that: \"The essence of formulating competitive strategy is relating a company to its environment.\"[17] Historical development Origins  The strategic management discipline originated in the 1950s and 1960s. Among the numerous early contributors, the most influential were Peter Drucker, Philip Selznick, Alfred Chandler, Igor Ansoff, and Bruce Henderson.[2] The discipline draws from earlier thinking and texts on 'strategy' dating back thousands of years. Prior to 1960, the term \"strategy\" was primarily used regarding war and politics, not business.[18] Many companies built strategic planning functions to develop and execute the formulation and implementation processes during the 1960s.[19]  Peter Drucker was a prolific management theorist and author of dozens of management books, with a career spanning five decades. He addressed fundamental strategic questions in a 1954 book The Practice of Management writing: \"...the first responsibility of top management is to ask the question 'what is our business?' and to make sure it is carefully studied and correctly answered.\" He wrote that the answer was determined by the customer. He recommended eight areas where objectives should be set, such as market standing, innovation, productivity, physical and financial resources, worker performance and attitude, profitability, manager performance and development, and public responsibility.[20]  In 1957, Philip Selznick initially used the term \"distinctive competence\" in referring to how the Navy was attempting to differentiate itself from the other services.[2] He also formalized the idea of matching the organization's internal factors with external environmental circumstances.[21] This core idea was developed further by Kenneth R. Andrews in 1963 into what we now call SWOT analysis, in which the strengths and weaknesses of the firm are assessed in light of the opportunities and threats in the business environment.[2]  Alfred Chandler recognized the importance of coordinating management activity under an all-encompassing strategy. Interactions between functions were typically handled by managers who relayed information back and forth between departments. Chandler stressed the importance of taking a long term perspective when looking to the future. In his 1962 ground breaking work Strategy and Structure, Chandler showed that a long-term coordinated strategy was necessary to give a company structure, direction and focus. He says it concisely, “structure follows strategy.” Chandler wrote that:      \"Strategy is the determination of the basic long-term goals of an enterprise, and the adoption of courses of action and the allocation of resources necessary for carrying out these goals.\"[9]  Igor Ansoff built on Chandler's work by adding concepts and inventing a vocabulary. He developed a grid that compared strategies for market penetration, product development, market development and horizontal and vertical integration and diversification. He felt that management could use the grid to systematically prepare for the future. In his 1965 classic Corporate Strategy, he developed gap analysis to clarify the gap between the current reality and the goals and to develop what he called “gap reducing actions”.[22] Ansoff wrote that strategic management had three parts: strategic planning; the skill of a firm in converting its plans into reality; and the skill of a firm in managing its own internal resistance to change.[23]  Bruce Henderson, founder of the Boston Consulting Group, wrote about the concept of the experience curve in 1968, following initial work begun in 1965. The experience curve refers to a hypothesis that unit production costs decline by 20-30% every time cumulative production doubles. This supported the argument for achieving higher market share and economies of scale.[24]  Porter wrote in 1980 that companies have to make choices about their scope and the type of competitive advantage they seek to achieve, whether lower cost or differentiation. The idea of strategy targeting particular industries and customers (i.e., competitive positions) with a differentiated offering was a departure from the experience-curve influenced strategy paradigm, which was focused on larger scale and lower cost.[17] Porter revised the strategy paradigm again in 1985, writing that superior performance of the processes and activities performed by organizations as part of their value chain is the foundation of competitive advantage, thereby outlining a process view of strategy.[25] Change in focus from production to marketing  The direction of strategic research also paralleled a major paradigm shift in how companies competed, specifically a shift from the production focus to market focus. The prevailing concept in strategy up to the 1950s was to create a product of high technical quality. If you created a product that worked well and was durable, it was assumed you would have no difficulty profiting. This was called the production orientation. Henry Ford famously said of the Model T car: \"Any customer can have a car painted any color that he wants, so long as it is black.\"[26]  Management theorist Peter F Drucker wrote in 1954 that it was the customer who defined what business the organization was in.[12] In 1960 Theodore Levitt argued that instead of producing products then trying to sell them to the customer, businesses should start with the customer, find out what they wanted, and then produce it for them. The fallacy of the production orientation was also referred to as marketing myopia in an article of the same name by Levitt.[27]  Over time, the customer became the driving force behind all strategic business decisions. This marketing concept, in the decades since its introduction, has been reformulated and repackaged under names including market orientation, customer orientation, customer intimacy, customer focus, customer-driven and market focus.  It's more important than ever to define yourself in terms of what you stand for rather than what you make, because what you make is going to become outmoded faster than it has at any time in the past. Jim Collins[28]  Jim Collins wrote in 1997 that the strategic frame of reference is expanded by focusing on why a company exists rather than what it makes.[28] In 2001, he recommended that organizations define themselves based on three key questions:      What are we passionate about?     What can we be best in the world at?     What drives our economic engine?[29]  Nature of strategy  In 1985, Professor Ellen Earle-Chaffee summarized what she thought were the main elements of strategic management theory where consensus generally existed as of the 1970s, writing that strategic management:[8]      Involves adapting the organization to its business environment;     Is fluid and complex. Change creates novel combinations of circumstances requiring unstructured non-repetitive responses;     Affects the entire organization by providing direction;     Involves both strategy formulation processes and also implementation of the content of the strategy;     May be planned (intended) and unplanned (emergent);     Is done at several levels: overall corporate strategy, and individual business strategies; and     Involves both conceptual and analytical thought processes.  Chaffee further wrote that research up to that point covered three models of strategy, which were not mutually exclusive:      Linear strategy: A planned determination of goals, initiatives, and allocation of resources, along the lines of the Chandler definition above. This is most consistent with strategic planning approaches and may have a long planning horizon. The strategist \"deals with\" the environment but it is not the central concern.     Adaptive strategy: In this model, the organization's goals and activities are primarily concerned with adaptation to the environment, analogous to a biological organism. The need for continuous adaption reduces or eliminates the planning window. There is more focus on means (resource mobilization to address the environment) rather than ends (goals). Strategy is less centralized than in the linear model.     Interpretive strategy: A more recent and less developed model than the linear and adaptive models, interpretive strategy is concerned with \"orienting metaphors constructed for the purpose of conceptualizing and guiding individual attitudes or organizational participants.\" The aim of interpretive strategy is legitimacy or credibility in the mind of stakeholders. It places emphasis on symbols and language to influence the minds of customers, rather than the physical product of the organization.[8]  Concepts and frameworks  The progress of strategy since 1960 can be charted by a variety of frameworks and concepts introduced by management consultants and academics. These reflect an increased focus on cost, competition and customers. These \"3 Cs\" were illuminated by much more robust empirical analysis at ever-more granular levels of detail, as industries and organizations were disaggregated into business units, activities, processes, and individuals in a search for sources of competitive advantage.[18] SWOT Analysis Main article: SWOT Analysis A SWOT analysis, with its four elements in a 2×2 matrix.  By the 1960s, the capstone business policy course at the Harvard Business School included the concept of matching the distinctive competence of a company (its internal strengths and weaknesses) with its environment (external opportunities and threats) in the context of its objectives. This framework came to be known by the acronym SWOT and was \"a major step forward in bringing explicitly competitive thinking to bear on questions of strategy.\" Kenneth R. Andrews helped popularize the framework via a 1963 conference and it remains commonly used in practice.[2] Experience curve Experience curve Main article: Experience curve  The experience curve was developed by the Boston Consulting Group in 1966.[18] It is a hypothesis that total per unit costs decline systematically by as much as 15-25% every time cumulative production (i.e., \"experience\") doubles. It has been empirically confirmed by some firms at various points in their history.[30] Costs decline due to a variety of factors, such as the learning curve, substitution of labor for capital (automation), and technological sophistication. Author Walter Kiechel wrote that it reflected several insights, including:      A company can always improve its cost structure;     Competitors have varying cost positions based on their experience;     Firms could achieve lower costs through higher market share, attaining a competitive advantage; and     An increased focus on empirical analysis of costs and processes, a concept which author Kiechel refers to as \"Greater Taylorism.\"  Kiechel wrote in 2010: \"The experience curve was, simply, the most important concept in launching the strategy revolution...with the experience curve, the strategy revolution began to insinuate an acute awareness of competition into the corporate consciousness.\" Prior to the 1960s, the word competition rarely appeared in the most prominent management literature; U.S. companies then faced considerably less competition and did not focus on performance relative to peers. Further, the experience curve provided a basis for the retail sale of business ideas, helping drive the management consulting industry.[18] Corporate strategy and portfolio theory Main articles: Modern portfolio theory and Growth–share matrix Portfolio growth–share matrix  The concept of the corporation as a portfolio of business units, with each plotted graphically based on its market share (a measure of its competitive position relative to its peers) and industry growth rate (a measure of industry attractiveness), was summarized in the growth–share matrix developed by the Boston Consulting Group around 1970. By 1979, one study estimated that 45% of the Fortune 500 companies were using some variation of the matrix in their strategic planning. This framework helped companies decide where to invest their resources (i.e., in their high market share, high growth businesses) and which businesses to divest (i.e., low market share, low growth businesses.)[18]  Porter wrote in 1987 that corporate strategy involves two questions: 1) What business should the corporation be in? and 2) How should the corporate office manage its business units? He mentioned four concepts of corporate strategy; the latter three can be used together:[31]      Portfolio theory: A strategy based primarily on diversification through acquisition. The corporation shifts resources among the units and monitors the performance of each business unit and its leaders. Each unit generally runs autonomously, with limited interference from the corporate center provided goals are met.     Restructuring: The corporate office acquires then actively intervenes in a business where it detects potential, often by replacing management and implementing a new business strategy.     Transferring skills: Important managerial skills and organizational capability are essentially spread to multiple businesses. The skills must be necessary to competitive advantage.     Sharing activities: Ability of the combined corporation to leverage centralized functions, such as sales, finance, etc. thereby reducing costs.[31]  Other techniques were developed to analyze the relationships between elements in a portfolio. The growth-share matrix, a part of B.C.G. Analysis, was followed by G.E. multi factoral model, developed by General Electric. Companies continued to diversify as conglomerates until the 1980s, when deregulation and a less restrictive anti-trust environment led to the view that a portfolio of operating divisions in different industries was worth more as many independent companies, leading to the breakup of many conglomerates.[18] While the popularity of portfolio theory has waxed and waned, the key dimensions considered (industry attractiveness and competitive position) remain central to strategy.[2] Competitive advantage Main article: Competitive advantage  In 1980, Porter defined the two types of competitive advantage an organization can achieve relative to its rivals: lower cost or differentiation. This advantage derives from attribute(s) that allow an organization to outperform its competition, such as superior market position, skills, or resources. In Porter's view, strategic management should be concerned with building and sustaining competitive advantage.[25] Industry structure and profitability A graphical representation of Porter's Five Forces Main article: Porter five forces analysis  Porter developed a framework for analyzing the profitability of industries and how those profits are divided among the participants in 1980. In five forces analysis he identified the forces that shape the industry structure or environment. The framework involves the bargaining power of buyers and suppliers, the threat of new entrants, the availability of substitute products, and the competitive rivalry of firms in the industry. These forces affect the organization's ability to raise its prices as well as the costs of inputs (such as raw materials) for its processes.[17]  The five forces framework helps describe how a firm can use these forces to obtain a sustainable competitive advantage, either lower cost or differentiation. Companies can maximize their profitability by competing in industries with favorable structure. Competitors can take steps to grow the overall profitability of the industry, or to take profit away from other parts of the industry structure. Porter modified Chandler's dictum about structure following strategy by introducing a second level of structure: while organizational structure follows strategy, it in turn follows industry structure.[17] Generic competitive strategies Main article: Porter's generic strategies Michael Porter's Three Generic Strategies  Porter wrote in 1980 that strategy target either cost leadership, differentiation, or focus.[17] These are known as Porter's three generic strategies and can be applied to any size or form of business. Porter claimed that a company must only choose one of the three or risk that the business would waste precious resources. Porter's generic strategies detail the interaction between cost minimization strategies, product differentiation strategies, and market focus strategies.  Porter described an industry as having multiple segments that can be targeted by a firm. The breadth of its targeting refers to the competitive scope of the business. Porter defined two types of competitive advantage: lower cost or differentiation relative to its rivals. Achieving competitive advantage results from a firm's ability to cope with the five forces better than its rivals. Porter wrote: \"[A]chieving competitive advantage requires a firm to make a choice...about the type of competitive advantage it seeks to attain and the scope within which it will attain it.\" He also wrote: \"The two basic types of competitive advantage [differentiation and lower cost] combined with the scope of activities for which a firm seeks to achieve them lead to three generic strategies for achieving above average performance in an industry: cost leadership, differentiation and focus. The focus strategy has two variants, cost focus and differentiation focus.\"[25]  The concept of choice was a different perspective on strategy, as the 1970s paradigm was the pursuit of market share (size and scale) influenced by the experience curve. Companies that pursued the highest market share position to achieve cost advantages fit under Porter's cost leadership generic strategy, but the concept of choice regarding differentiation and focus represented a new perspective.[18] Value chain Michael Porter's Value Chain Main article: Value chain  Porter's 1985 description of the value chain refers to the chain of activities (processes or collections of processes) that an organization performs in order to deliver a valuable product or service for the market. These include functions such as inbound logistics, operations, outbound logistics, marketing and sales, and service, supported by systems and technology infrastructure. By aligning the various activities in its value chain with the organization's strategy in a coherent way, a firm can achieve a competitive advantage. Porter also wrote that strategy is an internally consistent configuration of activities that differentiates a firm from its rivals. A robust competitive position cumulates from many activities which should fit coherently together.[32]  Porter wrote in 1985: \"Competitive advantage cannot be understood by looking at a firm as a whole. It stems from the many discrete activities a firm performs in designing, producing, marketing, delivering and supporting its product. Each of these activities can contribute to a firm's relative cost position and create a basis for differentiation...the value chain disaggregates a firm into its strategically relevant activities in order to understand the behavior of costs and the existing and potential sources of differentiation.\"[2] Core competence Main article: Core competency  Gary Hamel and C. K. Prahalad described the idea of core competency in 1990, the idea that each organization has some capability in which it excels and that the business should focus on opportunities in that area, letting others go or outsourcing them. Further, core competency is difficult to duplicate, as it involves the skills and coordination of people across a variety of functional areas or processes used to deliver value to customers. By outsourcing, companies expanded the concept of the value chain, with some elements within the entity and others without.[33] Core competency is part of a branch of strategy called the resource-based view of the firm, which postulates that if activities are strategic as indicated by the value chain, then the organization's capabilities and ability to learn or adapt are also strategic.[2] Theory of the business  Peter Drucker wrote in 1994 about the “Theory of the Business,” which represents the key assumptions underlying a firm's strategy. These assumptions are in three categories: a) the external environment, including society, market, customer, and technology; b) the mission of the organization; and c) the core competencies needed to accomplish the mission. He continued that a valid theory of the business has four specifications: 1) assumptions about the environment, mission, and core competencies must fit reality; 2) the assumptions in all three areas have to fit one another; 3) the theory of the business must be known and understood throughout the organization; and 4) the theory of the business has to be tested constantly.  He wrote that organizations get into trouble when the assumptions representing the theory of the business no longer fit reality. He used an example of retail department stores, where their theory of the business assumed that people who could afford to shop in department stores would do so. However, many shoppers abandoned department stores in favor of specialty retailers (often located outside of malls) when time became the primary factor in the shopping destination rather than income.  Drucker described the theory of the business as a \"hypothesis\" and a \"discipline.\" He advocated building in systematic diagnostics, monitoring and testing of the assumptions comprising the theory of the business to maintain competitiveness.[34] Strategic thinking Main article: Strategic thinking  Strategic thinking involves the generation and application of unique business insights to opportunities intended to create competitive advantage for a firm or organization. It involves challenging the assumptions underlying the organization's strategy and value proposition. Mintzberg wrote in 1994 that it is more about synthesis (i.e., \"connecting the dots\") than analysis (i.e., \"finding the dots\"). It is about \"capturing what the manager learns from all sources (both the soft insights from his or her personal experiences and the experiences of others throughout the organization and the hard data from market research and the like) and then synthesizing that learning into a vision of the direction that the business should pursue.\" Mintzberg argued that strategic thinking is the critical part of formulating strategy, more so than strategic planning exercises.[35]  General Andre Beaufre wrote in 1963 that strategic thinking \"is a mental process, at once abstract and rational, which must be capable of synthesizing both psychological and material data. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself--and the diagnosis in fact amounts to a choice between alternative courses of action.\"[36]  Will Mulcaster[37] argued that while much research and creative thought has been devoted to generating alternative strategies, too little work has been done on what influences the quality of strategic decision making and the effectiveness with which strategies are implemented. For instance, in retrospect it can be seen that the financial crisis of 2008–9 could have been avoided if the banks had paid more attention to the risks associated with their investments, but how should banks change the way they make decisions to improve the quality of their decisions in the future? Mulcaster's Managing Forces framework addresses this issue by identifying 11 forces that should be incorporated into the processes of decision making and strategic implementation. The 11 forces are: Time; Opposing forces; Politics; Perception; Holistic effects; Adding value; Incentives; Learning capabilities; Opportunity cost; Risk and Style. Strategic planning Main article: Strategic planning  Strategic planning is a means of administering the formulation and implementation of strategy. Strategic planning is analytical in nature and refers to formalized procedures to produce the data and analyses used as inputs for strategic thinking, which synthesizes the data resulting in the strategy. Strategic planning may also refer to control mechanisms used to implement the strategy once it is determined. In other words, strategic planning happens around the strategy formation process.[11] Environmental analysis  Porter wrote in 1980 that formulation of competitive strategy includes consideration of four key elements:      Company strengths and weaknesses;     Personal values of the key implementers (i.e., management and the board)     Broader societal expectations.[17]  The first two elements relate to factors internal to the company (i.e., the internal environment), while the latter two relate to factors external to the company (i.e., the external environment).[17]  There are many analytical frameworks which attempt to organize the strategic planning process. Examples of frameworks that address the four elements described above include:      External environment: PEST analysis or STEEP analysis is a framework used to examine the remote external environmental factors that can affect the organization, such as political, economic, social/demographic, and technological. Common variations include SLEPT, PESTLE, STEEPLE, and STEER analysis, each of which incorporates slightly different emphases.     Industry environment: The Porter Five Forces Analysis framework helps to determine the competitive rivalry and therefore attractiveness of a market. It is used to help determine the portfolio of offerings the organization will provide and in which markets.     Relationship of internal and external environment: SWOT analysis is one of the most basic and widely used frameworks, which examines both internal elements of the organization — Strengths and Weaknesses — and external elements — Opportunities and Threats. It helps examine the organization's resources in the context of its environment.  Scenario planning  A number of strategists use scenario planning techniques to deal with change. The way Peter Schwartz put it in 1991 is that strategic outcomes cannot be known in advance so the sources of competitive advantage cannot be predetermined.[38] The fast changing business environment is too uncertain for us to find sustainable value in formulas of excellence or competitive advantage. Instead, scenario planning is a technique in which multiple outcomes can be developed, their implications assessed, and their likeliness of occurrence evaluated. According to Pierre Wack, scenario planning is about insight, complexity, and subtlety, not about formal analysis and numbers.[39]  Some business planners are starting to use a complexity theory approach to strategy. Complexity can be thought of as chaos with a dash of order. Chaos theory deals with turbulent systems that rapidly become disordered. Complexity is not quite so unpredictable. It involves multiple agents interacting in such a way that a glimpse of structure may appear. Measuring and controlling implementation Generic Strategy Map illustrating four elements of a balanced scorecard  Once the strategy is determined, various goals and measures may be established to chart a course for the organization, measure performance and control implementation of the strategy. Tools such as the balanced scorecard and strategy maps help crystallize the strategy, by relating key measures of success and performance to the strategy. These tools measure financial, marketing, production, organizational development, and innovation measures to achieve a 'balanced' perspective. Advances in information technology and data availability enable the gathering of more information about performance, allowing managers to take a much more analytical view of their business than before.  Strategy may also be organized as a series of \"initiatives\" or \"programs\", each of which comprises one or more projects. Various monitoring and feedback mechanisms may also be established, such as regular meetings between divisional and corporate management to control implementation. Evaluation  A key component to strategic management which is often overlooked when planning is evaluation. There are many ways to evaluate whether or not strategic priorities and plans have been achieved, one such method is Robert Stake's Responsive Evaluation.[40] Responsive evaluation  provides a naturalistic and humanistic approach to program evaluation. In expanding beyond the goal-oriented or pre-ordinate evaluation design, responsive evaluation takes into consideration the program’s background (history), conditions, and transactions among stakeholders. It is largely emergent, the design unfolds as contact is made with stakeholders. Limitations  While strategies are established to set direction, focus effort, define or clarify the organization, and provide consistency or guidance in response to the environment, these very elements also mean that certain signals are excluded from consideration or de-emphasized. Mintzberg wrote in 1987: \"Strategy is a categorizing scheme by which incoming stimuli can be ordered and dispatched.\" Since a strategy orients the organization in a particular manner or direction, that direction may not effectively match the environment, initially (if a bad strategy) or over time as circumstances change. As such, Mintzberg continued, \"Strategy [once established] is a force that resists change, not encourages it.\"[10]  Therefore, a critique of strategic management is that it can overly constrain managerial discretion in a dynamic environment. \"How can individuals, organizations and societies cope as well as possible with ... issues too complex to be fully understood, given the fact that actions initiated on the basis of inadequate understanding may lead to significant regret?\"[41] Some theorists insist on an iterative approach, considering in turn objectives, implementation and resources.[42] I.e., a \"...repetitive learning cycle [rather than] a linear progression towards a clearly defined final destination.\"[43] Strategies must be able to adjust during implementation because \"humans rarely can proceed satisfactorily except by learning from experience; and modest probes, serially modified on the basis of feedback, usually are the best method for such learning.\"[44]  In 2000, Gary Hamel coined the term strategic convergence to explain the limited scope of the strategies being used by rivals in greatly differing circumstances. He lamented that successful strategies are imitated by firms that do not understand that for a strategy to work, it must account for the specifics of each situation.[45] Woodhouse and Collingridge claim that the essence of being “strategic” lies in a capacity for \"intelligent trial-and error\"[44] rather than strict adherence to finely honed strategic plans. Strategy should be seen as laying out the general path rather than precise steps.[46] Means are as likely to determine ends as ends are to determine means.[47] The objectives that an organization might wish to pursue are limited by the range of feasible approaches to implementation. (There will usually be only a small number of approaches that will not only be technically and administratively possible, but also satisfactory to the full range of organizational stakeholders.) In turn, the range of feasible implementation approaches is determined by the availability of resources. Strategic themes  Various strategic approaches used across industries (themes) have arisen over the years. These include the shift from product-driven demand to customer- or marketing-driven demand (described above), the increased use of self-service approaches to lower cost, changes in the value chain or corporate structure due to globalization (e.g., off-shoring of production and assembly), and the internet. Self-service  One theme in strategic competition has been the trend towards self-service, often enabled by technology, where the customer takes on a role previously performed by a worker to lower the price.[6] Examples include:      Automated teller machine (ATM) to obtain cash rather via a bank teller;     Self-service at the gas pump rather than with help from an attendant;     Retail internet orders input by the customer rather than a retail clerk, such as online book sales;     Mass-produced ready-to-assemble furniture transported by the customer;     Self-checkout at the grocery store; and     Online banking and bill payment.[48]  Globalization and the virtual firm  One definition of globalization refers to the integration of economies due to technology and supply chain process innovation. Companies are no longer required to be vertically integrated (i.e., designing, producing, assembling, and selling their products). In other words, the value chain for a company's product may no longer be entirely within one firm; several entities comprising a virtual firm may exist to fulfill the customer requirement. For example, some companies have chosen to outsource production to third parties, retaining only design and sales functions inside their organization.[6] Internet and information availability  The internet has dramatically empowered consumers and enabled buyers and sellers to come together with drastically reduced transaction and intermediary costs, creating much more robust marketplaces for the purchase and sale of goods and services. Examples include online auction sites, internet dating services, and internet book sellers. In many industries, the internet has dramatically altered the competitive landscape. Services that used to be provided within one entity (e.g., a car dealership providing financing and pricing information) are now provided by third parties.[49] Further, compared to traditional media like television, the internet has caused a major shift in viewing habits through on demand content which has led to an increasingly fragmented audience.[citation needed]  Author Phillip Evans said in 2013 that networks are challenging traditional hierarchies. Value chains may also be breaking up (\"deconstructing\") where information aspects can be separated from functional activity. Data that is readily available for free or very low cost makes it harder for information-based, vertically integrated businesses to remain intact. Evans said: \"The basic story here is that what used to be vertically integrated, oligopolistic competition among essentially similar kinds of competitors is evolving, by one means or another, from a vertical structure to a horizontal one. Why is that happening? It's happening because transaction costs are plummeting and because scale is polarizing. The plummeting of transaction costs weakens the glue that holds value chains together, and allows them to separate.\" He used Wikipedia as an example of a network that has challenged the traditional encyclopedia business model.[50] Evans predicts the emergence of a new form of industrial organization called a \"stack\", analogous to a technology stack, in which competitors rely on a common platform of inputs (services or information), essentially layering the remaining competing parts of their value chains on top of this common platform.[51] Strategy as learning See also: Organizational learning  In 1990, Peter Senge, who had collaborated with Arie de Geus at Dutch Shell, popularized de Geus' notion of the \"learning organization\".[52] The theory is that gathering and analyzing information is a necessary requirement for business success in the information age. To do this, Senge claimed that an organization would need to be structured such that:[53]      People can continuously expand their capacity to learn and be productive.     New patterns of thinking are nurtured.     Collective aspirations are encouraged.     People are encouraged to see the “whole picture” together.  Senge identified five disciplines of a learning organization. They are:      Personal responsibility, self-reliance, and mastery — We accept that we are the masters of our own destiny. We make decisions and live with the consequences of them. When a problem needs to be fixed, or an opportunity exploited, we take the initiative to learn the required skills to get it done.     Mental models — We need to explore our personal mental models to understand the subtle effect they have on our behaviour.     Shared vision — The vision of where we want to be in the future is discussed and communicated to all. It provides guidance and energy for the journey ahead.     Team learning — We learn together in teams. This involves a shift from “a spirit of advocacy to a spirit of enquiry”.     Systems thinking — We look at the whole rather than the parts. This is what Senge calls the “Fifth discipline”. It is the glue that integrates the other four into a coherent strategy. For an alternative approach to the “learning organization”, see Garratt, B. (1987).  Geoffrey Moore (1991) and R. Frank and P. Cook[54] also detected a shift in the nature of competition. Markets driven by technical standards or by \"network effects\" can give the dominant firm a near-monopoly.[55] The same is true of networked industries in which interoperability requires compatibility between users. Examples include Internet Explorer's and Amazon's early dominance of their respective industries. IE's later decline shows that such dominance may be only temporary.  Moore showed how firms could attain this enviable position by using E.M. Rogers' five stage adoption process and focusing on one group of customers at a time, using each group as a base for reaching the next group. The most difficult step is making the transition between introduction and mass acceptance. (See Crossing the Chasm). If successful a firm can create a bandwagon effect in which the momentum builds and its product becomes a de facto standard. Strategy as adapting to change  In 1969, Peter Drucker coined the phrase Age of Discontinuity to describe the way change disrupts lives.[56] In an age of continuity attempts to predict the future by extrapolating from the past can be accurate. But according to Drucker, we are now in an age of discontinuity and extrapolating is ineffective. He identifies four sources of discontinuity: new technologies, globalization, cultural pluralism and knowledge capital.  In 1970, Alvin Toffler in Future Shock described a trend towards accelerating rates of change.[57] He illustrated how social and technical phenomena had shorter lifespans with each generation, and he questioned society's ability to cope with the resulting turmoil and accompanying anxiety. In past eras periods of change were always punctuated with times of stability. This allowed society to assimilate the change before the next change arrived. But these periods of stability had all but disappeared by the late 20th century. In 1980 in The Third Wave, Toffler characterized this shift to relentless change as the defining feature of the third phase of civilization (the first two phases being the agricultural and industrial waves).[58]  In 1978, Derek F. Abell (Abell, D. 1978) described \"strategic windows\" and stressed the importance of the timing (both entrance and exit) of any given strategy. This led some strategic planners to build planned obsolescence into their strategies.[59]  In 1983, Noel Tichy wrote that because we are all beings of habit we tend to repeat what we are comfortable with.[60] He wrote that this is a trap that constrains our creativity, prevents us from exploring new ideas, and hampers our dealing with the full complexity of new issues. He developed a systematic method of dealing with change that involved looking at any new issue from three angles: technical and production, political and resource allocation, and corporate culture.  In 1989, Charles Handy identified two types of change.[61] \"Strategic drift\" is a gradual change that occurs so subtly that it is not noticed until it is too late. By contrast, \"transformational change\" is sudden and radical. It is typically caused by discontinuities (or exogenous shocks) in the business environment. The point where a new trend is initiated is called a \"strategic inflection point\" by Andy Grove. Inflection points can be subtle or radical.  In 1990, Richard Pascale wrote that relentless change requires that businesses continuously reinvent themselves.[62] His famous maxim is “Nothing fails like success” by which he means that what was a strength yesterday becomes the root of weakness today, We tend to depend on what worked yesterday and refuse to let go of what worked so well for us in the past. Prevailing strategies become self-confirming. To avoid this trap, businesses must stimulate a spirit of inquiry and healthy debate. They must encourage a creative process of self-renewal based on constructive conflict.  In 1996, Adrian Slywotzky showed how changes in the business environment are reflected in value migrations between industries, between companies, and within companies.[63] He claimed that recognizing the patterns behind these value migrations is necessary if we wish to understand the world of chaotic change. In “Profit Patterns” (1999) he described businesses as being in a state of strategic anticipation as they try to spot emerging patterns. Slywotsky and his team identified 30 patterns that have transformed industry after industry.[64]  In 1997, Clayton Christensen (1997) took the position that great companies can fail precisely because they do everything right since the capabilities of the organization also define its disabilities.[65] Christensen's thesis is that outstanding companies lose their market leadership when confronted with disruptive technology. He called the approach to discovering the emerging markets for disruptive technologies agnostic marketing, i.e., marketing under the implicit assumption that no one – not the company, not the customers – can know how or in what quantities a disruptive product can or will be used without the experience of using it.  In 1999, Constantinos Markides reexamined the nature of strategic planning.[66] He described strategy formation and implementation as an ongoing, never-ending, integrated process requiring continuous reassessment and reformation. Strategic management is planned and emergent, dynamic and interactive.  J. Moncrieff (1999) stressed strategy dynamics.[67] He claimed that strategy is partially deliberate and partially unplanned. The unplanned element comes from emergent strategies that result from the emergence of opportunities and threats in the environment and from \"strategies in action\" (ad hoc actions across the organization).  David Teece pioneered research on resource-based strategic management and the dynamic capabilities perspective, defined as “the ability to integrate, build, and reconfigure internal and external competencies to address rapidly changing environments\".[68] His 1997 paper (with Gary Pisano and Amy Shuen) \"Dynamic Capabilities and Strategic Management\" was the most cited paper in economics and business for the period from 1995 to 2005.[69]  In 2000, Gary Hamel discussed strategic decay, the notion that the value of every strategy, no matter how brilliant, decays over time.[45] Strategy as operational excellence Quality  A large group of theorists felt the area where western business was most lacking was product quality. W. Edwards Deming,[70] Joseph M. Juran,[71] A. Kearney,[72] Philip Crosby[73] and Armand Feignbaum[74] suggested quality improvement techniques such total quality management (TQM), continuous improvement (kaizen), lean manufacturing, Six Sigma, and return on quality (ROQ).  Contrarily, James Heskett (1988),[75] Earl Sasser (1995), William Davidow,[76] Len Schlesinger,[77] A. Paraurgman (1988), Len Berry,[78] Jane Kingman-Brundage,[79] Christopher Hart, and Christopher Lovelock (1994), felt that poor customer service was the problem. They gave us fishbone diagramming, service charting, Total Customer Service (TCS), the service profit chain, service gaps analysis, the service encounter, strategic service vision, service mapping, and service teams. Their underlying assumption was that there is no better source of competitive advantage than a continuous stream of delighted customers.  Process management uses some of the techniques from product quality management and some of the techniques from customer service management. It looks at an activity as a sequential process. The objective is to find inefficiencies and make the process more effective. Although the procedures have a long history, dating back to Taylorism, the scope of their applicability has been greatly widened, leaving no aspect of the firm free from potential process improvements. Because of the broad applicability of process management techniques, they can be used as a basis for competitive advantage.  Carl Sewell,[80] Frederick F. Reichheld,[81] C. Gronroos,[82] and Earl Sasser[83] observed that businesses were spending more on customer acquisition than on retention. They showed how a competitive advantage could be found in ensuring that customers returned again and again. Reicheld broadened the concept to include loyalty from employees, suppliers, distributors and shareholders. They developed techniques for estimating customer lifetime value (CLV) for assessing long-term relationships. The concepts begat attempts to recast selling and marketing into a long term endeavor that created a sustained relationship (called relationship selling, relationship marketing, and customer relationship management). Customer relationship management (CRM) software became integral to many firms. Reengineering  Michael Hammer and James Champy felt that these resources needed to be restructured.[84] In a process that they labeled reengineering, firm's reorganized their assets around whole processes rather than tasks. In this way a team of people saw a project through, from inception to completion. This avoided functional silos where isolated departments seldom talked to each other. It also eliminated waste due to functional overlap and interdepartmental communications.  In 1989 Richard Lester and the researchers at the MIT Industrial Performance Center identified seven best practices and concluded that firms must accelerate the shift away from the mass production of low cost standardized products. The seven areas of best practice were:[85]      Simultaneous continuous improvement in cost, quality, service, and product innovation     Breaking down organizational barriers between departments     Eliminating layers of management creating flatter organizational hierarchies.     Closer relationships with customers and suppliers     Intelligent use of new technology     Global focus     Improving human resource skills  The search for best practices is also called benchmarking.[86] This involves determining where you need to improve, finding an organization that is exceptional in this area, then studying the company and applying its best practices in your firm. Other perspectives on strategy Strategy as problem solving  Professor Richard P. Rumelt described strategy as a type of problem solving in 2011. He wrote that good strategy has an underlying structure called a kernel. The kernel has three parts: 1) A diagnosis that defines or explains the nature of the challenge; 2) A guiding policy for dealing with the challenge; and 3) Coherent actions designed to carry out the guiding policy.[87] President Kennedy outlined these three elements of strategy in his Cuban Missile Crisis Address to the Nation of 22 October 1962:      Diagnosis: “This Government, as promised, has maintained the closest surveillance of the Soviet military buildup on the island of Cuba. Within the past week, unmistakable evidence has established the fact that a series of offensive missile sites is now in preparation on that imprisoned island. The purpose of these bases can be none other than to provide a nuclear strike capability against the Western Hemisphere.”     Guiding Policy: “Our unswerving objective, therefore, must be to prevent the use of these missiles against this or any other country, and to secure their withdrawal or elimination from the Western Hemisphere.”     Action Plans: First among seven numbered steps was the following: “To halt this offensive buildup a strict quarantine on all offensive military equipment under shipment to Cuba is being initiated. All ships of any kind bound for Cuba from whatever nation or port will, if found to contain cargoes of offensive weapons, be turned back.”[88]  Active strategic management required active information gathering and active problem solving. In the early days of Hewlett-Packard (HP), Dave Packard and Bill Hewlett devised an active management style that they called management by walking around (MBWA). Senior HP managers were seldom at their desks. They spent most of their days visiting employees, customers, and suppliers. This direct contact with key people provided them with a solid grounding from which viable strategies could be crafted. Management consultants Tom Peters and Robert H. Waterman had used the term in their 1982 book In Search of Excellence: Lessons From America's Best-Run Companies.[89] Some Japanese managers employ a similar system, which originated at Honda, and is sometimes called the 3 G's (Genba, Genbutsu, and Genjitsu, which translate into “actual place”, “actual thing”, and “actual situation”). Creative vs analytic approaches  In 2010, IBM released a study summarizing three conclusions of 1500 CEOs around the world: 1) complexity is escalating, 2) enterprises are not equipped to cope with this complexity, and 3) creativity is now the single most important leadership competency. IBM said that it is needed in all aspects of leadership, including strategic thinking and planning.[90]  Similarly, Mckeown argued that over-reliance on any particular approach to strategy is dangerous and that multiple methods can be used to combine the creativity and analytics to create an \"approach to shaping the future\", that is difficult to copy.[91] Non-strategic management  A 1938 treatise by Chester Barnard, based on his own experience as a business executive, described the process as informal, intuitive, non-routinized and involving primarily oral, 2-way communications. Bernard says “The process is the sensing of the organization as a whole and the total situation relevant to it. It transcends the capacity of merely intellectual methods, and the techniques of discriminating the factors of the situation. The terms pertinent to it are “feeling”, “judgement”, “sense”, “proportion”, “balance”, “appropriateness”. It is a matter of art rather than science.”[92]  In 1973, Mintzberg found that senior managers typically deal with unpredictable situations so they strategize in ad hoc, flexible, dynamic, and implicit ways. He wrote, “The job breeds adaptive information-manipulators who prefer the live concrete situation. The manager works in an environment of stimulus-response, and he develops in his work a clear preference for live action.”[93]  In 1982, John Kotter studied the daily activities of 15 executives and concluded that they spent most of their time developing and working a network of relationships that provided general insights and specific details for strategic decisions. They tended to use “mental road maps” rather than systematic planning techniques.[94]  Daniel Isenberg's 1984 study of senior managers found that their decisions were highly intuitive. Executives often sensed what they were going to do before they could explain why.[95] He claimed in 1986 that one of the reasons for this is the complexity of strategic decisions and the resultant information uncertainty.[96]  Zuboff claimed that information technology was widening the divide between senior managers (who typically make strategic decisions) and operational level managers (who typically make routine decisions). She alleged that prior to the widespread use of computer systems, managers, even at the most senior level, engaged in both strategic decisions and routine administration, but as computers facilitated (She called it “deskilled”) routine processes, these activities were moved further down the hierarchy, leaving senior management free for strategic decision making.  In 1977, Abraham Zaleznik distinguished leaders from managers. He described leaders as visionaries who inspire, while managers care about process.[97] He claimed that the rise of managers was the main cause of the decline of American business in the 1970s and 1980s. Lack of leadership is most damaging at the level of strategic management where it can paralyze an entire organization.[98]  Dr Maretha Prinsloo developed the Cognitive Process Profile (CPP) psychometric from the work of Elliott Jacques. The CPP is a computer-based psychometric which profiles a person's capacity for strategic thinking. It is used worldwide in selecting and developing people for strategic roles.  According to Corner, Kinichi, and Keats,[99] strategic decision making in organizations occurs at two levels: individual and aggregate. They developed a model of parallel strategic decision making. The model identifies two parallel processes that involve getting attention, encoding information, storage and retrieval of information, strategic choice, strategic outcome and feedback. The individual and organizational processes interact at each stage. For instance, competition-oriented objectives are based on the knowledge of competing firms, such as their market share.[100] Strategy as marketing  The 1980s also saw the widespread acceptance of positioning theory. Although the theory originated with Jack Trout in 1969, it didn’t gain wide acceptance until Al Ries and Jack Trout wrote their classic book Positioning: The Battle For Your Mind (1979). The basic premise is that a strategy should not be judged by internal company factors but by the way customers see it relative to the competition. Crafting and implementing a strategy involves creating a position in the mind of the collective consumer. Several techniques enabled the practical use of positioning theory. Perceptual mapping for example, creates visual displays of the relationships between positions. Multidimensional scaling, discriminant analysis, factor analysis and conjoint analysis are mathematical techniques used to determine the most relevant characteristics (called dimensions or factors) upon which positions should be based. Preference regression can be used to determine vectors of ideal positions and cluster analysis can identify clusters of positions.  In 1992 Jay Barney saw strategy as assembling the optimum mix of resources, including human, technology and suppliers, and then configuring them in unique and sustainable ways.[101]  James Gilmore and Joseph Pine found competitive advantage in mass customization.[102] Flexible manufacturing techniques allowed businesses to individualize products for each customer without losing economies of scale. This effectively turned the product into a service. They also realized that if a service is mass-customized by creating a “performance” for each individual client, that service would be transformed into an “experience”. Their book, The Experience Economy,[103] along with the work of Bernd Schmitt convinced many to see service provision as a form of theatre. This school of thought is sometimes referred to as customer experience management (CEM). Information- and technology-driven strategy  Many industries with a high information component are being transformed.[104] For example, Encarta demolished Encyclopædia Britannica (whose sales have plummeted 80% since their peak of $650 million in 1990) before it was in turn, eclipsed by collaborative encyclopedias like Wikipedia. The music industry was similarly disrupted. The technology sector has provided some strategies directly. For example, from the software development industry agile software development provides a model for shared development processes.  Peter Drucker conceived of the “knowledge worker” in the 1950s. He described how fewer workers would do physical labor, and more would apply their minds. In 1984, John Naisbitt theorized that the future would be driven largely by information: companies that managed information well could obtain an advantage, however the profitability of what he called “information float” (information that the company had and others desired) would disappear as inexpensive computers made information more accessible.  Daniel Bell (1985) examined the sociological consequences of information technology, while Gloria Schuck and Shoshana Zuboff looked at psychological factors.[105] Zuboff distinguished between “automating technologies” and “informating technologies”. She studied the effect that both had on workers, managers and organizational structures. She largely confirmed Drucker's predictions about the importance of flexible decentralized structure, work teams, knowledge sharing and the knowledge worker's central role. Zuboff also detected a new basis for managerial authority, based on knowledge (also predicted by Drucker) which she called “participative management”.[106] Maturity of planning process  McKinsey & Company developed a capability maturity model in the 1970s to describe the sophistication of planning processes, with strategic management ranked the highest. The four stages include:      Financial planning, which is primarily about annual budgets and a functional focus, with limited regard for the environment;     Forecast-based planning, which includes multi-year budgets and more robust capital allocation across business units;     Externally oriented planning, where a thorough situation analysis and competitive assessment is performed;     Strategic management, where widespread strategic thinking occurs and a well-defined strategic framework is used.[18]  PIMS study  The long-term PIMS study, started in the 1960s and lasting for 19 years, attempted to understand the Profit Impact of Marketing Strategies (PIMS), particularly the effect of market share. The initial conclusion of the study was unambiguous: the greater a company's market share, the greater their rate of profit. Market share provides economies of scale. It also provides experience curve advantages. The combined effect is increased profits.[107]  The benefits of high market share naturally led to an interest in growth strategies. The relative advantages of horizontal integration, vertical integration, diversification, franchises, mergers and acquisitions, joint ventures and organic growth were discussed. Other research indicated that a low market share strategy could still be very profitable. Schumacher (1973),[108] Woo and Cooper (1982),[109] Levenson (1984),[110] and later Traverso (2002)[111] showed how smaller niche players obtained very high returns. Other influences on business strategy Military strategy See also: Military strategy  In the 1980s business strategists realized that there was a vast knowledge base stretching back thousands of years that they had barely examined. They turned to military strategy for guidance. Military strategy books such as The Art of War by Sun Tzu, On War by von Clausewitz, and The Red Book by Mao Zedong became business classics. From Sun Tzu, they learned the tactical side of military strategy and specific tactical prescriptions. From von Clausewitz, they learned the dynamic and unpredictable nature of military action. From Mao, they learned the principles of guerrilla warfare. Important marketing warfare books include Business War Games by Barrie James, Marketing Warfare by Al Ries and Jack Trout and Leadership Secrets of Attila the Hun by Wess Roberts.  The four types of business warfare theories are:      Offensive marketing warfare strategies     Defensive marketing warfare strategies     Flanking marketing warfare strategies     Guerrilla marketing warfare strategies  The marketing warfare literature also examined leadership and motivation, intelligence gathering, types of marketing weapons, logistics and communications.  By the twenty-first century marketing warfare strategies had gone out of favour in favor of non-confrontational approaches. In 1989, Dudley Lynch and Paul L. Kordis published Strategy of the Dolphin: Scoring a Win in a Chaotic World. \"The Strategy of the Dolphin” was developed to give guidance as to when to use aggressive strategies and when to use passive strategies. A variety of aggressiveness strategies were developed.  In 1993, J. Moore used a similar metaphor.[112] Instead of using military terms, he created an ecological theory of predators and prey(see ecological model of competition), a sort of Darwinian management strategy in which market interactions mimic long term ecological stability.  Author Phillip Evans said in 2014 that \"Henderson's central idea was what you might call the Napoleonic idea of concentrating mass against weakness, of overwhelming the enemy. What Henderson recognized was that, in the business world, there are many phenomena which are characterized by what economists would call increasing returns -- scale, experience. The more you do of something, disproportionately the better you get. And therefore he found a logic for investing in such kinds of overwhelming mass in order to achieve competitive advantage. And that was the first introduction of essentially a military concept of strategy into the business world... It was on those two ideas, Henderson's idea of increasing returns to scale and experience, and Porter's idea of the value chain, encompassing heterogenous elements, that the whole edifice of business strategy was subsequently erected.\"[113] Traits of successful companies  Like Peters and Waterman a decade earlier, James Collins and Jerry Porras spent years conducting empirical research on what makes great companies. Six years of research uncovered a key underlying principle behind the 19 successful companies that they studied: They all encourage and preserve a core ideology that nurtures the company. Even though strategy and tactics change daily, the companies, nevertheless, were able to maintain a core set of values. These core values encourage employees to build an organization that lasts. In Built To Last (1994) they claim that short term profit goals, cost cutting, and restructuring will not stimulate dedicated employees to build a great company that will endure.[114] In 2000 Collins coined the term “built to flip” to describe the prevailing business attitudes in Silicon Valley. It describes a business culture where technological change inhibits a long term focus. He also popularized the concept of the BHAG (Big Hairy Audacious Goal).  Arie de Geus (1997) undertook a similar study and obtained similar results.[115] He identified four key traits of companies that had prospered for 50 years or more. They are:      Sensitivity to the business environment — the ability to learn and adjust     Cohesion and identity — the ability to build a community with personality, vision, and purpose     Tolerance and decentralization — the ability to build relationships     Conservative financing  A company with these key characteristics he called a living company because it is able to perpetuate itself. If a company emphasizes knowledge rather than finance, and sees itself as an ongoing community of human beings, it has the potential to become great and endure for decades. Such an organization is an organic entity capable of learning (he called it a “learning organization”) and capable of creating its own processes, goals, and persona.[115]  Will Mulcaster[116] suggests that firms engage in a dialogue that centres around these questions:      Will the proposed competitive advantage create Perceived Differential Value?\"     Will the proposed competitive advantage create something that is different from the competition?\"     Will the difference add value in the eyes of potential customers?\" – This question will entail a discussion of the combined effects of price, product features and consumer perceptions.     Will the product add value for the firm?\" – Answering this question will require an examination of cost effectiveness and the pricing strategy.  See also      Balanced Scorecard     Business analysis     Business model     Business plan     Business Strategy Mapping     Concept Driven Strategy     Cost overrun     Cognitive Process Profile  \t      Dynamic capabilities     Integrated business planning     Marketing     Marketing plan     Marketing strategies     Management  \t      Management consulting     Military strategy     Morphological analysis     Overall equipment effectiveness     Real options valuation     Results-based management     Revenue shortfall  \t      Strategy (game theory)     Strategy dynamics     Strategic planning     Strategic Management Society     Strategy map     Strategy Markup Language     Strategy visualization     Value migration     Six Forces Model     Adversarial purchasing  Any person, corporation, or nation should know who or where they are, where they want to be, and how to get there.[2] The strategic-planning process utilizes analytical models that provide a realistic picture of the individual, corporation, or nation at its “consciously incompetent” level, creating the necessary motivation for the development of a strategic plan.[3] The process requires five distinct steps outlined below and the selected strategy must be sufficiently robust to enable the firm to perform activities differently from its rivals or to perform similar activities in a more efficient manner.[4]  A good strategic plan includes metrics that translate the vision and mission into specific end points.[5] This is critical because strategic planning is ultimately about resource allocation and would not be relevant if resources were unlimited. This article aims to explain how finance, financial goals, and financial performance can play a more integral role in the strategic planning and decision-making process, particularly in the implementation and monitoring stage. The Strategic-Planning and Decision-Making Process  1. Vision Statement  The creation of a broad statement about the company’s values, purpose, and future direction is the first step in the strategic-planning process.[6] The vision statement must express the company’s core ideologies—what it stands for and why it exists—and its vision for the future, that is, what it aspires to be, achieve, or create.[7]  2. Mission Statement  An effective mission statement conveys eight key components about the firm: target customers and markets; main products and services; geographic domain; core technologies; commitment to survival, growth, and profitability; philosophy; self-concept; and desired public image.[8] The finance component is represented by the company’s commitment to survival, growth, and profitability.[9] The company’s long-term financial goals represent its commitment to a strategy that is innovative, updated, unique, value-driven, and superior to those of competitors.[10]  3. Analysis  This third step is an analysis of the firm’s business trends, external opportunities, internal resources, and core competencies. For external analysis, firms often utilize Porter’s five forces model of industry competition,[11] which identifies the company’s level of rivalry with existing competitors, the threat of substitute products, the potential for new entrants, the bargaining power of suppliers, and the bargaining power of customers.[12]  For internal analysis, companies can apply the industry evolution model, which identifies takeoff (technology, product quality, and product performance features), rapid growth (driving costs down and pursuing product innovation), early maturity and slowing growth (cost reduction, value services, and aggressive tactics to maintain or gain market share), market saturation (elimination of marginal products and continuous improvement of value-chain activities), and stagnation or decline (redirection to fastest-growing market segments and efforts to be a low-cost industry leader).[13]  Another method, value-chain analysis clarifies a firm’s value-creation process based on its primary and secondary activities.[14] This becomes a more insightful analytical tool when used in conjunction with activity-based costing and benchmarking tools that help the firm determine its major costs, resource strengths, and competencies, as well as identify areas where productivity can be improved and where re-engineering may produce a greater economic impact.[15]  SWOT (strengths, weaknesses, opportunities, and threats) is a classic model of internal and external analysis providing management information to set priorities and fully utilize the firm’s competencies and capabilities to exploit external opportunities,[16] determine the critical weaknesses that need to be corrected, and counter existing threats.[17]  4. Strategy Formulation  To formulate a long-term strategy, Porter’s generic strategies model [18] is useful as it helps the firm aim for one of the following competitive advantages: a) low-cost leadership (product is a commodity, buyers are price-sensitive, and there are few opportunities for differentiation); b) differentiation (buyers’ needs and preferences are diverse and there are opportunities for product differentiation); c) best-cost provider (buyers expect superior value at a lower price); d) focused low-cost (market niches with specific tastes and needs); or e) focused differentiation (market niches with unique preferences and needs).[19]  5. Strategy Implementation and Management  In the last ten years, the balanced scorecard (BSC)[20] has become one of the most effective management instruments for implementing and monitoring strategy execution as it helps to align strategy with expected performance and it stresses the importance of establishing financial goals for employees, functional areas, and business units. The BSC ensures that the strategy is translated into objectives, operational actions, and financial goals and focuses on four key dimensions: financial factors, employee learning and growth, customer satisfaction, and internal business processes.[21] The Role of Finance  Financial metrics have long been the standard for assessing a firm’s performance. The BSC supports the role of finance in establishing and monitoring specific and measurable financial strategic goals on a coordinated, integrated basis, thus enabling the firm to operate efficiently and effectively. Financial goals and metrics are established based on benchmarking the “best-in-industry” and include:  1. Free Cash Flow  This is a measure of the firm’s financial soundness and shows how efficiently its financial resources are being utilized to generate additional cash for future investments.[22] It represents the net cash available after deducting the investments and working capital increases from the firm’s operating cash flow. Companies should utilize this metric when they anticipate substantial capital expenditures in the near future or follow-through for implemented projects.  2. Economic Value-Added  This is the bottom-line contribution on a risk-adjusted basis and helps management to make effective, timely decisions to expand businesses that increase the firm’s economic value and to implement corrective actions in those that are destroying its value.[23] It is determined by deducting the operating capital cost from the net income. Companies set economic value-added goals to effectively assess their businesses’ value contributions and improve the resource allocation process.  3. Asset Management  This calls for the efficient management of current assets (cash, receivables, inventory) and current liabilities (payables, accruals) turnovers and the enhanced management of its working capital and cash conversion cycle. Companies must utilize this practice when their operating performance falls behind industry benchmarks or benchmarked companies.  4. Financing Decisions and Capital Structure  Here, financing is limited to the optimal capital structure (debt ratio or leverage), which is the level that minimizes the firm’s cost of capital. This optimal capital structure determines the firm’s reserve borrowing capacity (short- and long-term) and the risk of potential financial distress.[24] Companies establish this structure when their cost of capital rises above that of direct competitors and there is a lack of new investments.  5. Profitability Ratios  This is a measure of the operational efficiency of a firm. Profitability ratios also indicate inefficient areas that require corrective actions by management; they measure profit relationships with sales, total assets, and net worth. Companies must set profitability ratio goals when they need to operate more effectively and pursue improvements in their value-chain activities.  6. Growth Indices  Growth indices evaluate sales and market share growth and determine the acceptable trade-off of growth with respect to reductions in cash flows, profit margins, and returns on investment. Growth usually drains cash and reserve borrowing funds, and sometimes, aggressive asset management is required to ensure sufficient cash and limited borrowing.[25] Companies must set growth index goals when growth rates have lagged behind the industry norms or when they have high operating leverage.  7. Risk Assessment and Management  A firm must address its key uncertainties by identifying, measuring, and controlling its existing risks in corporate governance and regulatory compliance, the likelihood of their occurrence, and their economic impact. Then, a process must be implemented to mitigate the causes and effects of those risks.[26] Companies must make these assessments when they anticipate greater uncertainty in their business or when there is a need to enhance their risk culture.  8. Tax Optimization  Many functional areas and business units need to manage the level of tax liability undertaken in conducting business and to understand that mitigating risk also reduces expected taxes.[27] Moreover, new initiatives, acquisitions, and product development projects must be weighed against their tax implications and net after-tax contribution to the firm’s value. In general, performance must, whenever possible, be measured on an after-tax basis. Global companies must adopt this measure when operating in different tax environments, where they are able to take advantage of inconsistencies in tax regulations. Conclusion  The introduction of the balanced scorecard emphasized financial performance as one of the key indicators of a firm’s success and helped to link strategic goals to performance and provide timely, useful information to facilitate strategic and operational control decisions. This has led to the role of finance in the strategic planning process becoming more relevant than ever.  Empirical studies have shown that a vast majority of corporate strategies fail during execution. The above financial metrics help firms implement and monitor their strategies with specific, industry-related, and measurable financial goals, strengthening the organization’s capabilities with hard-to-imitate and non-substitutable competencies. They create sustainable competitive advantages that maximize a firm’s value, the main objective of all stakeholders. Management Information Systems (MIS) is the key factor to facilitate and attain efficient decision making in an organization. This research explores the extent to which management information systems implemented to make successful decisions at two selected financial organizations. The research examined whether the selected financial institutions of Bahrain vary as to the use of Management Information Systems leadership of decision making for strategic and tactical planning purposes. The research adapted the quantitative research design to examine two research hypotheses. A total of 190 forms were equally distributed to those who are working at different management levels at the selected organizations. The results of the research showed that MIS was primarily used to enhance strategic planning in both financial institutions. The regression analysis revealed that Tactical planning is found to have no effect on Decision Making, while Strategic planning has a clear effect on the Decision Making Effectiveness in both organizations.  Keywords: Management Information Systems, Strategic Planning, Tactical Planning, Decision Making Process.        1. INTRODUCTION  Currently, organizations are in the race for enhancing their capability in order to survive in the competitions of the new century global market. Therefore, organizations are attempting to advance their agility level by improving the decision making process to be more efficient and highly effective to meet the successive fluctuations of the market. In an effort to achieve this, many modern organizations, either mid or large sized, have concerned with a cycle of progressive investments in and adopted new management information systems components. During last decade, a high percentage of financial organizations frequently used Management Information Systems to facilitate the provision of services; and that the speed of the adoption is expected to grow further as the technology expands.  Whitten et al. (2004, p.12.), stated that \"information is an arrangement of people, data, process, and information technology that interact to collect, process, store and provide as output the information needed to support an organization,\" which indicates that information system is an arrangement of groups, data, processes and technology that act together to accumulate, process, store and provide information output needed to enhance and speed up the process of decision making. In a Bank's information system, there is always a potential crisis which makes the bank endure an insufficiency; thus, an advanced information system supported by a superior mechanism control is required to make certain that an information system has achieved the required processes.  If the relevant information required in a decision-making process or an organization planning is not available at the appropriate time, then there is a good change to be a poor organization planning, inappropriate decision-making, poor priority of needs, and defective programming or scheduling of activities (Adebayo, 2007).  Information is essential for the endurance of a financial organization in the global and competitive market.  The nature of globalization and competitiveness in the market stress on the importance of developing an organization capability through better enhancing MIS.  Accordingly, the stored information must then be recalled and distributed for the use of an organization leadership and top management as well as mid-level managers to take effective long term (strategic) and short term (Tactical) decision-making. MIS is deemed to be a system which provides organizations top management and, even lower level management, with appropriate information based on data from both internal and external sources, to allow them to make effective and timely decisions that best achieve their organization goals and satisfy stakeholder requirements (Argyris, 1971, p. 291).  The conception of information catches the attention of different professionals from different fields such as computer science, economics, business and management, political science, statistics, communication and information studies (Newman 2001). However, the question is \"what type of information\"? How Information management can play an essential role in the decision making process? How can the coordination between different departments (internal and/or external) and sharing information at the real time accelerate and enhance the process of decision making and avoid decision making errors?  This paper focuses on how information management is needed to generate proper planning and then decisions at both strategic and tactical levels in the two selected financial organizations.  The process of dealing with the financial institutions was tainted by a lot of sensitivity, because of the refusal of those institutions to reveal their decision-making mechanism due to their Disclosure Rules. So we decided to call the first selected institutions case one and case two referring to the second selected organizations.  1.1 Significant Research  Few authors have explored that the critical information required by midlevel and strategic level management is efficiently provided by MIS. A small amount of research has deliberated that the limitations and deficiencies in the process of management information system performance are the main reason for diminishing the efficiency of decision-making process in the organization (Fabunmi, 2003; Knight Moore, 2005).  The questions related to what extent the managing of these information systems assists different decisions at different management levels and the type of responsibility of the financial institution's senior and tactical management in enhancing the management information has been raised with low empirically investigation and examination.  1.2 The purpose of the research  The purpose of this research is to explore the extent to which management information systems are used to make effective decisions of long and short term planning in two financial organizations at the Kingdom of Bahrain. The study will examine whether the government financial institution (Case one) and the Private financial institution (Case two) differ as to the use of management information systems for leadership decision makes in short and long term planning.  This paper aims to evaluate the impact of current MIS models being developed at the selected organizations, and how far they practice this concept in order to enhance their tactical and strategic planning.  1.3 The research organization  The remainder of this paper is organized as follows. Sections 2 and 3 discuss the literature review and research methods. In Section 4, we present results and analysis. In section 5, a discussion will be presented. Finally, conclusion and recommendations are presented in sections 6 and 7, respectively.     2 LITERATURE REVIEW  There is a lof of research on the approaches, techniques and technologies for the design and development of MIS. However, there are a few articles that cover the impact of Management Information Systems on planning strategies and decision making. While there are no universally accepted definitions of MIS and those that exist in literatures are just prejudices of the researchers (Adeoti-Adekeye, 1997). Lee, (2001) defined MIS as \"a system or process that provides information needed to manage organizations effectively\".  Additionally, Baskerville and Myers (2002) broadly define MIS as \"the development, use and application of information systems by individuals, organizations and society\". In his study, Becta (2005) describes an information system as \"a system consisting of the network of all communication channels used within an organization\". In their study, Laudon and Laudon (2003) have defined MIS as \"the study of information systems focusing on their use in business and management\". The abovementioned definitions showed that MIS has underlined the development, application and validation of relevant theories and models in attempts to encourage quality work in the area.  Referring to the literatures, the field of Management Information Systems (MIS) has had a variegated development in its relatively short life span. MIS has developed its own theme of research and studies (Baskerville and Myers, 2002).  Tracing previous literatures, we can report that during its first few decades, MIS concentrated on the information in the context of:      • Electronic data processing which carries out transaction processing functions and records detailed factual data.      • Management reporting systems which scrutinize the operational activities of an organization, providing summaries, information and feedback to management.  Only during the last two decades, the MIS field has shifted to the primary, considered the second type of communication, namely, instruction-based. This has become known as the domain of expert systems (Sasan Rahmatian, 1999). In attempts to review published studies on MIS and articles, Alavi and Carlson (1992) have identified popular research topics, the dominant research perspective, and the relationship between MIS research and practice. In contrast, Baskerville and Myers (2002) have examined the MIS field and found a constant shift of MIS research from a technical focus to a technology-organizational and management-social focus.  Skyrius (2001) underlines the decision maker's attitudes towards different factors influencing the quality of business decisions; these factors include information sources, analytical tools, and the role of information technologies.  Handzic (2001) also pays attention to the impact of information availability on people's ability to process and use information in short and long term planning and in decision making tasks. He revealed that the better the availability of information, the better the impact on both efficiency and accuracy of business decisions.  Liu and Young (2007) talk about key information models and their relationships in business decision support in three different scenarios. The authors proved that global businesses are in advance due to the Enterprise Applications System provided by modern IT tools such as Enterprise Resource Planning (ERP), Knowledge Management Systems (KMS) and Customer Relations Management (CRM) to enhance the efficiency and effectiveness of the Decision Making process.  In order to improve the financial organizational capability and enhance its level of competition in the market, financial organizations should understand the dimensions of the Information Management, and clearly define and develop the resources in case of human, technological, and internal operations, among others,, and manage them well across the organizational boundaries. However, establishing the link between Information System Management, planning and decision making is, at best, tricky.  In an article by Shu and Strassmann (2005), a survey was conducted at 12 banks in the US between 1989 and 1997. They noticed that even though Information Technology had been one of the most essentially dynamic factors relating all efforts, it could not improve banks' earnings.  However, conversely, there are many literatures approving the positive impacts of Information Technology expenses on business value. Kozak (2005) investigates the influence of the evolution in Information Technology on the profit and cost effectiveness of the banking zone during the period between 1992 and 2003. The study indicates an optimistic relationship among the executed Information Technology, productivity and cost savings.  Organizations that do not have formal Information sharing practices will fail to leverage their managers' intellectual capital for business innovation and growth (O'Neill & Adya, 2007). MIS enables the exchange of experiences, which transfers the required information to the management levels to sustain competitive advantage since it affects the decision making to improve the quality of services provided. Therefore, Barachini et al. (2009) supported that it is imperative that these organizations continuously motivate their employees to share valuable information so that their intellectual capital can be leveraged.  Management Information System will give the banking management a new dimension in managing its knowledge and help in carrying out and maximizing the management's initiatives in harmonizing the appropriate strategies in the short and long planning (Edmondson, 2002).  In his study, Obi (2003) suggested that MIS is indispensible in the area of decision-making as it can monitor by itself the instability in a system, verify a course of action and take action to keep the system in control. Literatures also suggested that non-programmed decisions are relevant as they provide support by supplying information to the search, the analysis, the evaluation and the choice and implementation process of decision making.  More recently, Adebayo (2007) explained that the existence of MIS is needed to improve and enhance decision making on the issues affecting human and material resources.  From the literatures presented, we can easily perceive that the importance of the role of both middle and top management to maintain a consistent approach to develop, use, and evaluate MIS systems within the institution. To financial institutions, MIS is used at various levels by top-management, middle and even by the operational staff as a support for decision making that aims to meet strategic goals and strategic objectives.  The above literatures also explore the importance of MIS in providing decision makers with facts, which consequently support and enhance the entire decision-making process. Furthermore, at the most senior level, MIS and DSS supply the data and required information to assist the board of directors and management levels to make an accurate and on time strategic decisions.     3 METHODOLOGY  The current study attempts to explain the relationship between various factors. Due to the nature of the current study and its hypothesis, the primary research purpose of the current study is, thus, explanatory. Explanatory (or causal) explains the complexity of the interrelated variables identified that were posited in the hypothesis and research  By developing several hypotheses, the study thus adapt the quantitative research design to better test those hypotheses. Quantitative research uses survey as the main instrument to collect data.  3.2 Research questions and hypotheses  To achieve the purpose of the current study, the following research questions have been formulated:      1. To what extent is MIS being utilized to support Strategic planning for decisions in Bahrain's financial organizations?      2. To what extent is MIS being utilized to support Tactical planning for decisions in Bahrain's financial organizations?  To answer these questions, the current study carries out various hypotheses that developed from previous literatures and studies (Ajayi et. al, 2007). Figure 1 presents the proposed model and factors affecting the process of decision making.        Thus, we consider the hypothesis below:      a. H1: The Tactical Planning (short term) generated by MIS is positively affecting the decision making process.      b. H2: The Strategic Planning (long term) generated by MIS is positively affecting the decision making process.  3.3 Survey Instrument  The participants were asked to indicate their perception on a likert scales (1-5) with response ranging from \"strongly disagree\" to \"strongly agree\". The collected data were analyzed based on correlation and regression analyses using the statistical package for social sciences (SPSS).  The questionnaire of this study is adapted from previous literature and studies (e.g. Ajayi et. al, 2007). The main reason why we need to refer to literatures when developing a questionnaire is to ensure the high reliability and validity of the survey.  The questionnaire we prepared for this paper was divided into 2 sections. The first section concentrates on the general profile of the respondent including his/her age group, education level and profession and income group. In the second section we were interested in gathering information about the importance of MIS and its use in Case one and Case two of financial institutions in Bahrain. The respondents were provided with a list of questions related to the following factors;      • The important of MIS in supporting Tactical planning and consequently enhancing the decision-making process      • The important of MIS in supporting Strategic planning and consequently enhancing the decision-making process      • The effectiveness of a bank's decision making process  Using a personal relationship, the questionnaire has been discussed with various levels of bank management and a pilot study has been conducted amongst low-level members of the bank's staff. This improved the questionnaire, which has been used for the research presented in this study.  3.4 Population and Data collection  The population for the study is the top management (strategic), mid level management (tactical) and normal staff in Case one (government financial institution) and Case two (private financial institution). This population is deemed to be fully aware of the MIS use at the bank level.  A total of 190 forms were distributed equally (each bank with 95 survey forms). 12% for case one and 14% for case two are having management positions. In a convenience sample, the managers and bank staff were randomly approached.  The distribution took place for a three-week period in early March, 2010. The survey was designed in English language. The research tested the time to fill the survey and it took approximately 5-8 minutes to be completed.  The questionnaire we prepared and used had been pre-tested initially with few people (5 users) working in different sectors to ensure consistency, clarity and relevance to the Bahraini case. Minor changes (related to the questions content, wording, and sequence) were requested by those people, which we implemented before carrying out the final copy.  3.5 Data Analysis  The current study used SPSS (Statistical Package for Social Science), software V.19 to analyze the data obtained from the survey. The current study use ANOVA to see if there are any differences between Case one and Case two when using MIS for supporting decision making processes. Moreover, the simple Regression was implemented to test the hypothesis and the linkages between dependent and independent variables.     4 RESULTS AND ANALYSIS  This section presents the factor analysis test. Then, the results of correlation analysis of the three variables (Strategic planning, Tactical Planning, and The Effectiveness of Decision-making) are discussed. Finally, the results of the paper are discussed in accordance with the research objectives and hypothesis of the study.  4.2 Correlation Tests  Correlation analysis was incorporated to describe the strength and direction of the linear relationship between the two independent variables and the dependent variable (Pallant, 2001). Effectiveness of the bank's decision-making processes, which are the dependent variables considered to be the bank's succession in management building and facility, financial aspects and staff issues. Previous studies underpinned the importance of conducting correlation tests before the regression testing, the correlation between variables thus necessary (Coakes and Steed, 2007). The result of the correlations is presented in the following table (table 1).  The results of correlation reveals that Strategic planning (r=0. 318, p < 0.01) found to be strongly and positively correlated with the bank's Decision Making (D.M) Effectiveness, while Tactical Planning (r=0. 263, p < 0.05) found to be positively correlated with the bank's D.M Effectiveness.  4.3 Regression  For further analysis, a Linear Regression analysis was conducted to examine the extent to which the independent variables (Strategic planning and Tactical Planning) influence the succession Effectiveness of the bank's decision making (dependent variable). The independent variables were regressed across organizational outcomes. Tables 2, 3 and 4 summarized the results of the Linear Regression analysis.  The results of regression reveals that the model is significant (p < 0.01) and the coefficient of determination (R2) for the regression is (0.490), indicating that (49%) of the variation in the dependent variable (decision-making effectiveness) was explained by the independent variables included in the regression. The results of regression indicated that the variance in the Effectiveness of the bank's decision making is explained by only one variable; Strategic planning, while Tactical Planning found not to affect the Effectiveness of the bank's decision making process.  The regression analysis was implemented to support the correlation test. However, the study revealed that the Tactical planning is found to have no effect on D.M Effectiveness (Sig=.128 > 0.05). The regression analysis showed that Strategic planning, on the other hand (Sig=.016 < 0.05), affects the D.M Effectiveness in the bank.  Table 5 reveals the research hypotheses accepting/rejecting based on the regression analysis.     5 DISCUSSION  The current study intended to measure the implementation and the use of MIS in two banks in the Kingdom of Bahrain. Two different planning activities have been identified to measure the banks implementation and use of MIS, these activities are: Strategic planning and Tactical Planning. Correlation analysis was incorporated to describe the strength and direction of the linear relationship between the two independent variables and the dependent variable.  The results of the descriptive statistics reveal that MIS was primarily used to enhance Strategic planning (long term) in the bank. The study also revealed that MIS is the least implemented in the Tactical Planning (short term).  Correlation test thus implemented investigate the relationship between the two variables. The result reveals that the two variables in this study are correlated with the effectiveness of the decision-making process in the banks. As for Cohen (1992), the result revealed that the Strategic planning is medium correlated with the effectiveness of the decision making process in the bank, while Tactical planning indicates small correlation with the effectiveness of the decision making process in the bank.  Moreover, the results of regression indicated that the variance in the Effectiveness of the bank's decision making process is explained by only one variable, the Strategic planning, while Tactical Planning found not to affect the Effectiveness of the bank's decision making process.     6 CONCLUSION  Management Information Systems is of paramount importance to reach effective decisions in an organization. The literatures presented in this study explained the significant role of MIS in the decision-making process enhancing in an organization. MIS is deemed to be an integrated user-machine system that provides information to support operations, management and decision-making functions at various levels of an organization. Organizations are aware that MIS is a special-purpose system useful for management objectives. The study has highlighted that MIS should be accessible in supplying appropriate and high quality of information from its generation to its users. To MIS, to be vital and effective, a carefully conceived, designed and executed database should exist to communicate the adaptive decisions.  The study has developed two independent variables (Strategic planning and Tactical planning) and one dependant variable (the Effectiveness of Bank Decisionmaking). To answer the research questions and to test the hypotheses, the study adapted the quantitative research design and implemented advance statistic methods (Correlation, ANOVA and regression). The study considered applied research as the results of this study expected to specifically assist bank top management and organizations in general to develop MIS designing, maintaining and implementation in order to enhance the process of decision-making.  In short, the results of the descriptive statistics revealed that MIS primarily used to enhance Strategic planning in the banks. The study also revealed that MIS is the least implemented in the Tactical planning.  Correlation analysis was incorporated to describe the strength and direction of the linear relationship between the two study variables. The results of correlation revealed that Strategic planning and Tactical Planning are positively correlated with the bank D.M Effectiveness.  For further analysis, a Linear Regression analysis was conducted to examine the extent to which the independent variables (Strategic planning and Tactical Planning) influence the succession Effectiveness of the bank decision making (dependent variable). The results of regression revealed that the model is significant and the coefficient of determination (R) for the regression is (0.490). The result of regression indicated that the variance in the Effectiveness of the bank decision making is explained by only one variable: Strategic planning. However, the study revealed that Tactical Planning is found to have no effect on D.M Effectiveness.     7 RECOMMENDATIONS  Based on the findings of this study, MIS was not very adequately implemented for decision making on Strategic planning and Tactical Planning in Bahrain banks. Although there are differences (Case one) in the use of MIS for decision-making processes, the results revealed that the effectiveness of decision making is similar to the Case two. It is therefore recommended that the MIS units should be adequately maintained to ensure the free flow of information and adequate use of MIS in decision making in Strategic and Tactical Planning. The study also recommends that a proper orientation should be conducted in order to help managers at all levels as to ensure proper and adequate use of MIS facilities in generating and disseminating information for better decisions in the banks.  The organization management does not always recognize the information needed, while the information professionals often do not comprehend and are aware of management in order to produce relevant information for the managers they serve. To be successful, an MIS should be designed and operated related to organizations, management and technical factors. The study encourages the organizations' top management to participate in enhancing MIS and make an effective contribution to system design. The information specialists (including systems analysts, designer, ITC personnel, accountants and operations researchers) should cooperate and become more conscious and ware of managerial functions needs so that more effective MIS is developed.  Finally, we believe that to enhance MIS, banks or any organization that use MIS to enhance its decision making processes should develop effective communication channels between management and information professionals. Good communications then facilitates the task of developing relevant and appropriate information systems. It is no simple checklist to automatically produce the perfect MIS. Organization thus would be aware that what is required is an awareness and understanding of key principles and functions in which the design, implementation and operation of MIS are the results of rational decisions rather than haphazard development without considering the real organizational needs.", "category": "Edison", "id": 129}
{"skillName": "DSENG06", "skillText": "Analytic applications are a type of business application software, used to measure and improve the performance of business operations. More specifically, analytic applications are a type of business intelligence solution. As such they use collections of historical data about business operations to provide business users with information and tools that allow them to make improvements in business functions.  The maturity levels for business intelligence solutions are as follows:      operational reporting     analytic reporting     business dashboards     analytic applications  It may extend further to predictive analytics, or predictive analysis may form part of the analytic application - depending on both the subject matter under analysis, and the nature of the analysis required.  Analytic applications are typically described as a subset of performance management. They specifically relate to the analysis of a business process (such as sales pipeline analysis, accounts payable analytics, or risk adjusted profitability analysis) in support of decision making.  To qualify as an application (rather than simply as a data warehousing tool), these tools should promote some form of automation. The maturity level of this automation is as follows:      reading data from a nominated operational system (ERP, CRM, SCM, etc.) into a data warehouse optimized for analysis (data led automation),     reports, dashboards and scorecards based on that data structure (reporting led automation),     what-if analysis and scenario-modeling (predictive or analytic led automation).  In most cases, these three levels are discrete functions, loosely banded together as a single product, and there is little automation of the process from end to end. Collecting reams of big data is one thing; doing something useful with all that information is another. But the former without the latter won't win business intelligence, analytics and IT teams any plaudits from corporate executives. As Gartner Inc. analyst Doug Laney put it at the consulting company's 2013 Business Intelligence and Analytics Summit in Grapevine, Texas, successful big data projects depend on companies \"recognizing that there are opportunities to really innovate with this information\" -- and then taking the required steps to capitalize on those opportunities.  That's where big data analytics applications come in. Finding the business value hidden in hoards of big data can be a tough nut to crack -- but there's a growing body of examples that can help organizations get cracking. And more businesses are getting going. At the 2013 Pacific Northwest BI Summit, a gathering of about 20 IT vendor execs and industry consultants held in Grants Pass, Ore., Claudia Imhoff, president of consultancy Intelligent Solutions Inc., said she was seeing companies \"starting to move more to the proactive side than the reactive side\" on big data analytics. There's a compelling reason to do so, Imhoff noted: \"As much as it's hyped, big data does open doors to things we couldn't do five years ago, or even two years ago.\"  SearchBusinessAnalytics has published a variety of articles and multimedia items that can help prepare your organization to walk through those doors. We've compiled many of them in a comprehensive guide to big data analytics tools and project management best practices. Separately, in a pair of articles, consultant Lyndsay Wise outlines key factors to consider in planning a big data analytics architecture and offers advice on managing successful projects; meanwhile, consultant Rick Sherman details big data analytics worst practices that you'll want to avoid. Wayne Eckerson, TechTarget's BI and analytics industry analyst, discusses big data analytics basics in one video interview and assesses the potential benefits of using Hadoop systems in another. David Loshin, another consultant, weighs in on the upgraded analytical capabilities of Hadoop 2. In addition, we catalog tips on building analytical models with big data and look at the potential big data punch of in-memory analytics software. That's just a sample of our editorial content on big data analytics applications and projects -- and our doors are always open to you. In management information systems, a dashboard is \"an easy to read, often single page, real-time user interface, showing a graphical presentation of the current status (snapshot) and historical trends of an organization’s or computer appliance's key performance indicators to enable instantaneous and informed decisions to be made at a glance.\"[1] Dashboards often provide at-a-glance views of KPIs (key performance indicators) relevant to a particular objective or business process (e.g. sales, marketing, human resources, or production).[2] In real-world terms, \"dashboard\" is another name for \"progress report\" or \"report.\"  Often, the \"dashboard\" is displayed on a web page that is linked to a database which allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.[3]  The term dashboard originates from the automobile dashboard where drivers monitor the major functions at a glance via the instrument cluster.  Contents      1 Benefits     2 Classification     3 Types of dashboards     4 Dashboards and scoreboards     5 Design     6 Assessing the quality of dashboards     7 History     8 See also         8.1 Dashboard software         8.2 Related terms     9 References     10 Further reading  Benefits  Digital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a \"snapshot\" of performance.  Benefits of using digital dashboards include:[3]      Visual presentation of performance measures     Ability to identify and correct negative trends     Measure efficiencies/inefficiencies     Ability to generate detailed reports showing new trends     Ability to make more informed decisions based on collected business intelligence     Align strategies and organizational goals     Saves time compared to running multiple reports     Gain total visibility of all systems instantly     Quick identification of data outliers and correlations  Classification  Dashboards can be broken down according to role and are either strategic, analytical, operational, or informational.[4] Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment's notice. Types of dashboards Dashboard of Sustainability screen shot illustrating example dashboard layout.  Digital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then drill down into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.  Three main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as desktop widgets. The last are driven by a widget engine.  Specialized dashboards may track all corporate functions. Examples include human resources, recruiting, sales, operations, security, information technology, project management, customer relationship management and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard[5] tracks important activities across lot of domains ranging from social media to sales  Digital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the metrics that were chosen for monitoring. Key performance indicators, balanced scorecards, and sales performance figures are some of the content appropriate on business dashboards. Dashboards and scoreboards  Balanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. A balanced scoreboard has what they called a \"prescriptive\" format. It should always contain these components (Active Strategy)...      Perspectives – groupings of high level strategic areas     Objectives – verb-noun phrases pulled from a strategy plan     Measures – also called Metric or Key Performance Indicators (KPIs)     Spotlight Indicators – red, yellow, or green symbols that provide an at-a-glance view of a measure’s performance.  Each of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.  The design of a dashboard is more loosely defined. Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted. Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards. However, dashboards can be customized to link their graphs and charts to strategic objectives.[6] Design  Digital dashboard technology is available \"out-of-the-box\" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, GE Aviation has developed a proprietary software/portal called \"Digital Cockpit\" to monitor the trends in aircraft spare parts business.  A good information design will clearly communicate key information to users and makes supporting information easily accessible.[7]  Assessing the quality of dashboards  There are four key elements to a good dashboard:.[8]      Simple, communicates easily     Minimum distractions...it could cause confusion     Supports organized business with meaning and useful data     Applies human visual perception to visual presentation of information  In management information systems, a dashboard is      \"An easy to read, often single page, real-time user interface, showing a graphical presentation of the current status (snapshot) and historical trends of an organization’s key performance indicators (KPIs) to enable instantaneous and informed decisions to be made at a glance.\"[9]  History  The idea of digital dashboards followed the study of decision support systems in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of Executive Information Systems (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources.[10] Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and online analytical processing (OLAP) allowed dashboards to function adequately.[citation needed] Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of key performance indicators (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's Balanced Scorecard.[11] In the late 1990s, Microsoft promoted a concept known as the Digital Nervous System and \"digital dashboards\" were described as being one leg of that concept.[12] Today, the use of dashboards forms an important part of Business Performance Management (BPM).   See also      Dashboard (disambiguation)  Dashboard software      LogiAnalytics      Graphite (software)     Kibana     QlikView     SAP Design Studio     SAP BusinessObjects Dashboards     SAP Lumira     Target Dashboard      HipDash - Smart Dashboard   Related terms      Business activity monitoring     Complex event processing     Corporate performance management     Dashboard of Sustainability     Data presentation architecture     Enterprise manufacturing intelligence     Event stream processing     Information graphics     Information design     Management cockpit     Scientific visualization  List of reporting software From Wikipedia, the free encyclopedia  The following is a list of notable report generator software. Reporting software is used to generate human-readable reports from various data sources. Free software      BIRT Project     D3.js     GNU Enterprise (reporting sub-package)     JasperReports     jsreport     KNIME     LibreOffice Base     OpenOffice Base     Pentaho     SpagoBI  Commercial software      ActiveReports     Actuate_Corporation     AnyChart     BOARD     Cognos BI     Crystal Reports     CyberQuery     DevExpress Reporting     GoodData     icCube     I-net Crystal-Clear     InetSoft     Information Builders' FOCUS and WebFOCUS     Infragistics NetAdvantage Reporting     InstantAtlas     Jedox     Jinfonet Software     List & Label     Logi Analytics     m-Power     MicroStrategy     Navicat     OBIEE     Oracle Discoverer     Oracle Reports     Hyperion     Oracle XML Publisher     Plotly     Proclarity     Procurify     QlikView     QuickBase     Roambi     RW3 Technologies     SiSense     Splunk     SQL Server Reporting Services     Stimulsoft Reports     Style Report     Tableau     Targit     TIBCO     Text Control     Windward Reports     XLCubed  In data processing operational reporting is reporting about operational details that reflects current activity. Operational reporting is intended to support the day-to-day activities of the organization. \"Examples of operational reporting include bank teller end-of-day window balancing reports, daily account audits and adjustments, daily production records, flight-by-flight traveler logs and transaction logs.\"[1] See also      Business reporting     List of reporting software     Reporting (disambiguation)", "category": "Edison", "id": 130}
{"skillName": "DSRMP04", "skillText": "Convene a planning group made up of individuals from the community affected by the problem or issue and others who are in a position to address it. Identify additional partners and invite them into your strategic planning process.           Related resources:     Involving Key Influentials in the Initiative     Involving People Most Affected by the Problem     Defining and Analyzing the Problem           Describe the vision for the community or initiative (their dreams for how things should be).         In a workshop, retreat, or dialogue about the group's vision, capture:             Dreams for the community or initiative (e.g., safe neighborhoods)             What success would look like (e.g., healthy youth)             How things ought to be (e.g., caring communities)             What people and conditions would look like if things were consistent with that picture (e.g., health for all)         Review the multiple vision statements.                   List all vision statements proposed.                   Choose one or several vision statements with particular power to communicate, and consider whether they are: 1) concise, 2) positive, 3) acceptable, 4) a clear expression of why the group has come together.             Choose visions that are embraced by the group             (As appropriate) Select/edit the several that are particularly effective             Check to see that everyone's voice is heard in the final selections                           Top three statements, and why.                       Select one statement that concisely expresses why the group has come to together and is acceptable to all group members.                   What is your group's vision?                   Related resources:         VMOSA (Vision, Mission, Objectives, Strategies, Action Plan): An Overview         Proclaiming Your Dream: Developing Vision and Mission Statements         Leading a Community Dialogue on Building a Healthy Community         Conducting a Workshop         Organizing a Retreat               State the mission (the what and why).         Develop (or refine) a mission statement that includes what is to be done and why it is to be done (e.g., \"Creating caring communities through education and opportunities to serve.\"). To do so:             Describe the essential \"what\" of the organization or initiative by reviewing its core functions and current programs and activities (e.g., training, advocacy, support, partnerships)             Explain the essential \"why\" of the organization or initiative by reviewing the vision statements (e.g., safe neighborhoods, healthy children)             Frame the mission statement as a single sentence that captures the common purpose (essential what and why) (e.g., \"Promoting health families [the why] through parent training and community support [the what]\")                           What is your drafted mission statement?                       Review the mission statement, making sure it is:             Clear regarding what is to be done and why             Concise (often one sentence)             Outcome oriented             Robust - it leaves open a variety of possible means             Inclusive - reflects the voices of all people who are involved                           What is your final mission statement?                           Related resources:             Proclaiming Your Dream: Developing Vision and Mission Statements                   State the objectives (how much of what the group hopes to accomplish by when).         To develop (or refine) objectives, clearly describe:             Benchmarks that would help us assess where we are now (baseline or pre-intervention) and where we would be if the initiative were successful (objectives).                           What baseline markers could we access and how would we hope they would change if success were attained?                           Behavioral objectives: the changes in behaviors we would see if the group's efforts were successful (What would people be saying and doing differently?) (e.g., For preventing adolescent pregnancy - \"By 2012, to increase by 40% the reported level of sexual abstinence among 12-15 year olds.\")             Population-level objectives: the changes in community-level indicators we would see if the group's objectives were met (How would changes in individual's behaviors add up to outcomes for all those in the community?) (e.g., For adolescent pregnancy - \"By 2015, the estimated pregnancy rate among 12-15 year olds will be reduced by 30%\")         Review the objectives to determine if they are: (SMART+C)             Specific             Measurable (at least potentially)             Achievable             Relevant (to the mission)             Timed (date for attainment)             Challenging (requiring extraordinary effort)         Be flexible with deadlines in creating objectives. Defining objectives is time consuming and may require second and third considerations for completeness.                   Related resources:         VMOSA (Vision, Mission, Objectives, Strategies, Action Plan): An Overview         Creating Objectives               Identify the strategies (how things will be accomplished).         To develop (or refine) strategies, clearly describe how the effort will bring about the mission and objectives. Identify for each:             The levels to be targeted ( i.e., individuals, families and kinship groups, organizations and sectors, and/or broader systems.)                           What different levels of your problem or goal will you target?                           For each strategy, consider if it will be universal (i.e., includes all of those who may be at risk or may benefit; e.g. all children and youth) or targeted (i.e., targets those who may be at greater risk for the problem; e.g., youth with a history of violence)             The personal and environmental factors to be addressed by the initiative                 Personal factors may include: knowledge, beliefs, skills, education and training, experience, cultural norms and practices, social status, cognitive or physical abilities, gender, age, genetic predisposition                 Environmental factors may include: social support, available resources and services, barriers (including financial, physical, and communication), social approval, policies, environmental hazards, living conditions, poverty, and disparity in status                                   What personal factors related to your vision and mission are common among those affected by the problem and those maintaining it?                                   What environmental factors related to your vision and mission are common within your targeted community?                               Those who can most benefit and contribute and how they can be reached or involved in the effort                 Targets of change: those who may at particular risk for the issue or whose actions are critical for success. For you, who would this include?                 Agents of change: those who may be in a position to contribute to the initiative, including targets of change. Who would this include?                 Community sectors: through which targets and agents of change can be reached or involved             Behavioral strategies to be used. Approaches may include:                 Providing information and enhancing skills (e.g., conducting a social marketing campaign to educate people about the problem or goal and how to address it)                 Modifying barriers, access, exposures, and opportunities (e.g., increase availability of affordable childcare for those entering work force)                 Changing the consequences (e.g., encourage housing developers to create green spaces and mixed income development)                 Enhancing services and supports (e.g., increase the number of public health centers that provide dental care)                 Modifying policies and broader systems (e.g., change business policies so that employees can get time off to care for their sick children)             Review the strategies and comment on their appropriateness to the situation and sufficiency in addressing the mission and objectives. Review the strategies for:                 Consistency with the overall vision, mission, and objectives                 Goodness of fit with the resources and opportunities available                 Anticipated resistance and barriers and how they can be minimized                 Whether those who are affected are reached                 Whether those who can contribute are involved                                   Any changes?                                   Related resources:                 VMOSA (Vision, Mission, Objectives, Strategies, Action Plan): An Overview                 Developing Successful Strategies: Planning to Win                 Identifying Opponents                 Identifying Targets and Agents of Change: Who Can Benefit and Who Can Help                 Identifying Risk and Protective Factors: Their Use in Selecting Potential Targets and Promising Strategies for Interventions                 Involving Key Influentials in the Initiative                 Involving People Most Affected by the Problem                 Using Community Sectors to Reach Targets and Agents of Change                 Overview of Tactics for Modifying Access, Barriers, and Opportunities                 Using Tax Incentives to Support Community Health and Development                 Changing Policies: An Overview                       Develop (or refine) the action plan by stating the specific community/system changes to be sought that will result in the accomplishment of your goals and objectives         For each strategy, identify specific community and system changes (i.e., new or modified programs, policies, and practices) to be sought.         After compiling a list of potential changes, review each candidate community or system change and rate it on two dimensions:             Importance to the mission (1=not at all, 5=very); and             Feasibility (1=not at all, 5=very)         Secure a formal decision from the group on what community or system changes (intervention components and elements) will be sought (or implemented), with priority given to those changes with high importance and high feasibility                   Related resources:         Our Model of Practice: Building Capacity for Community and System Change         Obtaining Feedback from Constituents: What Changes are Important and Feasible         Providing Information and Enhancing Skills         Overview of Tactics for Modifying Access, Barriers, and Opportunities               Identify action steps for one key community/system change in the action plan (who is going to do what by when). Describe:         What specific change (e.g., in program or policy) or aspect of the intervention that will occur?         Who will carry it out?         When the intervention will be completed or for how long it will be maintained?         Resources (money and staff) needed/available?         Communication - who should know what about this?                   Related resources:         Developing an Action Plan         Identifying Action Steps in Bringing About Community and System Change         Designing Community Interventions               Evaluate critically the appropriateness of the action plan (i.e., the activities or community/system changes to be implemented). Use the criteria that follow:         Completeness - Are all the intended activities or community/system changes included in your plan? Are a wide variety of strategies and sectors utilized?         Clarity - Is it apparent what will be done and who will do what by when, to bring about change?         Sufficiency - If all that is proposed were accomplished, would it meet the group's mission and objectives? If not, what additional changes need to be planned and implemented?         Resources (money/staff) needed/available?         Currency - Does the action plan reflect the current work and situation?         Flexibility - As the plan unfolds, is it flexible enough to respond to new opportunities, barriers, and changes in the community? Can it be modified as objectives are accomplished or goals adjusted?                    Modify your proposed community and system changes and action plans based on your answers (if necessary).                   Related resources:         Developing an Action Plan               Indicate how you will use the strategic and action plans. Consider the following potential uses:         Communicate the initiative's purpose to others.             Indicate who should know about the group's vision, mission, objectives, strategic and action plans.             Describe how the initiative will communicate this new framing of what it does and why.         Check the organization's core functions.             Indicate who should know about the group's vision, mission, objectives, and the core functions of the organization represented (e.g., advocacy, training).             What adjustments might be appropriate to the vision, mission, and action plan?         Find common ground and anticipate potential conflict.             Identify potential disagreements about ends and means that the group is facing.             Indicate how you might use this new framing of the problem or goal to build consensus.         Plan how to detect or discern opportunity             Identify the criteria that will be used to judge an \"opportunity\". These might include qualities such as:             Consistency with the vision             Consistency with the mission             Contributes to the action plan             In light of the vision, mission, and action plan, pinpoint new or emerging opportunities for the community initiative or organization.                           What steps will you take to better detect or discern new or emerging opportunities?                       Identify potential partners             Indicate who is out there who can help the group achieve its vision and mission. List organizations that share this common work.             In light of the vision, mission, and action plan, identify some potential partners that the community initiative or organization should collaborate with.                            Who or what organizations in your community would be good partners, based on your vision and mission?                           Related resources:             Developing an Action Plan             Communicating Information About Community Health and Development Issues             Developing a Plan for Communication             Discovering and Creating Possibilities             Coalition Building I: Starting a Coalition                   Begin implementing action planning steps. Choose the order by considering:         Which changes need to be completed before others can? Some changes may require other changes and relationships to be established.         Which changes are easier or quicker to bring about? Could completing them give the organization's membership a sense of success and provide the organization with much needed media exposure?         Which changes are the most important or key to the initiative's objectives?         Which changes would inspire and encourage participants and build credibility within the community?                   Using the list from Step 6, prioritize implementation considering the previous considerations (e.g., ease or quickness of accomplishing, importance to meeting objectives, etc.)               Review the action plan at regular intervals. As your coalition grows and the objectives are accomplished or conditions change, members may revise the plan.           Related resources:     Developing an Action Plan        Examples Example 1: Health for All Coalition: Increasing Access and Decreasing Disparities in Health Example 2: Roadmap to a Healthier Douglas County Example 3: Safe Streets Coalition: Preventing Neighborhood Violence and Increasing Safety Example 4: Access for All: Best Practices for Responding to Women with Disabilities Who are Abuse Victims Example 5: Global Immunization Vision and Strategy Example 6: Clean Drinking Water in Kakamega County, Kenya Example 7: Embrace Mental Health Organization in Beirut, Lebanon Image of a stack of books with an apple on top of them.  Sixteen modules for teaching this and other core skills. Learn more. Image of hands with a seedling in them and the words Donate Now below them.  Will you help us make sure the Tool Box remains available? Donate now.  The definition of a strategy, reflecting the vision prevailing among the stakeholders, is not sufficient to implement the strategy. The implementation of a strategy starts with its objectives. Each objective aims at an impact which translates into an outcome. The outcome, in turn, requires the production of outputs. Action plans show the activities needed to deliver the desired outputs. As mentioned in the previous chapter: “successful achievement of the strategic goals will depend on well-thought-out mid-term and long-term strategies, broken down into activities/action plans\".  Action plans should therefore be closely linked to the realisation of the outputs requested by the strategic goals defined in the NSDS. If the strategic goals are clearly defined, according to the SMART approach, i.e., Specific, Measurable, Achievable, Relevant and Time-bounded, it shouldn't be difficult to identify the actions related to the objectives.  Based on the overall strategy of the NSDS, operational objectives are linked to the main areas of intervention (See IDENTIFYING STRATEGIC GOALS). An action plan should be created for each area of intervention, with a clear identification of the tasks that have to be undertaken to deliver the desired outputs and to achieve the strategy.  Action Plans will refer to capacity areas of the NSS to which the operational objectives are related: political and technical governance (management), human resources, physical and statistical infrastructure, funding, statistical policies, processes and partnerships. All these aspects should be taken into consideration and reflect the national context and administrative set up of the country.  HOW TO DESIGN  AN ACTION PLAN  ? When you have clearly in mind what you want to achieve, you can define the outputs that are needed to reach the operational objectives. The purpose (outcome) for each objective has to be identified as well as the outputs. If it is the case, it will facilitate the identification of tasks and activities that need to be conducted.  The identification of activities by itself doesn't make the action plan, which needs to be more than the enumeration of activities that we need to carry out. Besides the enumeration, an action plan or work programme should include: a time frame (When?); an evaluation of the existing capacities in order to identify missing capacities (How?); a cost evaluation (How much ?); the identification of the actors (Who?); appropriate mechanisms for monitoring and assessing progress (What for ?).  An action plan includes:      Who is going to do what – assigning the responsibilities and setting targets;     When – estimating the schedule and duration of activity;      In what order – determining the sequence and dependence of activities;     How – defining human, technical and financial resources needed;     What for – identifying and selecting indicators that can be used to track progress and monitor the performance of the action.  One activity decomposed in tasks will facilitate the development of the timeframe. A precise action plan in the NSDS will be an added value not only for the NSI and NSS but also for donors and international organisations because it can give them a clear idea about the activities by sector that need to be implemented and the costs of each activity.  The Action Plan should be inserted in a logical framework reflecting the intervention logic of the strategy for the development of statistics in the country. The table below shows the logical framework linking a strategy and the related action plan.  Contents of the Logical Framework to present the strategy and action plan Overall objective \t The broad development impact to which the project/NSDS contributes Outcome (Purpose) \tThe development outcome at the end of the NSDS implementation, more specifically the expected benefits to the target groups Outputs \tThe direct and tangible results (goods and services) that the NSDS will deliver and which are largely under the project management control Activities \tThe tasks (work programme) that need to be carried out to deliver the planned results Indicators \tIndicators are linked to the objective-based planning and measure how the objectives, purpose, results and activities of the NSDS will be achieved      MAIN CONTENTS OF THE ACTION PLAN An action plan should be detailed and used like a daily instrument for the individuals, in charge of its development, to control the actions, costs and timeline, to monitor and evaluate the implementation, to make necessary adjustments and to assess the results.  In order to prepare a detailed activity plan, the following should be considered:      List the main activities     Break the activities into manageable tasks     Clarify the sequence and dependencies between the tasks     Estimate the start-up, duration and completion of the activities     Summarise scheduling of the main activities     Define the milestones     Define the existing capacities and the inputs (equipment, expertise...) that are missing     Allocate tasks among the team  A project management software can be used to prepare the action plan, but an excel file is enough and easier to prepare and disseminate.     BUDGETING FOR AN ACTION PLAN Once the activities for the period of the NSDS are defined, you need to translate them into annual action plans, with a detailed work programme and the respective budget. The work programme should be underpinned by a budget, to control operations and results.  The budget is in fact crucial for the implementation of the action plans. All the actions need to be carefully budgeted to have an overview of the total cost of the action plan and to identify the ways of financing it.  The budget will:      Show the total current and investment costs for the implementation of the actions;     Specify the expected burden on the national budget or external financing requirements;     Describe in some detail how resources will be used, by main expenditure items, current costs, incremental costs and capital expenditure.  The costing of a NSDS implementation in developing countries might be difficult to define due to the uncertainties of the countries, but it is crucial to have a clear cost estimation of the statistical operations and activities to be carried out to help the definition of the funding strategy (SEE BUDGETING AND FINANCE).     In practice  Who and When  The actors to be mobilised should be clearly identified in the action plan, with reference to the institutions concerned, inside and outside the NSS.  How  - Action plans are needed because their preparation processes contribute to restrict the gap between Objectives of the NSDS and its degree of implementation, one of the main drawbacks identified in the evaluation of the NSDS. Well prepared action plans will allow you to be realistic.  - Complete and extremely detailed action plans are a heavy burden in the preparation process. In order not to discourage the preparation team, a precise action plan should be available for the first implementation year of the NSDS. For the remaining years, estimations of the time framework and of the costs of the outputs can be rougher.  - Action plans should take into account the level of priorities in the strategic objectives of the NSDS. Action plans are needed for all the priorities defined in the strategy.  - An assessment of the capacities available in the NSS for each activity is needed in order to identify the missing capacities, the strategy to mobilise them and the costs to be faced. Action plans have no chance to be implemented if the corresponding resources have not been sufficiently secured in the preceding phase and rationalized in a funding strategy.  - When an activity is part of a sequence, the action plan should mention that it is a prerequisite for another activity or that another activity is a prerequisite and take into account the constraints implied.  - It bears repeating that countries have an on-going statistical programme including already planned censuses or surveys operations that both have to be maintained for the most part, revised for some and expanded as a result of agreed strategic goals for the period.  - The implementation of the first year of the plan has to include the ongoing programme for that same year.  One of Biggest Problems in Strategic Planning: Plan Isn't Implemented  At this point in planning, planners are sometimes fatigued from completing the earlier phases of planning. Action planning may seem detailed and tedious compared to earlier phases of strategic planning which often seem creative in nature. Therefore, action planning is too often ignored, leaving the results of earlier stages of planning much as “castles in the air” -- useless philosophical statements with no grounding in the day-to-day realities of the organization. Meaningful stages of earlier planning become utterly useless.  The organization's commitment to strategic planning is commensurate to the extent that a) the organization completes action plans to reach each strategic goal and b) includes numerous methods for verifying and evaluating the actual extent of implementation of the action plan. Developing Action Plans (or Work Plans)  1. Actions plans specify the actions needed to address each of the top organizational issues and to reach each of the associated goals, who will complete each action and according to what timeline.  2. Develop an overall, top-level action plan that depicts how each strategic goal will be reached.  3. Develop an action plan for each major function in the organization, e.g., marketing, development, finance, personnel, and for each program/service, etc. These plans, in total, should depict how the overall action plan will be implemented. In each action plan, specify the relationship of the action plan to the organization's overall, top-level action plan.  4. Ensure each manager (and, ideally each employee) has an action plan that contributes to the overall. These plans, in total, should depict how the action plans of the major functions will be implemented. Again, specify the relationship of these action plan to the organization's overall, top-level action plan.  5. The format of the action plan depends on the nature and needs of the organization. The plan for the organization, each major function, each manager and each employee, might specify: a) The goal(s) that are to be accomplished b) How each goal contributes to the organization's overall strategic goals c) What specific results (or objectives) much be accomplished that, in total, reach the goal of the organization d) How those results will be achieved e) When the results will be achieved (or timelines for each objective) Developing Objectives and Timelines  1. Objectives are specific, measurable results produced while implementing strategies.  2. While identifying objectives, keep asking “Are you sure you can do this?”  3. Integrate the current year’s objectives as performance criteria in each “implementer’s” job description and performance review.  4. Remember that objectives and their timelines are only guidelines, not rules set in stone. They can be deviated from, but deviations should be understood and explained.  5. Consider the following example format for action your plan.  Strategic Goal \t  Strategy \t  Objective \t  Responsibility \t  Timeline 1. (Goal #1) \t1.1 (first strategy to reach Goal #1) \t1.1.1 (first objective to reach while implementing Strategy #1.1) \t(who’s going to accomplish that objective) \t(when the implementer is going to be accomplish that objective) Step 1: Define your strategic focus areas.  Strategic focus areas represent big buckets of activity. Take your strategic plan and define these buckets. There should be at least three, but no more than 7.  Step 2: Create at least one measurable outcome for each focus area.  Within each focus area, what does success look like? Define the characteristics of success -- what people will see, feel, think, do, if you are successful -- and then figure out how you will measure each characteristic. If you can, outline a simple, aggressive target for each characteristic. Success comes with multiple characteristics for each outcome, so don't just focus on one number.  Step 3: Design Pathfinder Projects with milestones.  To reach your outcome you will need at least one project. We call these \"Pathfinder Projects\" because, in truth, you don't know what will work. Just like outcomes, though, you need to be crystal clear in defining a Pathfinder Project.  The best way to get clarity is by setting commonsense milestones. That way, you can tell if your project is getting off course, or worse yet, not working at all. Your milestones alert you to failure. When you fail early, you fail cheaply...and you learn.  Step 4: Draft a short Strategic Action Plan.  Outcomes and projects provide two key components to your strategy, but you do not move a strategy into action without a Strategic Action Plan, continuously revised. If you do not know what you are going to do next week to advance your strategy, chances are your strategic plan is going nowhere. Your Strategic Action Plan outlines next steps and who is responsible for taking them.  Step 5: Commit to a 30 day review process.  We learn what works by doing. Making adjustments along the way is critical to a successful strategy. A 30 day review process keeps you focused and enables you to make regular adjustments to your strategy. In today's world, strategy is like software development. You need to commit to continuous revision", "category": "Edison", "id": 131}
{"skillName": "DSENG02", "skillText": "Computational Science (also scientific computing or scientific computation (SC)) is a rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems. Computational science fuses three distinct elements:[1]      Algorithms (numerical and non-numerical) and modeling and simulation software developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems     Computer and information science that develops and optimizes the advanced system hardware, software, networking, and data management components needed to solve computationally demanding problems     The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information science  In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines.  The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers.  Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.  Numerical analysis is an important underpinning for techniques used in computational science.  Contents      1 Applications of computational science         1.1 Numerical simulations         1.2 Model fitting and data analysis         1.3 Computational optimization     2 Methods and algorithms     3 Reproducibility and open research computing     4 Journals     5 Education     6 Related fields     7 See also     8 References     9 Additional sources     10 External links  Applications of computational science  Problem domains for computational science/scientific computing include: Numerical simulations  Numerical simulations have different objectives depending on the nature of the task being simulated:      Reconstruct and understand known events (e.g., earthquake, tsunamis and other natural disasters).     Predict future or unobserved situations (e.g., weather, sub-atomic particle behaviour, and primordial explosions).  Model fitting and data analysis      Appropriately tune models or solve equations to reflect observations, subject to model constraints (e.g. oil exploration geophysics, computational linguistics).     Use graph theory to model networks, such as those connecting individuals, organizations, websites, and biological systems.  Computational optimization Main article: Mathematical optimization      Optimize known scenarios (e.g., technical and manufacturing processes, front-end engineering).     Machine learning  Methods and algorithms  Algorithms and mathematical methods used in computational science are varied. Commonly applied methods include:      Numerical analysis     Application of Taylor series as convergent and asymptotic series     Computing derivatives by Automatic differentiation (AD)     Computing derivatives by finite differences     Graph theoretic suites     High order difference approximations via Taylor series and Richardson extrapolation     Methods of integration on a uniform mesh: rectangle rule (also called midpoint rule), trapezoid rule, Simpson's rule     Runge Kutta method for solving ordinary differential equations     Monte Carlo methods     Molecular dynamics     Linear programming     Branch and cut     Branch and Bound     Numerical linear algebra     Computing the LU factors by Gaussian elimination     Cholesky factorizations     Discrete Fourier transform and applications.     Newton's method     Space mapping     Time stepping methods for dynamical systems  Both historically and today, Fortran remains popular for most applications of scientific computing.[2][3] Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell,[2] Julia,[2] Maple,[3] Mathematica,[4] MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library),[citation needed] R, SciLab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK.  Computational science application programs often model real-world changing conditions, such as weather, air flow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving equations that describe how the system operates; and then repeat the process to calculate the next state.  The term computational scientist is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computing in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering. Scientific computing has increasingly also impacted on other areas including economics, biology and medicine.  Computational science is now commonly considered a third mode of science, complementing and adding to experimentation/observation and theory.[5] The essence of computational science is numerical algorithm[6] and/or computational mathematics. In fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop, 2004.[7] Reproducibility and open research computing  The complexity of computational methods is a threat to the reproducibility of research.[8] Jon Claerbout has become prominent for pointing out that reproducible research requires archiving and documenting all raw data and all code used to obtain a result.[9][10][11] Nick Barnes, in the Science Code Manifesto, proposed five principles that should be followed when software is used in open science publication.[12] Tomi Kauppinen et al. established and defined Linked Open Science, an approach to interconnect scientific assets to enable transparent, reproducible and transdisciplinary research.[13] Journals  Most scientific journals do not accept software papers because a description of a reasonably mature software usually does not meet the criterion of novelty.[citation needed] Outside computer science itself, there are only few journals dedicated to scientific software. Established journals like Elsevier's Computer Physics Communications publish papers that are not open-access (though the described software usually is). To fill this gap, a new journal entitled Open research computation was announced in 2010;[14] it closed in 2012 without having published a single paper, for a lack of submissions probably due to excessive quality requirements.[15] A new initiative was launched in 2012, the Journal of Open Research Software.[16] In 2015, a new journal [17] dedicated to the replication of computational results has been started on GitHub. Education  Scientific computation is most often studied through an applied mathematics or computer science program, or within a standard mathematics, sciences, or engineering program. At some institutions a specialization in scientific computation can be earned as a \"minor\" within another program (which may be at varying levels). However, there are increasingly many bachelor's and master's programs in computational science. Some schools also offer the Ph.D. in computational science, computational engineering, computational science and engineering, or scientific computation.  There are also programs in areas such as computational physics, computational chemistry, etc. Related fields      Bioinformatics     Cheminformatics     Chemometrics     Computational archaeology     Computational biology     Computational chemistry     Computational economics     Computational electromagnetics     Computational engineering     Computational finance     Computational fluid dynamics     Computational forensics     Computational geophysics     Computational informatics     Computational intelligence     Computational law     Computational linguistics     Computational mathematics     Computational mechanics     Computational neuroscience     Computational particle physics     Computational physics     Computational sociology     Computational statistics     Computer algebra     Environmental simulation     Financial modeling     Geographic information system (GIS)     High performance computing     Machine learning     Network analysis     Neuroinformatics     Numerical linear algebra     Numerical weather prediction     Pattern recognition     Scientific visualization  See also      iconScience portal Computing portal iconMathematics portal       Computational science and engineering     Comparison of computer algebra systems     List of molecular modeling software     List of numerical analysis software     List of statistical packages     Timeline of scientific computing     Simulated reality     Extensions for Scientific Computation (XSC)    Computational science refers to the use of computers, networks, storage devices, software, and algorithms to solve problems, do simulations, build things, or create new knowledge. The figure below shows how Computational Science can be viewed as the intersection of:      Computing and networking hardware     Algorithms, Numerical Analysis, and Mathematics     Software, Programming, and Databases     Discipline specific knowledge   It is an incredibly broad discipline. The Institute of Electrical and Electronics Engineers (IEEE) states (paraphrased here):  The term Computational Science presents a broader view, implying science (and engineering) that is \"computational\" as opposed to \"experimental\" or \"theoretical\" in nature. Computer Science, of course, deals with the science and engineering of computers. Some areas of computational science require large computers to perform trillions of floating point operations (computational fluid dynamics, computational chemistry, computational meteorology, computational physics, etc.). Other areas of computational science build and utilize large databases and require terabytes of storage (bioinformatics, business, knowledge management, geographical information systems, etc.). And some areas will require networks of computers to accomplish their goals (web technologies, grid computing, collaborative software, systems of systems, online communities, etc.). Graphics and visualization are also important areas. The most exciting computational science problems might involve all of these: computing, data storage, and networking (e.g. artificial intelligence, computational steering, mobile robots, virtual reality, etc.). Software development and programming are also crucial parts of computational science (e.g. Java, C++, MPI, CORBA, OpenGL, mySQL, PHP, Perl, Linux, etc.).  logo    The Krell Institite states (in http://www.krellinst.org/AiS/textbook/unit1/compsci_n1.html):  \"Computational science is a relatively new discipline, and there is currently no consensus on a precise definition of what computational science actually is. In broad terms, computational science involves using computers to study scientific problems and complements the areas of theory and experimentation in traditional scientific investigation. Computational science seeks to gain understanding of science principally through the use and analysis of mathematical models on high performance computers.\"  \"Computational science has emerged as a powerful and indispensable method of analyzing a variety of problems in research, product and process development, and manufacturing. Computational simulation is being accepted as a third methodology in scientific research, complementing the traditional approaches of theory and experiment. Computer simulations provide both qualitative and quantitative insights into many phenomena that are too complex to be dealt with by analytical methods or too expensive or dangerous to study by experiments. Many experiments and investigations that have traditionally been performed in a laboratory, a wind tunnel, or the field are being augmented or replaced by computer simulations. Some studies, such as nuclear repository integrity and global climate change, involve time scales that preclude the use of realistic physical experiments. The availability of high performance computers, graphic workstations, and high speed networks, coupled with major advances in algorithms and software, has brought about a revolution in the way scientific and engineering investigations are carried out.\"  \"Computational science should not be confused with computer science. Computational science focuses on a scientific or engineering problem and draws from computer science and mathematics to gain an improved understanding of the problem. Computer science focuses on the computer itself. Even though the areas are quite distinct, many of the topics typically considered to be in the domain of computer science are of much value in computational science. \"  \"Traditional or established areas of computational science include: Computational fluid dynamics  Atmospheric science Seismology Structural analysis Chemistry Magnetohydrodynamics Reservoir modeling Global ocean/climate modeling Environmental studies; and, Nuclear engineering   Some nontraditional and emerging areas of computational science include: Biology Economics Materials research Medical imaging; and, Animal science.   The list continues to grow. More recently, computational science has begun to make inroads into other areas, such as music and the visual arts.\" What is a Computational Scientist?  The term computational scientist is commonly used to describe scientists, engineers, and mathematicians who apply high performance computer technologies in creative and essential ways to advance the state of knowledge in their respective fields. A computational scientist must have expertise in an applied discipline and must also be familiar with leading-edge computer architectures and the data structures issues associated with those architectures. A computational scientist must also have a good understanding of both the analysis and implementation of numerical algorithms and the ways that algorithms map to data structures and computer architectures, and additionally, must be comfortable with networking technologies that permit access to remote computers, massive databases, and visualization facilities. Recently, scientific visualization has become an essential tool of the computational scientist for the preprocessing of data sets and the interrogation of massive amounts of computational results. In summary, a computational scientist, using networking and visualization tools, works in the intersection of 1) an applied science or engineering discipline; 2) computer science; and 3) mathematics. This multi-disciplinary activity has given rise to a new way of conducting research.  The Society of Industrial and Applied Mathematics ( SIAM ) states:  \"Computation is now regarded as an equal and indispensable partner, along with theory and experiment, in the advance of scientific knowledge and engineering practice. Numerical simulation enables the study of complex systems and natural phenomena that would be too expensive or dangerous, or even impossible, to study by direct experimentation. The quest for ever higher levels of detail and realism in such simulations requires enormous computational capacity, and has provided the impetus for dramatic breakthroughs in computer algorithms and architectures. Due to these advances, computational scientists and engineers can now solve large-scale problems that were once thought intractable.\"  \"Computational science and engineering (CSE) is a rapidly growing multidisciplinary area with connections to the sciences, engineering, mathematics and computer science. CSE focuses on the development of problem-solving methodologies and robust tools for the solution of scientific and engineering problems. We believe that CSE will play an important if not dominating role for the future of the scientific discovery process and engineering design.\"  The Cornell theory Center defines Computational Science to be:  \"A field that concentrates on the effective use of computer software, hardware and mathematics to solve real problems. It is a term used when it is desirable to distinguish the more pragmatic aspects of computing from (1) computer science, which often deals with the more theoretical aspects of computing; and from (2) computing engineering, which deals primarily with the design and construction of computers themselves. Computational science is often thought of as the third leg of science along with experimental and theoretical science.\"   Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the \"third mode of discovery\" (next to theory and experimentation).[1] In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).  It is typically offered as a masters or doctorate program at several institutions.  Contents      1 Methods     2 Applications     3 See also     4 References     5 External links  Methods  Computational Science and Engineering methods and frameworks include:      High performance computing and techniques to gain efficiency (through change in computer architecture, parallel algorithms etc.)     Modeling and simulation     Algorithms for solving discrete and continuous problems     Analysis and visualization of data     Mathematical foundations: Numerical and applied linear algebra, initial & boundary value problems, Fourier analysis, optimization     Data Science for developing methods and algorithms to handle and extract knowledge from large scientific data  With regard to computing, computer programming, algorithms, and parallel computing play a major role in CSE. The most widely used programming language in the scientific community is FORTRAN. Recently, C++ and C have increased in popularity over FORTRAN. Due to the wealth of legacy code in FORTRAN and its simpler syntax, the scientific computing community has been slow in completely adopting C++ as the lingua franca. Because of its very natural way of expressing mathematical computations, and its built-in visualization capacities, the proprietary language/environment MATLAB is also widely used, especially for rapid application development and model verification. Python along with external libraries (such as NumPy, SciPy, Matplotlib) has gain some popularity as a free and Copycenter alternative to MATLAB. Applications A numerical solution to the heat equation on a pump casing model using the finite element method.  Computational Science and Engineering finds diverse applications, including in:      Aerospace Engineering and Mechanical Engineering: combustion simulations, structural dynamics, computational fluid dynamics, computational thermodynamics, computational solid mechanics, vehicle crash simulation, biomechanics, trajectory calculation of satellites     Astrophysical systems     Battlefield simulations and military gaming, homeland security, emergency response     Biology and Medicine: protein folding simulations (and other macromolecules), bioinformatics, genomics, computational neurological modeling, modeling of biological systems (e.g., ecological systems), 3D CT ultrasound, MRI imaging, molecular bionetworks, cancer and seizure control     Chemistry: calculating the structures and properties of chemical compounds/molecules and solids, computational chemistry/cheminformatics, molecular mechanics simulations, computational chemical methods in solid state physics, chemical pollution transport     Civil Engineering: finite element analysis, structures with random loads, construction engineering, water supply systems, transportation/vehicle modeling     Computer Engineering, Electrical Engineering, and Telecommunications: VLSI, computational electromagnetics, semiconductor modeling, simulation of microelectronics, energy infrastructure, RF simulation, networks     Epidemiology: influenza spread     Environmental Engineering and Numerical weather prediction: climate research, Computational geophysics (seismic processing), modeling of natural disasters     Finance: derivative pricing, risk management     Industrial Engineering: discrete event and Monte-Carlo simulations (for logistics and manufacturing systems for example), queueing networks, mathematical optimization     Material Science: glass manufacturing, polymers, and crystals     Nuclear Engineering: nuclear blast modeling, fusion simulations     Petroleum engineering: petroleum reservoir modeling, oil and gas exploration     Physics: Computational particle physics, automatic calculation of particle interaction or decay, plasma modeling, cosmological simulations     Transportation  See also      Computational science     Applied mathematics     Computational fluid dynamics     Numerical analysis     Multiphysics     Grand Challenges    This new discipline in science combines computational thinking, modern computational methods, devices and collateral technologies to address problems far beyond the scope of traditional numerical methods.  Computational science typically unifies three distinct elements:  • Modeling, Algorithms and Simulations (e.g. numerical and non-numerical, discrete and continuous); • Software developed to solve science (e.g., biological, physical, and social), engineering, medicine, and humanities problems; • Computer and information science that develops and optimizes the advanced system hardware, software, networking, and data management components (e.g. problem solving environments).SE is a broad multidisciplinary area that encompasses applications in science/engineering, applied mathematics, numerical analysis, and computer science. Computer models and computer simulations have become an important part of the research repertoire, supplementing (and in some cases replacing) experimentation. Going from application area to computational results requires domain expertise, mathematical modeling, numerical analysis, algorithm development, software implementation, program execution, analysis, validation and visualization of results. CSE involves all of this.  One point we would like to emphasize in this document is that CSE is a legitimate and important academic enterprise, even if it has yet to be formally recognized as such at some institutions. Although it includes elements from computer science, applied mathematics, engineering and science, CSE focuses on the integration of knowledge and methodologies from all of these disciplines, and as such is a subject which is distinct from any of them.", "category": "Edison", "id": 132}
{"skillName": "DSDK02", "skillText": "How do firms develop ideas, turn them into products, and decide which ones to bring to market? Most firms do so through a series of steps known as the New Product Development (NPD) or the Stage Gate process: idea generation; idea screening; idea development and testing; business analysis; beta testing and market testing; technical implementation; commercialization; and new product pricing. These steps, or stages (formalized by Dr. Robert G. Cooper in his book Robert’s Rules of Innovation based on industry research), are loose, with steps performed concurrently, and/or eliminated if unnecessary, and flexible enough to provide for firm or industry variation. This framework is also ongoing, with firms, ideally in a state of continuous development. Big Data and New Product Development  © Shutterstock.com | tandaV  The era of Big Data has created substantial opportunities for developing products aligned with consumer demands, forecasting their profitability, and production. Using the NPD framework in this article, we will discuss 1) the benefits of using big data in new product design, 2) transforming Big Data into actionable consumer insights, 3) developing new products using Big Data, 4) improving existing products using Big Data, 4) a case study of how Big Data informs and enhances Procter and Gamble’s new product development.  THE BENEFITS OF USING BIG DATA IN NEW PRODUCT DEVELOPMENT  Using Big Data to inform new product development has many benefits. Firms can create products that connect with the consumer, provide increased consumer value, minimize the risks associated with a new product’s launch, and both allocate, and coordinate the use of, internal R&D resources efficiently. Through data mining, firms can also identify consumer needs it might not otherwise have captured. By continuously developing products that fulfill consumer needs, firms can deepen customer brand engagement and increase customer lifetime value. Through modeling and predictive analytics, the firm can forecast the performance of the product(s) in the market both pre- and post-launch in near-real-time, determine the optimal distribution chains, and optimize marketing strategies to acquire the greatest number of customers at the lowest cost.  In sum, Big Data can help transforming big data into actionable consumer insights; develop new products; and improve existing ones. TRANSFORMING BIG DATA INTO ACTIONABLE CONSUMER INSIGHTS  When firms pare business intelligence tools, data mining, predictive analytics, and other Big Data tools with traditional market research techniques in order to collect actionable insights about their consumers’ needs, and/or similar or related brands/products, firms are able to develop a proactive approach to new product development. They are able to innovate by developing entirely new products, as well as identify opportunities to introduce new product features, new product extensions and/or improve existing product lines. By developing a proactive, rather than reactive approach in which they are responding to the actions of competitors, they are able to ensure product quality, brand consistency, and marketing effectiveness, and exert more influence in their market. Further, they are able to minimize the uncertainty that comes with new product launches, as failures can be quite expensive. This can be a springboard for stage one, idea generation, and aid greatly in stage two, the idea screening process.  NPD teams – usually cross-functional groups consisting of marketers, engineers and data scientists, working in firms that implement Big Data architecture can mine their internal databases from across the firm, as well as firm data and industry data from external sources. Firms can filter and analyze this data to determine existing, latent, and untapped consumer needs; these needs may inspire product ideas and concepts. This analysis may also be used in the second stage, idea screening.  Idea screening involves filtering out ideas that do not provide sufficient customer value, satisfy a profitable target market, face too much competition, and/or are difficult to produce. Internal data can answer these questions and more, allowing product developers to sharply refine the ideas they will pursue long before they conduct a single focus group.  Stage three, which involves consumer outreach, involves a further refinement of ideas for profitability, supply chain logistics, originality, and consumer acceptance. Once again, Big Data can greatly aid in this endeavor. Firms may be able to pull detailed manufacturing data or supply chain data to determine the feasibility of production or distribution respectively. Data scientists can build mathematical models of the product’s hypothetical production and distribution, costs, and use predictive analytics to develop revenue and profitability projections. Beyond determining overall feasibility, these models can help determine optimal conditions for product launch; enhance focus group discussions and surveys, allowing the firm’s market researchers to drill down on specific aspects of the hypothetical product; and further help discard unprofitable ideas.  Stage four – business analysis, involves projections – demand, performance, and profitability. Predictive analytics play a huge role here, though in the absence of historical data on which to draw, data scientists must forecast using different mathematical models. There are three primary methods for predicting new product success: the Bass model; the Fourt-Woodlock model; and the Assessor model.      Bass model: Data scientists using this model try to estimate the shape of the demand curve for existing products and apply it to the new products.     Fourt-Woodlock model: This model can be used to estimate product sales; it is based on the number of consumers who make trial purchases and those who repeat those purchases within the first year of the product being on the market.     Assessor model: This model relies on assessments of the strength of the firm’s brand, and is used to project both brand preference and brand awareness over time, the latter by analysis of the firm’s planned marketing mix.  Firms may use other measures to project product performance in the absence of historical sales, including internal capacity; online and offline conversion rates for similar products in the firm’s portfolio; sales performance forecasting (especially for firms using direct sales methods); analysis of the firm’s other new product launches, among others. Big Data can provide a multiplicity of variables with which to refine a firm’s forecasts. However, it is worth noting that all of the aforementioned measures typically entail a much higher degree of uncertainty than regression-based forecasts using historical data.  Stages two through four allow firms to broaden their criteria for what constitutes a profitable product or target market. Optimizing marketing, production, distribution and pricing, as well as employing market customization strategies, can allow firms to match the products to the consumer at the highest margin. Take a well-known manufacturer of thermal products that is considering introducing a new branded thermal curtain that is 30% more efficient than its closest competitor, but is expensive to produce. The producer might develop models of just those target consumers willing to buy the curtains at the highest price point; develop, test and refine marketing messages; forecast the demand for the product, accounting for seasonality; and enter into an agreement with a low cost third-party manufacturer to produce the branded curtains on demand and directly ship them to the customer. A product that might otherwise have been scuttled due to high costs, might become a cash cow for the manufacturer.  The product concept that survives elimination at this point, is now ready for prototyping and beta testing begins in the following stage. Using mathematical models of target consumers before beta tests can yield insights on potential adoption rates, necessary marketing and sales strategies, optimal distribution channels, and desired product features and functions. Here, the theoretical meets the practical, as consumers provide their feedback. The firms that maximize Big Data will scour this feedback not just for insights about the product being beta tested, but also its overall brand and other exiting products. DEVELOPING NEW PRODUCTS USING BIG DATA  Once beta testing is complete and successful, firms then begin to determine how to scale the product’s manufacturing and integrate it into existing operations. This includes everything from determining optimal suppliers to contingency planning. Firms can use optimization models to predict quality and yield; account for variation in production processes down to the machine or individual level, as well as outputs; forecast demand in order to set target yields; employ mass customization strategies; and determine return on investment for every component in the production process. This data can strengthen decision-making, and yield both higher ROI and greater performance.  Stage seven, or commercialization, involves the actual product launch. Optimization models can predict the national, regional, or local distribution targets most likely to yield the greatest levels of consumer adoption with the lowest customer acquisition cost. This can help inform the ideal launch location(s), which will in turn inform the distribution strategy. Further, Big Data management tools can be used to optimize the operational aspects of the distribution chain, from packaging to delivery scheduling.  Modeling tools can also help optimize media planning the process of finding the media (advertising, public relations, digital) channels that will help a firm achieve its marketing goals. Digital advertising, in particular, provides a wealth of performance data that, when analyzed, can yield terabytes of insights about consumer behavior and consumer purchase behavior in real-time. Marketing analytics firms and in-house quantitative marketing teams can analyze the impact of marketing across channels and across media, allowing firms to evaluate their marketing performance and adjust their marketing strategies in real-time to meet and exceed marketing objectives.  The last stage, which begins earlier in the framework, involves adjusting pricing to reflect actual (rather than projected) supply, production, and distribution costs, as well as market demand, sales, and responses from competitors . This also involves assessing the new product’s actual performance in context to the firm’s overall product portfolio. IMPROVING PRODUCTS USING BIG DATA  Sometimes, the consumer insights captured through market research about a new product involve the firm’s existing products. For example, customer service feedback is often ripe with constructive criticism about a firm’s existing product line – insights that can be used when launching a brand extension of a product. Aggregating that data and feeding it to product marketers at the idea generation stage can be a great source of new product ideas and concepts. Further, firms can mine social networks, industry websites, and other online sources for relevant data about their brand and how their products meet (or fail to meet) consumer needs. Firms can use this information to develop solutions to products currently on the market, or build solutions into planned product extensions/next generation products.  The Internet of Things – a lasting business trend in which firms connect products (all products, but especially those that have been historically unconnected, such as household appliances) through wireless technologies, has tremendous applications for proactive product development. By providing firms with real-time data about consumer usage, firms can identify and exploit opportunities to maximize customer revenue and increase product value to the customer. For example, a smart refrigerator, one programmed to keep stock of the items inside it, also may be programmed to retain diagnostic information to aid the firm in preventative and/or emergency maintenance efforts. A firm, upon learning, that repeated customer complaints have been received about a particular feature, can proactively improve it for free or at cost.  A firm can also use predictive analytics to determine which product features it should introduce to next generation products that will generate the most return for the least cost. For example, a firm might beta test a new video game console with different features, such as new controllers, wireless apps, and games, and analyze usage and purchase behavior to determine which combination should be rolled out with a mass-market strategy. Alternatively, the firm can use this data to determine pricing for various customized versions of the console that will enable the firm to achieve its revenue and profit goals. PROCTER AND GAMBLE: A CASE STUDY  Procter and Gamble (P&G), the consumer goods manufacturer, is one firm that has leveraged Big Data successfully into its new product development process, by aggregating consumer data from multiple brand touchpoints and using it to both launch and promote new products. They use modeling and simulation tools extensively to minimize prototyping expenses. For example, they’ve used them to determine how the molecules in certain household products like dishwashing liquids will react over time to refine the product.  P&G is not only able to use Big Data as a springboard for new ideas; it is able to strategically plan, produce, and launch them. Among other internal business initiatives which optimize operations, P&G has developed what its former CEO, Robert McDonald refers to as “consumer pulse”, which aggregates and filters external data, such as comments and news mentions, using Bayesian analysis (a method of statistical inference used for the dynamic analysis of data sequences) on P&G’s products and brands in real-time, allowing them to react as market developments occur.  P&G has also implemented a system called Control Tower, which provides real-time data on all transportation activity at P&G in over 80 countries. They’ve used this system to not only improve their transportation, but also to reduce their carbon footprint. They use a similar system called Distributor Connect, which lets them manage inventory in real-time. Moreover, the firm keeps connected to their retailers through a globally synchronized data warehouse that allows them to manage commercial transactions in a completely automated fashion.  These systems all aggregate data, which are harnessed by P&G’s marketers, data scientists, and engineers to develop new products and improve existing ones. The firm spends almost $2 billion dollars annually on R&D, and in recent years has worked to systematize innovation by creating multiple groups responsible for creating new products and development. Rather than innovation being pigeon-holed into a single department, P&G linked firm-wide company, business and innovation strategies together for senior executive leadership to review. Moreover, it harnessed Big Data tools and testing. Tide Dry Cleaners, a branded dry cleaning franchise, for example, was developed by leveraging consumer insights about the deficits of the existing dry cleaning industry, its brand, and its own insights into consumer household cleaning habits. There are Tide Dry Cleaners all over the country, featuring 24-hour pickup, drive-through service, and environmentally safe cleaning processes – all consumer preferences P&G packages into a franchise and sells entrepreneurs for a hefty fee. Big data spells big opportunity. Companies that understand how to leverage their structured and unstructured data can uncover valuable customer insights, new ways of marketing their products and services, and entirely new lines of business. But big data is new territory for many businesses, and that makes big data consulting services a potentially lucrative opportunity for IT solution providers with the proper experience, skill sets and know-how.According to CompTIA's Quick Start Guide for solution providers on easing into big data, \"In many ways, big data has become a proxy for data initiatives in general. As such, the big data trend will present a range of direct and indirect business opportunities for IT channel partners with the right mix of technical skill and business savvy.\"  Solution providers with established big data consulting practices agree that big data is simply an evolution of business intelligence and data warehousing. \"The idea of being able to economically scale and manage machine-generated data has been around for at least a decade, if not more,\" explained Mike Fahey, CTO at Chicago-based solution provider Clarity Solution Group. \"The advent of open source technologies that are reliable, as well as an industry that's gotten behind those technologies to make them feasible and accessible to more than just the very large, very mature organizations that have budget to spend is newer.\"  Put more simply, Fahey said, \"I would argue that Wal-Mart has done big data for ages, but now it's economically feasible for a business that's smaller than Wal-Mart [to do big data].\" A new name means new business cases  Solution providers like Clarity Solution Group have offered what they consider big data consulting services for the past two to three years. For these IT channel companies, it's been simply a matter of semantics. John Onder, principal for sales and strategic alliances at CBIG Consulting, a Rosemont, Ill.-based professional services firm, explained: \"Some of it is just marketing and making sure you're addressing what the market is asking for. When the market started recognizing [big data] terms and the technologies, and they started becoming more accepted in traditional business models, and our clients started looking at those technologies -- that's when it made sense to start calling it big data services.\"  The big data space, as a whole, is still being defined. \"The business cases are still forming -- not all there yet -- and I think that's a big thing going on. Over the next couple years, those reference architectures and business use cases and how to use the technology will further define this space and what big data services are,\" Onder said.  That uncertainty presents opportunities for IT channel companies. \"The biggest opportunity for solution providers with this skill set is to do the consulting work around big data,\" said Carolyn April, director of industry analysis at CompTIA. End users aren't that far along in their big data projects, she explained. Many have IT departments that would love to leverage data, but they don't know what data assets they have or how to make use of them. Solution providers can do an assessment of data assets and put together a plan with the business owners regarding what they want to achieve and how to do so.  \"From a business perspective, it's being able to go in and be that consultant who can help the end user figure out what exactly they are trying to do or what the possibilities are. If you can provide some sort of use case and [return on investment] ROI, that's very valuable to the end customer,\" April said.  Clarity's Fahey agreed. \"Our larger clients already have mature data warehousing,\" he said. \"We are helping them figure out how they can launch a big data initiative inside the infrastructure and get ROI out of it.\" Preparing staff to offer big data consulting services  When it comes to acquiring the skill sets needed to offer big data consulting services, CompTIA advises channel firms to \"be prepared to have a multifaceted customer engagement involving technical and business-objective problem-solving with staff across many functional areas.\"  Certifications are helpful in this endeavor, but they must be accompanied by experience. \"Anyone can go out and get a certification,\" explained Paul Cattrone, big data and analytics practice director at Lilien Systems, a Larkspur, Calif.-based solutions provider. \"That doesn't necessarily mean that you have an understanding of technology and what its strengths and weaknesses are. It means you understand from a textbook point of view but not necessarily an implementation point of view. That's where growing up with the technology is a benefit.\"      When you have a conversation with a customer and can say, 'This is how I solved that [same] problem this year or two years ago,' … it gives us a lot of credibility.     Paul Cattrone, big data and analytics practice director, Lilien Systems  Cattrone said his team has worked with big data tools like Hadoop since 2010. This hands-on experience enables them to successfully address customer problems. \"When you have a conversation with a customer and can say, 'This is how I solved that [same] problem this year or two years ago,' … it gives us a lot of credibility,\" he said.  IT channel companies that don't have a background and experience in big data can still develop a big data consulting practice. \"Companies can make it a fit if they work hard enough at it and have the money to invest,\" CompTIA's April said. But, she added, \"That's going to be a much more difficult adoption. It doesn't mean [those IT channel companies] shouldn't do it, but they need to start with a data management practice or database development work and get experience there before jumping into more complex types of analytics that you see being done under the big data scope.\" How to stand out in the big data crowd  IT solution providers considering a foray into big data consulting services should be mindful of the size and scope of the marketplace. \"While the set of pure-play big data firms is relatively small, the broader ecosystem of firms in the data space is quite large. When thinking about potential competitors, it's a good idea to think broadly,\" CompTIA advises in its report.  Big data services can run the gamut from data assessments to business strategy to implementation. These services are being offered by a variety of businesses beyond VARs, systems integrators and IT consultants, including PR firms, survey research firms, brand management firms, marketing firms and business consultants. The CompTIA report advises that solution providers consider these competitors, as well as how they can offer services that are differentiated from those of competitors.  April advised, \"Don't bite off more than you can chew. Do a good self-assessment. Be honest about your skill level, your resources financially and from a human standpoint, and your commitment to this. If you don't do that, you could end up falling flat on your face and have a bad customer experience at the beginning.\"  While the scope of the market is sizeable, April said she isn't seeing a huge immersion in the channel. \"The differentiators are a, doing [big data consulting] and b, doing it competently right now,\" she said.  Beyond those, the differentiators April identified for big data consulting services should sound familiar to IT solution providers, as they could apply to any practice: \"The differentiators are competence and having a good relationship with the customer and being able to provide business advice. Know when to steer customers away from a solution that won't be beneficial and may be an unnecessary expense for them,\" she said.  Of course, being able to make that call requires experience -- a differentiator that Onder said works well for CBIG Consulting. \"The depth of knowledge of our people and [their] experience level and focus in this space is different than most. Most of our people have 10 years of experience in this space. From a big data perspective, we have people who have been there and done it, and seen the evolution and how the pieces fit together, versus someone that has just done Hadoop and doesn't understand where those technologies fit well and where they don't.\" Uncovering big data opportunities  Like establishing differentiators, ramping up a big data consulting business is like starting any other practice area. CompTIA's April advised that IT solution providers begin by looking for opportunities among existing customers. \"That's where you need to start if you're new to this business. You know how their business runs, and you have a relationship. Because the familiarity is there, it will be easier than going to a net-new customer and trying to apply the same insights into a company that you're not familiar with.\"  That said, nearly any business can benefit from big data consulting services. \"Even the smallest businesses are generating so much data internally, almost anyone is a good candidate,\" April said. \"But if you work backwards from the goals of the customer, those are the ones that you want to target -- those that have a good sense of what they want to do.\"  April said solution providers can gather case studies and proof points from these engagements and present them to net-new customers as proof of experience 7 Strategy& In the regulatory arena, new oversight requirements since the financial  crisis have expanded the categories of data that financial-services firms  need to manage, process, and disseminate. For example, new capital  and risk analytics will be required to efficiently manage Basel III and  Dodd-Frank.  Finally, consumer habits and expectations are changing rapidly, driving  some consumers to switch their banks and insurance providers. Social  sites such as Facebook, Twitter, and Pinterest have whetted people’s  appetite for more comprehensive information, and retail sites such as  Amazon and eBay are broadly influencing customer experience  expectations. Google and eBay are quickly entering into the payments  space, demonstrating how nontraditional competitors can enter the  landscape via a data advantage.  The end result for the industry will be a new group of high-margin  business opportunities. This is good news for the industry as a whole,  but it is also likely to attract outside firms with expertise in monetizing  data. Think of PayPal and Google Wallet as the first of these new  competitors. Companies from the computer industry could easily give  some established financial firms a run for their money. Strategy&  estimates that leading financial firms risk losing 10 percent or more of  their potential top-line revenue to nonfinancial competitors within the  next few years if they do not move aggressively to transform the  enterprise today. 8 Strategy& Five basic business models  The first step in seizing the opportunities of data monetization is  picking an appropriate business model for your data and analytics  transformation strategy. There are five basic models — five ways to use  data profitably and effectively ( see Exhibit 2 ). Together they make up a  spectrum of activity for transforming your approach to information: •  Your current core business, improved through use of enhanced data •  Better return on your marketing investment using insights about  your customers or transactions Leverage enhanced  data for core  business Generate new  insights White-label  capabilities &  infrastructure Create new data Develop new businesses Enter adjacent businesses Enhance current business Create new offerings –  Seek opportunities to  enrich existing service  thr ough new data  sour ces –  Develop and leverage  new platforms  –  Deliver enhanced  services (e.g., in r eal  time)  – Understand deep client  insights – Enhance marketing  campaign ROI and  conversion  – Monetize existing  analytics capabilities via  white labeling to clients  and other partners  acr oss the value chain  – Commer cialize  infrastructur e to sell  platforms as a service  – Partner with adjacent  players acr oss the  business value chain – Identify new sour ces of  data (e.g., unstructur ed)  to join with existing data  sets  – Monetize new sets  of data  – Develop new sets  of analytics and  data pr oducts (e.g.,  benchmarks, tools) – Develop new pr oducts  that benefit fr om  enhanced data and  analytics (e.g., re al-time  net asset value, active  non-disclosed exchange  traded funds)  Exhibit 2 Data and analytics transformation spectrum Source: Strategy&  9 Strategy& •  White-label capabilities and infrastructure delivered by other firms •  Delivery of new types of data directly to customers •  New products and services made possible by data and analytics  All these business models will generate incremental revenue. However,  some are more transformative than others — in other words, they are  more likely to reshape your business overall. Financial-services firms  that are creating new data and developing data and analytics products  are truly evolving into something more like a technology business. They  may even start to enjoy the higher valuation multiples associated with  technology and data firms.  Corporate leaders can ask themselves a series of questions to  understand where they would best fit on the data and analytics  transformation spectrum: 1.  Does my firm have a strong product set but limited conversions,  cross-sells, or marketing results?  2.  Do I own the market in key categories but have trouble enticing  customers with new products?  3.  Do customers need my products in real time or integrated with  common industry platforms? 4.  Is my firm’s accessible data robust? For example, do I see 20 percent  or more of the industry’s transactions or volume? 5.  Can I partner with others to improve the quality, value, and delivery  of my data? 6.  Can I use data to develop products and services in adjacent or new  business sectors? The answers to these questions will help a company identify the  best business model for its data and analytics transformation strategy  ( see Exhibit 3, next page ). Frequently, companies will pursue more than  one of these models to reach various customers and address different  needs. 13 Strategy& Case study: Tesco market adjacencies offer new opportunities to leverage your data.  For example, by understanding how merchants interact with order  management system vendors, a payments company can identify  potential partnerships and opportunities for its data. One merchant  might benefit from a better understanding of segment-specific demand  patterns; another might benefit from understanding sales volume by  time of day. To identify these relationships and opportunities, it’s  helpful to map the data ecosystem to see your adjacent relationships,  as well as those relationships’ adjacent relationships, and so on.  Size the value:  Understanding the market opportunity for  commercializing data is a key design principle. Each of these business  models can yield a substantial internal rate of return; indeed, each of  them can be self-funded with the right design. One reason is that  U.K.-based supermarket giant Tesco is a  prime example of a nonfinancial company  that’s using data to compete effectively  with traditional financial players. Until  2008, the company ran Tesco Bank as a  50/50 joint venture with the Royal Bank  of Scotland. That year Tesco bought out  RBS and began developing a completely  new infrastructure for the business,  built a new team, and brought in new  expertise. The transition was not always  smooth — for instance, online customers  were locked out of accounts for several  days in 2011 when Tesco moved data  from the RBS systems to its own — but  it’s now complete. To fully exploit this  treasure trove of data, the company took  a significant stake in Dunnhumby, a U.K.  data mining firm that will help Tesco  monetize the consumer data from both  the retail and banking operations.  At its core, Tesco Bank is underpinned by  the Clubcard. The insights the bank gains  from the Clubcard customer data allow  the company to understand customer  needs and make the most relevant offers  in the store and in the bank. The Clubcard  credit card rewards customers with points  whenever they use their card — one  Clubcard point for every £4 (US$6.12)  spent. Clubcard customers can also receive  preferential deals when buying Tesco Bank  products — including discounts on car,  home, pet, and travel insurance — and can  use points to buy Tesco Bank insurance.  This year, Tesco Bank gave customers  around £70 million (US$107 million)  worth of points to spend in the store or  on Clubcard rewards. In terms of systems  and IT, Tesco’s new platforms significantly  improve customer service. Instant decisions  are now possible on loan applications,  and customers can open and fund savings  accounts in just 10 minutes rather than the  two weeks required in the past.  The conversion is still in its early days,  but Tesco’s efforts are paying dividends  in the form of increased market share  across a range of products. In 2009, Tesco  Bank credit cards made up 9 percent  of all MasterCard and Visa credit card  transactions in the U.K., and by 2012  that figure had grown to 12 percent.  Meanwhile, from 2008 to 2012, the  company’s car insurance gross written  premiums increased by 39 percent and  pet insurance gross written premiums  rose 44 percent. 14 Strategy& investing to digitize your business can save 25 to 35 percent in  operating costs. To size value, you need to understand your competitors  and your own key inputs — such as the various costs — as well as the  marketplace demand for the data products. Enhance the infrastructure:  At the foundation of the data strategy must  be an information technology infrastructure that is sophisticated,  transparent, and flexible enough for a company to unlock the value of  its data. At a minimum, the infrastructure needs to provide single  sources of data truth, real-time data for messaging and transactions that  can be monetized (even without additional analytics), and a highly  available infrastructure. These five core design principles should guide your practice as you  design and execute your strategy for monetizing data. You will invest  and manage a host of new activities ( see Exhibit 4 ). They will enable you  to build the capabilities you need for the data gold rush.  Exhibit 4 Building capabilities for the data gold rush Source: Strategy&  Investment in  continuous  improvement Cataloging &  mapping  existing data Developing  insight  adjacencies Combining  structured &  unstructured data Building data  infrastructure - Invest in continuous lear ning and management of clients’ or customers’ unmet needs acr oss the value chain. - Truly understand the delivery and integration models that clients r equire  to benefit fr om enhancements to  products or new pr oducts and services. - Understand, catalog, and map data housed acr oss all business lines. - Map data and analytics services acr oss business units to understand what types of capabilities can be  leveraged to build new pr oducts and services.  - Determine additional opportunities acr oss the value chain by developing insights into adjacencies, for both  data and partners. - Create a compr ehensive view of the data ecosystem. - Combine “inter nally owned” structur ed data with both inte rnal and exter nally sour ced semi-structur ed and  unstructur ed data, market data, telemetry data, etc. - Seek out opportunities to enhance the cor e business or develop new pr oducts and services. - Put in place a data infrastructur e that can pr ovide the necessary foundation to enable the or ganization to  unlock the value of data assets.  - Continue to evaluate cor e capabilities to determine what suits the particular situation best . 15 Strategy& Conclusion Most financial-services companies still don’t think of their data as an  asset they can monetize if they build the right capabilities. They’re  overlooking an opportunity in plain sight. Not only can they make  better use of their data to enhance products and services for current  clients, but new technologies now allow them to take the next step and  combine their data with external data. This marriage will create  powerful new insights into the customer and supply base and could  transform the enterprise, unleashing a whole new generation of high- margin products and services for current customers and customers in  adjacent markets. The enormous revenue potential will not go ignored  by others. If financial-services firms don’t embark on data-led  transformations to seize this opportunity, more nimble nonfinancial", "category": "Edison", "id": 133}
{"skillName": "DSDA04", "skillText": "Data blending lets analysts in the line of business access and combine data from multiple sources to reveal deeper intelligence that drives better business decision-making. Analysts use data blending to build an analytic dataset to answer a specific business question or take advantage of opportunities, with insight into customer preferences, marketing campaign results, financial operations, site and merchandising optimization, and much more. Alteryx's platform for self-service data analytics delivers the data blending needs that analysts require to create the most comprehensive data needed to answer these business questions.    Data blending is when you combine data from multiple data source types in a single worksheet. The data is joined on common dimensions. Data Blending does not create row level joins and is not a way to add new dimensions or rows to your data. Refer to Join Your Data to learn how to create those types of joins. Instead, data blending should be used when you have related data in multiple data sources that you want to analyze together in a single view. For example, you may have Sales data collected in an Oracle database and Sales Goal data in an Excel spreadsheet. To compare actual sales to target sales, you can blend the data based on common dimensions to get access to the Sales Goal measure.  To blend your data, you must first define common dimensions between the primary and secondary data sources. For example, when blending Actual and Target sales data, the two data sources may have a Date field in common. The Date field must be specified as a linking field. If the two dimensions don’t have the same name, you can define a custom relationship that creates the correct mapping between fields.  For each data source that is used on the sheet, a query is sent to the database and the results are processed. Then all the results are left joined on the common dimensions. The join is done on the member aliases of the common dimensions so if the underlying values aren’t an exact match, you can fix it up in Tableau.  With a left join, the view uses all data rows from the primary data source but only those data rows from the secondary data source that have values for fields that are in the view or for fields that are designated as linking fields. So changing the linking field, or designating multiple linking fields, can actually pull in different or additional data rows from the secondary data source, thereby changing the values returned by aggregations.  In general, a good test to see whether data can be blended smoothly is to drag the dimensions from the primary data source into a text table on one sheet. Then on another sheet, drag the same fields from the secondary data source into a text table. If the two tables match up then the data is most likely going to blend correctly.   Data integration involves combining data residing in different sources and providing users with a unified view of these data.[1] This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume and the need to share existing data explodes.[2] It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.  Contents      1 History     2 Example     3 Theory of data integration         3.1 Definitions         3.2 Query processing     4 Data Integration tools     5 Data integration in the life sciences     6 See also     7 References  History Figure 1:  schematic for a data warehouse. The ETL process extracts information from the source databases, transforms it and then loads it into the data warehouse. Figure 2:  schematic for a data-integration solution. A system designer constructs a mediated schema against which users can run queries. The virtual database interfaces with the source databases via wrapper code if required.  Issues with combining heterogeneous data sources, often referred to as information silos, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.[3] The first data integration system driven by structured metadata was designed at the University of Minnesota in 1991, for the Integrated Public Use Microdata Series (IPUMS). IPUMS used a data warehousing approach, which extracts, transforms, and loads data from heterogeneous sources into a single view schema so data from different sources become compatible.[4] By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach offers a tightly coupled architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.[5]  The data warehouse approach is less feasible for datasets that are frequently updated, requiring the ETL process to be continuously re-executed for synchronization. Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data. This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.  As of 2009 the trend in data integration favored loosening the coupling between data[citation needed] and providing a unified query-interface to access real time data over a mediated schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the SOA approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases. Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the \"Global As View\" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the \"Local As View\" (LAV) approach). The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.  As of 2010 some of the work in data integration research concerns the semantic integration problem. This problem addresses not the structuring of the architecture of the integration, but how to resolve semantic conflicts between heterogeneous data sources. For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like \"earnings\" inevitably have different meanings. In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer). A common strategy for the resolution of such problems involves the use of ontologies which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents ontology-based data integration. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.[6]  As of 2011 it was determined that current data modeling methods were imparting data isolation into every data architecture in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.[7][8] One enhanced data modeling method recasts data models by augmenting them with structural metadata in the form of standardized data entities. As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models. Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models. Multiple data models that contain the same standard data entity may participate in the same commonality relationship. When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.  Since 2011, Data hub approaches have been of greater interest than fully structured (relational) Enterprise Data Warehouses. Since 2013, Data lake approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.[9] These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub. Example  Consider a web application where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.). Traditionally, the information must be stored in a single database with a single schema. But any single enterprise would find information of this breadth somewhat difficult and expensive to collect. Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.  A data-integration solution may address this problem by considering these external resources as materialized views over a virtual mediated schema, resulting in \"virtual data integration\". This means application-developers construct a virtual schema — the mediated schema — to best model the kinds of answers their users want. Next, they design \"wrappers\" or adapters for each data source, such as the crime database and weather website. These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2). When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources. Finally, the virtual database combines the results of these queries into the answer to the user's query.  This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them. It contrasts with ETL systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage virtual mediated schema to implement data harmonization; whereby the data are copied from the designated \"master\" source to the defined targets, field by field. Advanced Data virtualization is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using hub and spoke architecture.  Each data source is disparate and as such is not designed to support reliable joins between data sources. Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets. Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.  One solution is to recast disparate databases to integrate these databases without the need for ETL. The recast databases support commonality constraints where referential integrity may be enforced between databases. The recast databases provide designed data access paths with data value commonality across databases. Theory of data integration  The theory of data integration[1] forms a subset of database theory and formalizes the underlying concepts of the problem in first-order logic. Applying the theories gives indications as to the feasibility and difficulty of data integration. While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,[10] including those that include nested relational / XML databases [11] and those that treat databases as programs.[12] Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as JDBC and are not studied at the theoretical level. Definitions  Data integration systems are formally defined as a triple ⟨ G , S , M ⟩ {\\displaystyle \\left\\langle G,S,M\\right\\rangle } \\left\\langle G,S,M\\right\\rangle where G {\\displaystyle G} G is the global (or mediated) schema, S {\\displaystyle S} S is the heterogeneous set of source schemas, and M {\\displaystyle M} M is the mapping that maps queries between the source and the global schemas. Both G {\\displaystyle G} G and S {\\displaystyle S} S are expressed in languages over alphabets composed of symbols for each of their respective relations. The mapping M {\\displaystyle M} M consists of assertions between queries over G {\\displaystyle G} G and queries over S {\\displaystyle S} S. When users pose queries over the data integration system, they pose queries over G {\\displaystyle G} G and the mapping then asserts connections between the elements in the global schema and the source schemas.  A database over a schema is defined as a set of sets, one for each relation (in a relational database). The database corresponding to the source schema S {\\displaystyle S} S would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the source database. Note that this single source database may actually represent a collection of disconnected databases. The database corresponding to the virtual mediated schema G {\\displaystyle G} G is called the global database. The global database must satisfy the mapping M {\\displaystyle M} M with respect to the source database. The legality of this mapping depends on the nature of the correspondence between G {\\displaystyle G} G and S {\\displaystyle S} S. Two popular ways to model this correspondence exist: Global as View or GAV and Local as View or LAV. Figure 3: Illustration of tuple space of the GAV and LAV mappings.[13] In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.  GAV systems model the global database as a set of views over S {\\displaystyle S} S. In this case M {\\displaystyle M} M associates to each element of G {\\displaystyle G} G a query over S {\\displaystyle S} S. Query processing becomes a straightforward operation due to the well-defined associations between G {\\displaystyle G} G and S {\\displaystyle S} S. The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases. If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.  In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators. For example, consider if one of the sources served a weather website. The designer would likely then add a corresponding element for weather to the global schema. Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website. This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.  On the other hand, in LAV, the source database is modeled as a set of views over G {\\displaystyle G} G. In this case M {\\displaystyle M} M associates to each element of S {\\displaystyle S} S a query over G {\\displaystyle G} G. Here the exact associations between G {\\displaystyle G} G and S {\\displaystyle S} S are no longer well-defined. As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor. The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.[1]  In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources. Consider again if one of the sources serves a weather website. The designer would add corresponding elements for weather to the global schema only if none existed already. Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas. The complexity of adding the new source moves from the designer to the query processor. Query processing  The theory of query processing in data integration systems is commonly expressed using conjunctive queries and Datalog, a purely declarative logic programming language.[14] One can loosely think of a conjunctive query as a logical function applied to the relations of a database such as \" f ( A , B ) {\\displaystyle f(A,B)} f(A,B) where A < B {\\displaystyle A<B} A<B\". If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query. While formal languages like Datalog express these queries concisely and without ambiguity, common SQL queries count as conjunctive queries as well.  In terms of data integration, \"query containment\" represents an important property of conjunctive queries. A query A {\\displaystyle A} A contains another query B {\\displaystyle B} B (denoted A ⊃ B {\\displaystyle A\\supset B} A\\supset B) if the results of applying B {\\displaystyle B} B are a subset of the results of applying A {\\displaystyle A} A for any database. The two queries are said to be equivalent if the resulting sets are equal for any database. This is important because in both GAV and LAV systems, a user poses conjunctive queries over a virtual schema represented by a set of views, or \"materialized\" conjunctive queries. Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query. This corresponds to the problem of answering queries using views (AQUV).[15]  In GAV systems, a system designer writes mediator code to define the query-rewriting. Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source. Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent. While the designer does the majority of the work beforehand, some GAV systems such as Tsimmis  involve simplifying the mediator description process.  In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a  expansion strategy. The integration system must execute a search over the space of possible queries in order to find the best rewrite. The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete. As of 2009 the MiniCon algorithm[15] is the leading query rewriting algorithm for LAV data integration systems.  In general, the complexity of query rewriting is NP-complete.[15] If the space of rewrites is relatively small this does not pose a problem — even for integration systems with hundreds of sources. Data Integration tools      Alteryx     Analytics Canvas     Cloud Elements API Integration     DataWatch     Denodo Platform     HiperFabric      Lavastorm     ParseKit (enigma.io)     Paxata     RapidMiner Studio     Red Hat JBoss Data Virtualization. Community project: teiid.     Azure Data Factory (ADF)     SQL Server Integration Services (SSIS)  Data integration in the life sciences  Large-scale questions in science, such as global warming, invasive species spread, and resource depletion, are increasingly requiring the collection of disparate data sets for meta-analysis. This type of data integration is especially challenging for ecological and environmental data because metadata standards are not agreed upon and there are many different data types produced in these fields. National Science Foundation initiatives such as Datanet are intended to make data integration easier for scientists by providing cyberinfrastructure and setting standards. The five funded Datanet initiatives are DataONE,[16] led by William Michener at the University of New Mexico; The Data Conservancy,[17] led by Sayeed Choudhury of Johns Hopkins University; SEAD: Sustainable Environment through Actionable Data,[18] led by Margaret Hedstrom of the University of Michigan; the DataNet Federation Consortium,[19] led by Reagan Moore of the University of North Carolina; and Terra Populus,[20] led by Steven Ruggles of the University of Minnesota. The Research Data Alliance,[21] has more recently explored creating global data integration frameworks. The OpenPHACTS project, funded through the European Union Innovative Medicines Initiative, built a drug discovery platform by linking datasets from providers such as European Bioinformatics Institute, Royal Society of Chemistry, UniProt, WikiPathways and DrugBank. See also      Business semantics management     Core data integration     Customer data integration     Data curation     Data fusion     Data mapping     Data virtualization     Data Warehousing     Data wrangling     Database model     Datalog     Dataspaces     Edge data integration     Enterprise application integration     Enterprise Architecture framework     Enterprise Information Integration (EII)     Enterprise integration     Extract, transform, load     Geodi: Geoscientific Data Integration     Information integration     Information Server     Information silo     Integration Competency Center     Integration Consortium     JXTA     Master data management     Object-relational mapping     Ontology based data integration     Open Text     Schema Matching     Semantic Integration     SQL     Three schema approach     UDEF     Web service   data blending Informally: the answer is not always written at the same book as the question. Thus, we must learn to decipher it from multiple books. Some of them are in a foreign language, some are hundreds of times thicker than others, and most of them are by different authors who have never agreed on a literary style. And there is no catalogue.  Data integration refers to collection of data from multiple sources, including changes of format and cleanup of redundant or useless entries. The outcome is a standardized, unified table.  Data fusion almost invariably means integration of imperfect data sources overlapping over a small group of objects (perhaps a single object, think: target tracking).  Data blending (as we have been using it) allows sources to be imperfect, incomplete, and overlap over a few objects or none at all, requiring inspired guesses and generalizations. These guesses will then be subjected to rigorous hypothesis testing, which is where it becomes science again, not narrative about data.  We are at the initial stage of multidisciplinary investigation. Thus, we are happy with having just the narrative about data. Comparable Topics in Applied Science  Consider the core data set describing patients in medical care. The variables (or features) describing patients are schematically divided into two groups: [A] [B]. There is also an outcome, or a set of outcomes of interest Y=F(A,B) approximated by a model M: Y=M(A,B). Here, F is unknowable “true” relationship between cause and effect. M is its practical approximation, with simplifications and noise.  Features in A are private, highly specialized (difficult to obtain, transfer or interpret without additional skills and tools, e.g. 3-d internal scans); many of them are in the status of unknown knowns (i.e. we don’t know if such measurements are possible before we specifically ask for them). External researchers should not be able to see A until they have very specific reasons.  Features in B are possible to share for research. They are standard clinical variables listed in our data dictionaries. We provide ~100 variables, but in practice there are time-dependent sets of thousands of features (more if we include genomic data). In ~1970s – 2010s they were extensively used in clinical informatics. Modern methods of statistics / machine learning were used to create models in the format Y=M(B). For many such models, we have reached a stage where incremental development continues, but qualitative improvement is very hard or impossible.  The connected society of the 21st century offers another option to analysis of healthcare data – via blending with socioeconomic data. Consider the general population as represented through multiple information sources. In the population, a person (or aggregate of a group of people) will be characterized by features [B’] [C]. Here B’ is a much simplified subset of B, perhaps including only a few general labels for condition of interest, such as: ‘obesity’, or ‘PTSD’, or ‘clinical inpatient’. The most interesting part is variables in [C] as many of them were never considered as a part of healthcare study.  The task of data blending, put very simply, consists of the two parts:      Given [B], find data sets that include [B’ C]. Even that valuable: many, if not most academic data science programs have not developed this capacity.     Make a case that [B’ C] can be used to predict Y. Does not have to predict “better”; we are happy with additional volume of data at the cost of prediction quality. The narrative does not have to be mathematically rigorous; we hope to see a lot of arguments using subject matter knowledge.  Expectations for Participants  Once we review the narratives from the teams, the next goal of the study would be to compare Y=M1(B) versus Y=M2(B’,C) , where M1 is some standard clinical model (we will be using very basic medical informatics literature) and M2 is innovative, based on subject knowledge and guesswork, perhaps not yet rigorously tested.  Mature machine learning must imitate human ability to acquire data from multiple sources. While we have not achieved true AI yet, a modern data-driven organization with humans and computers is a working substitute. In the language of machine learning/statistical inference, data blending closely corresponds to inductive transfer, or transfer learning, and there is a good amount of mathematical literature on the subject.  However, human and organizational intelligence does not consciously reproduce mathematical process. We transfer skills and portions of knowledge, and then test their appropriateness in the new situation. In that type of cognitive activity, thinking by analogy is allowed, and ability to set up the connection is relatively more valuable (we have statistical approaches to testing, so the latter part of the process is largely figured out). History of Data Blending  When we turn to the practical experience in the industry that introduced the concept of data blending, we see that the concept emerged in the data science community as a topic of interest around 2014 or late 2013. At the time, software packages like Tableau were offering a “data blending” method, which was intended to improve productivity and experience for the segment of the user population whose primary interface to data was through the tool itself (as opposed to power users who could combine multiple data feeds themselves and often had no need for this convenience). The way this method worked is follows: suppose one is interested in combining spreadsheet-based data (e.g., in Excel or a local .csv file) with data stored in an enterprise data-management system, perhaps Oracle or Hadoop. Typically, a business analyst would require an exchange with the team responsible for data engineering or ETL in order to realize this workflow. In 2014, “Data Blending” meant that the BI tool was capable of providing this functionality directly for the end-user by, for example, treating Oracle and Excel abstractly as relational stores and leveraging user- or enterprise-defined metadata to reason about the necessary joining structure. At this point in time, the “data blending” workflow consisted of: 1) identifying the data sources one wishes to blend 2) describe to the tool some metadata concerning the desired join 3) perform some standard cleaning and sanity-checks against the results.  This led to some interesting consequences, as the user community encountered certain performance bottlenecks in dealing with compute loads distributed across server and client. For instance, Teradata is designed to map the join of two large tables efficiently, and the underlying appliance has enough horsepower to return this result to the user in a timely manner. In a data blending scenario, the BI tool is performing the join across multiple systems, which are each (presumably) ignorant of the total data-space of the join. In this situation, it falls to the client (or some intermediary machine) to marshal resources to execute the join. In the worst-case scenario, this means transferring large amounts of data to the user’s machine and doing large-scale joins on the client-side. Users quickly sought to employ the typical trick, which is to coursen the join parameter and stage intermediate joins on the client side. For example, if joining by date, do the joins one-year-at-a-time. This is an asymmetrical method leveraging the asymmetrical computing power between server and host. Extending the Method  The inversion of this use-pattern led to the idea of data blending as we have been presenting it. This method has been described as the creation of fictitious identifiers between clusters, and the joining thereof–which is true, and carries all of the caution tape associated with the intentional addition of bias into a data science workflow. However, it also represents an opportunity for the user to include domain-, method-, or problem-specific metadata which associates data the underlying system may not itself have a capacity to associate. In industry, it is not uncommmon, for example, to granularize features such as income, temperature, highest-education-level, occupation, etc, by zip code (geospatial discretization) in a way that the resulting set is its own commercially viable product or is representative of the knowledge base of a particular enterprise. Even subjective data has use–it is no stretch of the imagination that Google would happily pay a large amount for a model which would perfectly identify whether a given user was an active smoker. However, it is known that the company is in no hurry to retire whatever model they may already have, however imperfect it may be.  This is not the only necessary approach. Analysts could condition their models upon different classes of clusters, resulting in a multitude of models for individual cluster-combinations. Practitioners realized that there aren’t that many dimensions of freedom in the underlying “universal” parametrization of the system, so either one is forced to take clever approaches toward aliasing cluster-combinations (e.g., probabilistically), or to have been gifted with a sufficiently large amount of data to properly train a respectable subset of the cluster-combinations Conclusions  Another aspect is in getting systems to take the essential step to data-blending, even on their own. This is a ubiquitous challenge for those engineering Big Data systems. A technically-challenged user might “know” what they want to do–for instance, they want a 90-day moving average of the regional sale figures across the geographically-diverse business units of a company–but they don’t necessarily know how to make the system perform the requisite operations. Unfortunately, it is extraordinarily difficult to design a system which is capable of inferring this requirement, both with or without the introduction of carefully-curated metadata about the business and its underlying business processes. The insight here is that the successful user is performing operations of which the system is incapable: they are leveraging metadata to reason about the combination of data elements. Successful users leverage the bias they’ve gained as domain-experts in order to create solutions to what are otherwise combinatorially-challenging problems.  Although this provides very few concrete examples of how to do so, we hope to convey at least one interest facet behind the intent of this Collider: leverage interesting metadata. Introductory statistics contains useful insight regarding how one quantifies and removes bias. An entire specialty is concerned with the careful design of experiment. The practitioner then performs complex operations with a degree of confidence that the “average case” holds. Contrast to the state-of-the-art in the field: introduce bias in a controlled manner, inspect its implications, and inject knowledge in a way that unlocks the true potential of the analyst and of the data.     2 Data Blending Defined Data blending is the process of combining data from multiple sources to create  an actionable analytic dataset for business decision-making (such as retail site  selection or multichannel profiling) or for driving a specific business process  (such as packaging data for sale by data aggregators). Data blending is needed when an organization’s data  management processes and infrastructure are insufficient  for bringing together analytic or specific datasets  required by line-of-business groups. It can, for example,  readily bring together disparate data, such as customer  information from a cloud sales automation system  (e.g., Salesforce.com) with clickstream web data stored  in a Hadoop file system and segmentation models  from Microsoft Excel. This is important, because while  organizations aspire to have a completely integrated  data management system, the majority of data required  to make strategic business decisions still resides outside  their IT-controlled data environment.  Data blending differs from data integration and data  warehousing in that its primary use is not to create the  single unified version of the truth that is stored in a  data warehouse, data mart, or other system of record  within an organization—and is conducted by  a data warehousing or business intelligence professional.  Instead, this process is conducted by a business or data  analyst with the goal of  an analytic dataset  to assist in answering a specific business question. Common Use Cases Implementing data blending into the line of business can deliver greater  benefits and deeper insight in hours—significantly faster than the weeks  required for manual processes and traditional IT approaches.  This time savings can be realized in the myriad business situations in which  data analysts find themselves. Let’s look at a few examples where data  blending can positively impact business decision-making. Sales and Marketing For every organization, growing revenue coincides with targeting  prospects who are ready to buy. Marketing departments spend a lot of time  trying to identify these prospects so they can focus campaigns, and allow  salespeople to concentrate their cross-sell and up-sell efforts most efficiently.  Most customer data is stored in a CRM system—either in a database or possibly  in a cloud solution, such as Salesforce.com—while marketing prospect data is  stored in a separate system, such as Marketo. And information about customer  and website prospect activity is captured by web analytics technology, such as  Google Analytics. Historically, finding the relevancy of this data could require  generating spreadsheet-based reports from both the marketing automation  and CRM systems. From there, an analyst might need to combine these into a  single spreadsheet with multiple tabs and construct formulas using VLOOKUP  functions to reference relevant information. Or, they may just combine the two  spreadsheets into one and manually look for duplications. What’s more, the  web analytics may not be something that can be incorporated at all without  some sort of custom work by IT staff.  3 The beauty of data blending is that an analyst can access this data directly  from the environment in which it is located. All they need is the right  credentials to access the data. Then, they can pull the data from the right  systems and start combining the data on common fields, blending in the  specific information for which they are looking. They can combine data on  customer ID, for instance, and discover what products or services not only  have the biggest impact on sales, but also which of these drive the interest  of prospective buyers. Financial Operations Analysts within the realm of finance understand how critical it is to get the  right information to deliver the right results. For instance, data plays an  important role in the loan and credit card approval processes, making the  difference between approving individuals with a low probability of default  or fraud and those with a high probability, impacting the financial risk for the  organization. It can be as  as  out a customer model and then  tracking and trending detailed client information over several years. Typically,  this includes combining a lot of data from several sources, including web  logs, which can be unstructured or need to be cleaned up, and even multiple  third-party databases that contain information on past and current customers. Data blending reduces the time to insight from weeks to hours, allowing  analysts to work with the data directly to improve its quality and cleanliness,  and combine it into a usable format that can be fed directly into existing models.  Site and Merchandising Optimization In order to have a successful store, understanding your potential customers  and prospects is crucial. You may need to look at customer spend levels,  purchase history, and path-to-purchase to discover these customers. Once you  understand that, you can then use that insight for targeting, media planning,  and other multi-channel initiatives. This might mean taking data from an  existing CRM system, looking at loyalty card data, or even reviewing inventory  data. But what if you don’t have enough of this data to make a well-informed  decision about where to locate a new store or what merchandise you need  to stock in order to make that store successful? One way to do this is to analyze data from third-party providers, such as  Experian, Dun and Bradstreet, or the US Census Bureau, and combine it  with internal customer data to identify the factors that indicate the highest  propensity to buy. Examples include ethnicity mix, age, and consumer spending  on similar goods and services. By determining these market factors up front,  you can optimize your real estate investment by opening stores in the right  location and putting the right types of merchandise in each store to drive  profitability. Fulfilling the Requirements of Data Blending Data blending empowers data analysts to incorporate data of any type or  source into their analysis for faster, deeper business insight, but how do  organizations enable a data analyst to perform data blending? Many line-of- business analysts have abandoned spreadsheets and custom work projects in  favor of  Alteryx Analytics  because it fulfills today’s data blending requirements. Understand the progression of data.  The drag-and-drop workflow environment  in Alteryx allows analysts to build out analytic datasets the way they think. It  lets the analyst understand how data progresses through the process without  any “black boxes” and quickly identify where issues may lie. This drag-and- drop technology allows analysts to focus more on the data and less on the  technology by eliminating the need for coding or programming.    4 Enable direct access to data.  Alteryx gives analysts direct access to data  of any type or source to help deliver a more complete view of the insight they  need to make more informed decisions. Because they no longer need to rely on  overworked IT staff or data specialists, analysts can access all the information  they need to make informed business decisions, including: •  Local data (spreadsheets, user device generated data, enterprise data  warehouses, etc.) •  Third-party data (Dun & Bradstreet, Experian, Tom Tom, US Census, etc.)  •  Cloud/social data (Twitter, Facebook, Salesforce.com, Marketo, etc.)  •  Other analytics platforms (SPSS, SAS, Google Analytics, etc.)  Expedite data cleansing and preparation.  Studies estimate that 60% to 80%  of an analyst’s time is spent preparing data for analysis. Alteryx offers extensive  tools for data preparation and data cleansing to speed up the time to create  the right dataset, without having to rely on outside intervention. With options  for restructuring, reformatting, and filling in missing or incomplete data,  Alteryx ensures that data quality, integration, and transformations are done  by the people who know the data and understand the business best, leading  to the right dataset in the least amount of time.  Simplify blending of data.  Alteryx gives users complete flexibility in joining  multiple datasets thanks to an array of tools that can address virtually any  data situation. Joining data in Alteryx is not limited to just one field or column;  Alteryx allows data of any type or level to be brought together. This means that  data can be joined at both the record and  field levels, and it can even be expanded  to include multiple key fields. What’s more,  Alteryx is flexible enough to join data  from non-identical fields as well as  incorporate spatial characteristics, such  as customer points, into the dataset. Other  tools, such as Fuzzy Matching, give users  the ability to match two datasets based on  non-matching data—names and  addresses. In addition, tools such as Append  Fields, Find Replace, and Make Group  allow users to do even more to effectively  blend or refine their resulting dataset.   Automate and repeat processes.  With the amount of ad-hoc analysis required  by today’s analysts, what if there were a way to make this process easier, faster,  and repeatable? With Alteryx, there is. Alteryx workflows can easily be saved  and repeated for further data blending, processing, updates, and analysis.  Updating the analysis or report is as  as updating the data input(s).  Output data easily.  Once the heavy lifting of data blending is completed,  analysts can implement this data into the right processes of the business.  This means that resulting outputs can then be pushed back into a database,  incorporated into an operational process, analyzed further using statistical,  spatial, or predictive methods, or pumped into visualization software,  such as QlikView or Tableau. Conclusion Traditionally, data was the domain of IT and data scientists—doling out access  to a select few via careful SQL queries, heavily structured reports, BI dashboards,  and, maybe, programmatic access. With first-generation tools, the process to  generate results was long, expensive, and difficult. Highly skilled and expensive  data scientists would work with Ph.D.-level statisticians and IT professionals  to obtain and massage data, develop complex analytic models and, ultimately,  generate analytic results. Analysts were left at the door waiting for results that  would then have to be extensively reviewed, tested, and re-adjusted to fit their  original business cases. The result? Data was often neither timely nor adequate  to answer their questions. While traditional data analysts use traditional IT tools to generate reports on  historic data, today’s analysts must extend that capability with their business  insight and natural creativity to find information their organizations really  need. With improvements in information technology and the constant influx  of Big Data, a flood of new opportunities for business insight has appeared. Empowered by next-generation tools such as Alteryx, today’s analysts can now  do what previous generations of analysts could only dream of doing. These  analysts are able to perform data blending to create the analytic dataset they  need to deliver the deeper business insights they require.  Data fusion is the process of integration of multiple data and knowledge representing the same real-world object into a consistent, accurate, and useful representation. fusion of the data from 2 sources (dimension #1 & #2) can yield a classifier superior to any classifiers based on dimension #1 or dimension #2 alone  Data fusion processes are often categorized as low, intermediate or high, depending on the processing stage at which fusion takes place.[1] Low level data fusion combines several sources of raw data to produce new raw data. The expectation is that fused data is more informative and synthetic than the original inputs.  For example, sensor fusion is also known as (multi-sensor) data fusion and is a subset of information fusion.  Contents      1 Geospatial applications     2 Data integration     3 The JDL/DFIG model         3.1 Application areas     4 Position data fusion     5 Data fusion from multiple traffic sensing modalities     6 Decision Fusion     7 Data fusion for Enhanced Contextual Awareness     8 See also     9 References     10 General references     11 Books     12 External links  Geospatial applications  In the geospatial (GIS) domain, data fusion is often synonymous with data integration. In these applications, there is often a need to combine diverse data sets into a unified (fused) data set which includes all of the data points and time steps from the input data sets. The fused data set is different from a  combined superset in that the points in the fused data set contain attributes and metadata which might not have been included for these points in the original data set.  A simplified example of this process is shown below where data set \"α\" is fused with data set β to form the fused data set δ. Data points in set \"α\" have spatial coordinates X and Y and attributes A1 and A2. Data points in set β have spatial coordinates X and Y and attributes B1 and B2. The fused data set contains all points and attributes  Input Data Set α Point \tX \tY \tA1 \tA2 α1 \t10 \t10 \tM \tN α2 \t10 \t30 \tM \tN α3 \t30 \t10 \tM \tN α4 \t30 \t30 \tM \tN  Input Data Set β Point \tX \tY \tB1 \tB2 β1 \t20 \t20 \tQ \tR β2 \t20 \t40 \tQ \tR β3 \t40 \t20 \tQ \tR β4 \t40 \t40 \tQ \tR  Fused Data Set δ Point \tX \tY \tA1 \tA2 \tB1 \tB2 δ1 \t10 \t10 \tM \tN \tQ \tR δ2 \t10 \t30 \tM \tN \tQ \tR δ3 \t30 \t10 \tM \tN \tQ \tR δ4 \t30 \t30 \tM \tN \tQ \tR δ5 \t20 \t20 \tM \tN \tQ \tR δ6 \t20 \t40 \tM \tN \tQ \tR δ7 \t40 \t20 \tM \tN \tQ \tR δ8 \t40 \t40 \tM \tN \tQ \tR  In this  case all attributes are uniform across the entire analysis domain, so attributes may be simply assigned. In more realistic applications, attributes are rarely uniform and some type of interpolation is usually required to properly assign attributes to the data points in the fused set. Visualization of fused data sets for rock lobster tracks in the Tasman Sea. Image generated using Eonfusion software by Myriax Pty. Ltd. - eonfusion.myriax.com  In a much more complicated application, marine animal researchers use data fusion to combine animal tracking data with bathymetric, meteorological, sea surface temperature (SST) and animal habitat data to examine and understand habitat utilization and animal behavior in reaction to external forces such as weather or water temperature. Each of these data sets exhibit a different spatial grid and sampling rate so a  combination would likely create erroneous assumptions and taint the results of the analysis. But through the use of data fusion, all data and attributes are brought together into a single view in which a more complete picture of the environment is created. This enables scientists to identify key locations and times and form new insights into the interactions between the environment and animal behaviors.  In the figure at right, rock lobsters are studied off the coast of Tasmania. Dr. Hugh Pederson of the University of Tasmania used data fusion software to fuse southern rock lobster tracking data (color-coded for in yellow and black for day and night, respectively) with bathymetry and habitat data to create a unique 4D picture of rock lobster behavior. Data integration  In applications outside of the geospatial domain, differences in the usage of the terms Data integration and Data fusion apply. In areas such as business intelligence, for example, data integration is used to describe the combining of data, whereas data fusion is integration followed by reduction or replacement. Data integration might be viewed as set combination wherein the larger set is retained, whereas fusion is a set reduction technique with improved confidence. The JDL/DFIG model  In the mid-1980s, the Joint Directors of Laboratories formed the Data Fusion Subpanel (which later became known as the Data Fusion Group). With the advent of the World Wide Web, data fusion thus included data, sensor, and information fusion. The JDL/DFIG introduced a model of data fusion that divided the various processes. Currently, the six levels with the Data Fusion Information Group (DFIG) model are:  Level 0: Source Preprocessing/subject Assessment  Level 1: Object Assessment  Level 2: Situation Assessment  Level 3: Impact Assessment (or Threat Refinement)  Level 4: Process Refinement  Level 5: User Refinement (or Cognitive Refinement)  Although the JDL Model (Level 1-4) is still in use today, it is often criticized for its implication that the levels necessarily happen in order and also for its lack of adequate representation of the potential for a human-in-the-loop. The DFIG model (Level 0 - 5) explored the implications of situation awareness, user refinement, and mission management.[2] Despite these shortcomings, the JDL/DFIG models are useful for visualizing the data fusion process, facilitating discussion and common understanding,[3] and important for systems-level information fusion design.[2] Application areas \tThis article is in a list format that may be better presented using prose. You can help by converting this article to prose, if appropriate. Editing help is available. (February 2012) \tThis section includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. Please help to improve this section by introducing more precise citations. (February 2012) (Learn how and when to remove this template message)      Geospatial Information Systems     Soil Mapping     Business intelligence     Oceanography     Discovery science     Business performance management     Intelligent transport systems     Loyalty card     Cheminformatics         Quantitative structure-activity relationship     Bioinformatics     Intelligence services     Wireless sensor networks     Biometrics Position data fusion  The distance or position of an object can be measured with different sensors. By taking sensors based on different physical principles (magnetic, optical, mechanical) as well the resolution can be lowered as the bandwidth of measurement can be increased. Optimal filtering (in sense of minimizing some norm over a frequency) is a very effective tool used for combining sensor data in real-time. Applied methods with Matlab(TM) code and explanation can be found in the Master Thesis 'Sensor Fusion for Nanopositioning'.[4] Data fusion from multiple traffic sensing modalities  The data from the different sensing technologies can be combined in intelligent ways to determine the traffic state accurately. A Data fusion based approach that utilizes the road side collected acoustic, image and sensor data has been shown to combine the advantages of the different individual methods.[5] Decision Fusion  In many cases, geographically-dispersed sensors are severely energy- and bandwidth-limited. Therefore, the raw data concerning a certain phenomenon are often summarized in a few bits from each sensor. When inferring on a binary event (i.e., H 0 {\\displaystyle {\\mathcal {H}}_{0}} \\mathcal{H}_0 or H 1 {\\displaystyle {\\mathcal {H}}_{1}} {\\mathcal {H}}_{1} ), in the extreme case only binary decisions are sent from sensors to a Decision Fusion Center (DFC) and combined in order to obtain improved classification performance. [6][7][8] Data fusion for Enhanced Contextual Awareness  With a multitude of built-in sensors including motion sensor, environmental sensor, position sensor, a modern mobile device gives mobile applications access to a number of sensory data which could be leveraged to enhance the contextual awareness. Using signal processing and data fusion techniques such as feature generation, feasibility study and Principal Component Analysis (PCA) to analyze such sensory data will greatly improve the positive rate of classifying the motion and contextual relevant status of the device.[9] See also      Data integration     Data mungling     Information integration     Image fusion     Sensor Fusion     Integrative level   In computing and data management, data mapping is the process of creating data element mappings between two distinct data models. Data mapping is used as a first step for a wide variety of data integration tasks including:      Data transformation or data mediation between a data source and a destination     Identification of data relationships as part of data lineage analysis     Discovery of hidden sensitive data such as the last four digits of a social security number hidden in another user id as part of a data masking or de-identification project     Consolidation of multiple databases into a single data base and identifying redundant columns of data for consolidation or elimination  For example, a company that would like to transmit and receive purchases and invoices with other companies might use data mapping to create data maps from a company's data to standardized ANSI ASC X12 messages for items such as purchase orders and invoices.  Contents      1 Standards     2 Hand-coded, graphical manual     3 Data-driven mapping     4 Semantic mapping     5 See also     6 References     7 Bibliography     8 External links  Standards  X12 standards are generic Electronic Data Interchange (EDI) standards designed to allow a company to exchange data with any other company, regardless of industry. The standards are maintained by the Accredited Standards Committee X12 (ASC X12), with the American National Standards Institute (ANSI) accredited to set standards for EDI. The X12 standards are often called ANSI ASC X12 standards.  In the future, tools based on semantic web languages such as Resource Description Framework (RDF), the Web Ontology Language (OWL) and standardized metadata registry will make data mapping a more automatic process. This process will be accelerated if each application performed metadata publishing. Full automated data mapping is a very difficult problem (see Semantic translation). Hand-coded, graphical manual  Data mappings can be done in a variety of ways using procedural code, creating XSLT transforms or by using graphical mapping tools that automatically generate executable transformation programs. These are graphical tools that allow a user to \"draw\" lines from fields in one set of data to fields in another. Some graphical data mapping tools allow users to \"Auto-connect\" a source and a destination. This feature is dependent on the source and destination data element name being the same. Transformation programs are automatically created in SQL, XSLT, Java programming language or C++. These kinds of graphical tools are found in most ETL Tools (Extract, Transform, Load Tools) as the primary means of entering data maps to support data movement. Examples include SAP BODS and Informatica PowerCenter. Data-driven mapping  This is the newest approach in data mapping and involves simultaneously evaluating actual data values in two data sources using heuristics and statistics to automatically discover complex mappings between two data sets. This approach is used to find transformations between two data sets and will discover substrings, concatenations, arithmetic, case statements as well as other kinds of transformation logic. This approach also discovers data exceptions that do not follow the discovered transformation logic. Semantic mapping  Semantic mapping is similar to the auto-connect feature of data mappers with the exception that a metadata registry can be consulted to look up data element synonyms. For example, if the source system lists FirstName but the destination lists PersonGivenName, the mappings will still be made if these data elements are listed as synonyms in the metadata registry. Semantic mapping is only able to discover exact matches between columns of data and will not discover any transformation logic or exceptions between columns.  Data Lineage is a track of the life cycle of each piece of data as it is ingested, processed and output by the analytics system. This provides visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.[1] See also      Big structure     Bots open source software for data mapping     Data integration     Data wrangling     Identity transform     ISO/IEC 11179 - The ISO/IEC Metadata registry standard     Metadata     Metadata publishing     Schema matching     Semantic heterogeneity     Semantic mapper     Semantic translation     Semantic web     Semantics     XSLT - XML Transformation Language Core data integration is the use of data integration technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:      ETL (Extract, transform, load) implementations     EAI (Enterprise Application Integration) implementations     SOA (Service-Oriented Architecture) implementations     ESB (Enterprise Service Bus) implementations  Core data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.  Because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create edge data integration, using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. See also      data integration     edge data integration User centered approaches are well known in the visualization community (although not always implemented) [D'Amico et al. 2005, Munzner et al. 2009]. Jointly developing the visualizations themselves, however, is rather rare. As we have very good experience with co-creative techniques in design and innovation, we wanted to apply them to the domain of data visualization as well. For example, we tried to experiment with data sets during a day-long workshop with a larger group of stakeholders (a session we called the “data picnic” because everyone brought his/her data and tools). Visualization  For this paper, we focused on a pixel oriented technique [Keim 2000] to fullfill requirements such as visualization of raw data or a chronological view of data to preserve the course of events. We stack graphical representations for various parameters of a log line (such as IP, user name, request or message) so that we get small columns for each log line. Lining up these stacks produces a dense visual representation with distinct patterns. This is why we call it the Pixel Carpet. Other subgroups of our research group took different approaches that can be found at other places in this blog. Snapshot of the Pixel Carpet interface. Each \"multi pixel\" represents one log line, as it a appears at the bottom of the screen.Snapshot of the Pixel Carpet interface. Each “multi pixel” represents one log line, as it a appears at the bottom of the screen. Data and Code  Our data sources included an ssh log (~13.000 lines, unpublished for privacy reasons) and an Apache (web server) access log (~145.000 lines, unpublished), and ~4.500 lines (raw data available, including countries from ip2geo .csv | .json ).  We implemented our ideas in a demonstrator in plain HTML/JavaScript (demo online – caution, will heavily stress your CPU). It helped us iterate quickly and evaluate the idea at various stages, also with new stakeholders. While the code achieves what we need, we are also aware that computing performance is rather bad. If you want to take a look or even improve it, you can find it on github.  To bring it closer to a productive tool, we would turn the Pixel Carpet into a plugin for state-of-the-art data processing engines such as ElasticSearch/Kibana or splunk (scriptable with d3.js since version 6). Time Series Visualizations – An overview by Kim Albrecht\ton October 17, 2013, 1 comment  “Time-series — sets of values changing over time” A Tour Through the Visualization Zoo  http://hci.stanford.edu/jheer/files/zoo/  This description of the word “Time-Series” is very close to the explanation in Oxfords dictionary which adds that the word comes from a statistic background and often the intervals are equal within the time-series. http://www.oxforddictionaries.com/definition/english/time-series?q=time-series  Within our research project we are mainly interested in the visualization part within the vast field of statistics. In the book “The Visual Display of Quantitative Information” Edward Tufte defines time-series visualizations as:  “With one dimension marching along to the regular rhythm of seconds, minutes, hours, days, weeks, months, years, centuries, or millennia, the natural ordering of the time scale gives this design a strength and efficiency of interpretation found in no other graphic arrangement.”  Edward R. Tufte The Visual Display of Quantitative Information p. 28  Classical datasets of time series visualizations are temperature, wind, condensation (or any other kind of weather measurement), stock data, population change, electricity usage etc. the field is so vast that Tufte writes that in a study that analysed graphics between 1974 and 1980 75% of the graphics where time-series visualizations. Obviously more than 30 years later the field has changed but time-series still seams to be an important part within the area.  In my opinion most Security Network Data doesn’t provide information with changing values over time initially. For example Flow Data is structured through nodes and edges with additional information. These single incidents in time don’t hold the same characteristics as usual time-series datasets where one value changes. But on a certain level of abstraction (for example by counting incidents within set timeframes) or by combining time-series with other methods like network visualizations this kind of graphics could be very helpful for us.  This article first summarises a few classical time-series examples and than looks at recent developments in the field.  The first time-series visualization was designed in the tenth or possibly eleventh century. It shows the changing positions of the planets with the time on the x-axis.  As we will see the use of the x-axis is still the most common form of presenting time-series graphics. Nathan Yau gives an overview of the most common forms of time-series visualizations in his book “data points” which are in his opinion bar graphs, line charts, dot plots & dot-bar graphs. All of this charts are actually similar in what they do. The only difference is the graphical representation of the data. While all of them use the time dimension on the x-axis, Nathan Yau gives two examples for different representation methods. Radial plots, which are similar to line charts, just circular and calendar heat maps.  Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky from Stanford University are giving a different overview of time-series visualizations in their article “A Tour Through the Visualization Zoo”. Their overview starts with index charts, which is an interactive line chart. Index Chart  Stacked Graphs. Which are Area Charts that are stacked on top of each other. They are also called stream graphs. What makes them special is the fact that we get a visual summation of all time-series values.  The controversy around stacked graphs is very big. Alberto Cairo, graphics director at El Mundo Online wrote in a blog article that stacked graphs are “one of the worst graphics the New York Times have published – ever!” on the other hand the publisher of the first paper on stacked graphs wrote: “simplifying the user’s task of tracking individual themes through time by providing a continuous ‘flow’ from one time point to the next”. Furthermore, “we believe this metaphor is familiar and easy to understand and that it requires little cognitive effort to interpret the visualization” both points seam valid to me the cognitive effort needed in some contemporary visualizations is so high that it becomes hard to understand them without putting a lot of effort into them. Stacked Graphs are very simple to understand for the complexity they hold but the information output that can be generated from them is questionable. Andy Kirk from visualisingdata.com credits both sides very fairly in his blog article about the graphs with these comments:  “… a streamgraph is a fantastic solution to displaying large data sets to a mass audience.”  “The main problem facing static streamgraphs lies in the difficulty of reading data points formed by uncommon shapes.”  Tools: D3, Processing  Paper: ThemeRiver: Visualizing Theme Changes over Time,  Stacked Graphs – Geometry & Aesthetics  Example: The Ebb and Flow of Movies, How Different Groups Spend Their Day, Trace (this one is about visualizing wireless networks)    Stacked Graph  Small Multiples are multiple time-series graphs (what kind these graphs are is another question, in this case, area charts) arranged within a grid. Small multiples are more use full to understand different datasets on its own and not as a summary apposed to the stacked graphs. Small Multiples  The last example from the article are horizon graphs. These are actual also area charts which are mirrored and separated by occupacity. This is especially interesting in combination with small multiples because the “data density” is much higher than which classic area charts which leads to more information in a smaller space. An important factor when we are dealing with big datasets. Horizon Graph  There is some interesting research about the usefulness of horizon graphs that I recommend: Tool, Paper, Article     The list of graphics from the Stanford Group are much more contemporary than the examples from Nathan Yau, but still all of these examples use the same mechanism to visualize time-series data by using one axis as a dimension for time. This now more than 1.000 years old way to visualize time is helpful and very common but might not always be the best choice. As we know from scatter-plot visualizations our two space dimensions within a graphic are maybe the most powerful ones for pattern recognition and time might not be the main factor to identify these patterns. So what other ways are there to use time as a dimension within a visualization a part from space?  Animation: At least since Hans Roslings famous TED talks the usage of animation for displaying time is common and it seams to be the most obvious way to visualize time very literal though time. But the technique needs to be used with caution. Tamara Munzners visualization principles give a great insight on page 59 why visualizing time with animation is dangerous:  Principle: external cognition vs. internal memory      easy to compare by moving eyes between side-by-side views –harder to compare visible item to memory of what you saw  Implications for animation      great for choreographed storytelling     great for transitions between two states     poor for many states with changes everywhere  There is also a paper about the topic which gives more insights into the problem.  Small multiples: I already mentioned small multiples above but as I raised before the idea behind small multiples is more of a frame for visualizations than an actual kind of visualization. Like this we can also use each multiple as a timeframe. A beautiful example of small multiples with time as a dimension comes from the NYTimes Graphics department.  Binning time in bubbles: The idea here is to use bubble charts where the time dimension gets binned by minutes, days, years etc. into one bubble and compared to each other. In the Nasdaq 100 Index example each year is represented by one bubble.  Scatterplots: Scatterplots where time is displayed as connected points against two variables. This is similar to the animation idea. But in this case the animated dots leave behind a path behind. Also here the NYTimes has a good example.", "category": "Edison", "id": 134}
{"skillName": "DSDK06", "skillText": "Marketing research is \"the process or set of processes that links the producers, customers, and end users to the marketer through information  — information used to identify and define marketing opportunities and problems; generate, refine, and evaluate marketing actions; monitor marketing performance; and improve understanding of marketing as a process. Marketing research specifies the information required to address these issues, designs the method for collecting information, manages and implements the data collection process, analyzes the results, and communicates the findings and their implications.\"[1]  It is the systematic gathering, recording, and analysis of qualitative and quantitative data about issues relating to marketing products and services. The goal of marketing research is to identify and assess how changing elements of the marketing mix impacts customer behavior. The term is commonly interchanged with market research; however, expert practitioners may wish to draw a distinction, in that market research is concerned specifically with markets, while marketing research is concerned specifically about marketing processes.[2]  Marketing research is often partitioned into two sets of categorical pairs, either by target market:      Consumer marketing research, and     Business-to-business (B2B) marketing research  Or, alternatively, by methodological approach:      Qualitative marketing research, and     Quantitative marketing research  Consumer marketing research is a form of applied sociology that concentrates on understanding the preferences, attitudes, and behaviors of consumers in a market-based economy, and it aims to understand the effects and comparative success of marketing campaigns[citation needed]. The field of consumer marketing research as a statistical science was pioneered by Arthur Nielsen with the founding of the ACNielsen Company in 1923.[3]  Thus, marketing research may also be described as the systematic and objective identification, collection, analysis, and dissemination of information for the purpose of assisting management in decision making related to the identification and solution of problems and opportunities in marketing.[4]  Contents      1 Role     2 History     3 Characteristics     4 Related business research     5 Classification     6 Types     7 Methods     8 Business to business     9 Small businesses and nonprofits     10 International plan     11 Common terms     12 Careers     13 Corporate hierarchy     14 See also     15 Notes     16 References     17 External links  Role  The task of marketing research (MR) is to provide management with relevant, accurate, reliable, valid, and current market information. Competitive marketing environment and the ever-increasing costs attributed to poor decision making require that marketing research provide sound information. Sound decisions are not based on gut feeling, intuition, or even pure judgment.  Managers make numerous strategic and tactical decisions in the process of identifying and satisfying customer needs. They make decisions about potential opportunities, target market selection, market segmentation, planning and implementing marketing programs, marketing performance, and control. These decisions are complicated by interactions between the controllable marketing variables of product, pricing, promotion, and distribution. Further complications are added by uncontrollable environmental factors such as general economic conditions, technology, public policies and laws, political environment, competition, and social and cultural changes. Another factor in this mix is the complexity of consumers. Marketing research helps the marketing manager link the marketing variables with the environment and the consumers. It helps remove some of the uncertainty by providing relevant information about the marketing variables, environment, and consumers. In the absence of relevant information, consumers' response to marketing programs cannot be predicted reliably or accurately. Ongoing marketing research programs provide information on controllable and non-controllable factors and consumers; this information enhances the effectiveness of decisions made by marketing managers.[5]  Traditionally, marketing researchers were responsible for providing the relevant information and marketing decisions were made by the managers. However, the roles are changing and marketing researchers are becoming more involved in decision making, whereas marketing managers are becoming more involved with research. The role of marketing research in managerial decision making is explained further using the framework of the \"DECIDE\" model.   History  Marketing research has evolved in the decades since Arthur Nielsen established it as a viable industry, one that would grow hand-in-hand with the B2B and B2C economies. Markets naturally evolve, and since the birth of ACNielsen, when research was mainly conducted by in-person focus groups and pen-and-paper surveys, the rise of the Internet and the proliferation of corporate websites have changed the means by which research is executed.  Web analytics were born out of the need to track the behaviour of site visitors and, as the popularity of e-commerce and web advertising grew,businesses demanded details on the information created by new practices in web data collection, such as click-through and exit rates. As the Internet boomed,websites became larger and more complex and the possibility of two-way communication between businesses and their consumers became a reality. Provided with the capacity to interact with online customers, Researchers were able to collect large amounts of data that were previously unavailable, further propelling the Marketing Research Industry.  In the new millennium, as the Internet continued to develop and websites became more interactive, data collection and analysis became more commonplace for those Marketing Research Firms whose clients had a web presence. With the explosive growth of the online marketplace came new competition for companies; no longer were businesses merely competing with the shop down the road — competition was now represented by a global force. Retail outlets were appearing online and the previous need for bricks-and-mortar stores was diminishing at a greater pace than online competition was growing.With so many online channels for consumers to make purchases, companies needed newer and more compelling methods, in combination with messages that resonated more effectively, to capture the attention of the average consumer.  Having access to web data did not automatically provide companies with the rationale behind the behaviour of users visiting their sites,which provoked the marketing research industry to develop new and better ways of tracking, collecting and interpreting information. This led to the development of various tools like online focus groups and pop-up or website intercept surveys. These types of services allowed companies to dig deeper into the motivations of consumers, augmenting their insights and utilizing this data to drive market share.  As information around the world became more accessible, increased competition led companies to demand more of Market Researchers. It was no longer sufficient to follow trends in web behaviour or track sales data; companies now needed access to consumer behaviour throughout the entire purchase process.This meant the Marketing Research Industry, again, needed to adapt to the rapidly changing needs of the marketplace, and to the demands of companies looking fora competitive edge.  Today, Marketing Research has adapted to innovations in technology and the corresponding ease with which information is available. B2B and B2C companies are working hard to stay competitive and they now demand both quantitative (“What”) and qualitative (“Why?”) marketing research in order to better understand their target audience and the motivations behind customer behaviours.  This demand is driving Marketing Researchers to develop new platforms for interactive, two-way communication between their firms and consumers. Mobile devices such as SmartPhones are the best example of an emerging platform that enables businesses to connect with their customers throughout the entire buying process. Innovative research firms, such as OnResearch with their OnMobile app, are now providing businesses with the means to reach consumers from the point of initial investigation through to the decision and, ultimately, the purchase.  As personal mobile devices become more capable and widespread, the Marketing Research Industry will look to further capitalize on this trend. Mobile devices present the perfect channel for Research Firms to retrieve immediate impressions from buyers and to provide their clients with a holistic view of the consumers within their target markets, and beyond. Now, more than ever,innovation is the key to success for Marketing Researchers. Marketing Research Clients are beginning to demand highly personalized and specifically-focused products from the MR firms; big data is great for identifying general market segments, but is less capable of identifying key factors of niche markets, which now defines the competitive edge companies are looking for in this mobile-digital age. Characteristics  First, marketing research is systematic. Thus systematic planning is required at all the stages of the marketing research process. The procedures followed at each stage are methodologically sound, well documented, and, as much as possible, planned in advance. Marketing research uses the scientific method in that data are collected and analyzed to test prior notions or hypotheses. Experts in marketing research have shown that studies featuring multiple and often competing hypotheses yield more meaningful results than those featuring only one dominant hypothesis.[6]  Marketing research is objective. It attempts to provide accurate information that reflects a true state of affairs. It should be conducted impartially. While research is always influenced by the researcher's research philosophy, it should be free from the personal or political biases of the researcher or the management. Research which is motivated by personal or political gain involves a breach of professional standards. Such research is deliberately biased so as to result in predetermined findings. The objective nature of marketing research underscores the importance of ethical considerations. Also, researchers should always be objective with regard to the selection of information to be featured in reference texts because such literature should offer a comprehensive view on marketing. Research has shown, however, that many marketing textbooks do not feature important principles in marketing research.[7] Related business research  Other forms of business research include:      Market research is broader in scope and examines all aspects of a business environment. It asks questions about competitors, market structure, government regulations, economic trends, technological advances, and numerous other factors that make up the business environment (see environmental scanning). Sometimes the term refers more particularly to the financial analysis of companies, industries, or sectors. In this case, financial analysts usually carry out the research and provide the results to investment advisors and potential investors.     Product research — This looks at what products can be produced with available technology, and what new product innovations near-future technology can develop (see new product development).     Advertising research - is a specialized form of marketing research conducted to improve the efficacy of advertising. Copy testing, also known as \"pre-testing,\" is a form of customized research that predicts in-market performance of an ad before it airs, by analyzing audience levels of attention, brand linkage, motivation, entertainment, and communication, as well as breaking down the ad’s flow of attention and flow of emotion. Pre-testing is also used on ads still in rough (ripomatic or animatic) form. (Young, p. 213)  Classification  Organizations engage in marketing research for two reasons: (1) to identify and (2) solve marketing problems. This distinction serves as a basis for classifying marketing research into problem identification research and problem solving research.  Problem identification research is undertaken to help identify problems which are, perhaps, not apparent on the surface and yet exist or are likely to arise in the future like company image, market characteristics, sales analysis, short-range forecasting, long range forecasting, and business trends research. Research of this type provides information about the marketing environment and helps diagnose a problem. For example, The findings of problem solving research are used in making decisions which will solve specific marketing problems.  The Stanford Research Institute, on the other hand, conducts an annual survey of consumers that is used to classify persons into homogeneous groups for segmentation purposes. The National Purchase Diary panel (NPD) maintains the largest diary panel in the United States.  Standardized services are research studies conducted for different client firms but in a standard way. For example, procedures for measuring advertising effectiveness have been standardized so that the results can be compared across studies and evaluative norms can be established. The Starch Readership Survey is the most widely used service for evaluating print advertisements; another well-known service is the Gallup and Robinson Magazine Impact Studies. These services are also sold on a syndicated basis.      Customized services offer a wide variety of marketing research services customized to suit a client's specific needs. Each marketing research project is treated uniquely.     Limited-service suppliers specialize in one or a few phases of the marketing research project. Services offered by such suppliers are classified as field services, coding and data entry, data analysis, analytical services, and branded products. Field services collect data through the internet, traditional mail, in-person, or telephone interviewing, and firms that specialize in interviewing are called field service organizations. These organizations may range from small proprietary organizations which operate locally to large multinational organizations with WATS line interviewing facilities. Some organizations maintain extensive interviewing facilities across the country for interviewing shoppers in malls.     Coding and data entry services include editing completed questionnaires, developing a coding scheme, and transcribing the data on to diskettes or magnetic tapes for input into the computer. NRC Data Systems provides such services.     Analytical services include designing and pretesting questionnaires, determining the best means of collecting data, designing sampling plans, and other aspects of the research design. Some complex marketing research projects require knowledge of sophisticated procedures, including specialized experimental designs, and analytical techniques such as conjoint analysis and multidimensional scaling. This kind of expertise can be obtained from firms and consultants specializing in analytical services.     Data analysis services are offered by firms, also known as tab houses, that specialize in computer analysis of quantitative data such as those obtained in large surveys. Initially most data analysis firms supplied only tabulations (frequency counts) and cross tabulations (frequency counts that describe two or more variables simultaneously). With the proliferation of software, many firms now have the capability to analyze their own data, but, data analysis firms are still in demand.[citation needed]     Branded marketing research products and services are specialized data collection and analysis procedures developed to address specific types of marketing research problems. These procedures are patented, given brand names, and marketed like any other branded product.  Types  Marketing research techniques come in many forms, including:      Ad Tracking – periodic or continuous in-market research to monitor a brand’s performance using measures such as brand awareness, brand preference, and product usage. (Young, 2005)     Advertising Research – used to predict copy testing or track the efficacy of advertisements for any medium, measured by the ad’s ability to get attention (measured with AttentionTracking), communicate the message, build the brand’s image, and motivate the consumer to purchase the product or service. (Young, 2005)     Brand equity research — how favorably do consumers view the brand?     Brand association research — what do consumers associate with the brand?     Brand attribute research — what are the key traits that describe the brand promise?     Brand name testing - what do consumers feel about the names of the products?     Commercial eye tracking research — examine advertisements, package designs, websites, etc. by analyzing visual behavior of the consumer     Concept testing - to test the acceptance of a concept by target consumers     Coolhunting - to make observations and predictions in changes of new or existing cultural trends in areas such as fashion, music, films, television, youth culture and lifestyle     Buyer decision making process research — to determine what motivates people to buy and what decision-making process they use; over the last decade, Neuromarketing emerged from the convergence of neuroscience and marketing, aiming to understand consumer decision making process     Copy testing – predicts in-market performance of an ad before it airs by analyzing audience levels of attention, brand linkage, motivation, entertainment, and communication, as well as breaking down the ad’s flow of attention and flow of emotion. (Young, p 213)     Customer satisfaction research - quantitative or qualitative studies that yields an understanding of a customer's satisfaction with a transaction     Demand estimation — to determine the approximate level of demand for the product     Distribution channel audits — to assess distributors’ and retailers’ attitudes toward a product, brand, or company     Internet strategic intelligence — searching for customer opinions in the Internet: chats, forums, web pages, blogs... where people express freely about their experiences with products, becoming strong opinion formers.     Marketing effectiveness and analytics — Building models and measuring results to determine the effectiveness of individual marketing activities.     Mystery consumer or mystery shopping - An employee or representative of the market research firm anonymously contacts a salesperson and indicates he or she is shopping for a product. The shopper then records the entire experience. This method is often used for quality control or for researching competitors' products.     Positioning research — how does the target market see the brand relative to competitors? - what does the brand stand for?     Price elasticity testing — to determine how sensitive customers are to price changes     Sales forecasting — to determine the expected level of sales given the level of demand. With respect to other factors like Advertising expenditure, sales promotion etc.     Segmentation research - to determine the demographic, psychographic, cultural, and behavioural characteristics of potential buyers     Online panel - a group of individual who accepted to respond to marketing research online     Store audit — to measure the sales of a product or product line at a statistically selected store sample in order to determine market share, or to determine whether a retail store provides adequate service     Test marketing — a small-scale product launch used to determine the likely acceptance of the product when it is introduced into a wider market     Viral Marketing Research - refers to marketing research designed to estimate the probability that specific communications will be transmitted throughout an individual's Social Network. Estimates of Social Networking Potential (SNP) are combined with estimates of selling effectiveness to estimate ROI on specific combinations of messages and media.  All of these forms of marketing research can be classified as either problem-identification research or as problem-solving research.  There are two main sources of data — primary and secondary. Primary research is conducted from scratch. It is original and collected to solve the problem in hand. Secondary research already exists since it has been collected for other purposes. It is conducted on data published previously and usually by someone else. Secondary research costs far less than primary research, but seldom comes in a form that exactly meets the needs of the researcher.  A similar distinction exists between exploratory research and conclusive research. Exploratory research provides insights into and comprehension of an issue or situation. It should draw definitive conclusions only with extreme caution. Conclusive research draws conclusions: the results of the study can be generalized to the whole population.  Exploratory research is conducted to explore a problem to get some basic idea about the solution at the preliminary stages of research. It may serve as the input to conclusive research. Exploratory research information is collected by focus group interviews, reviewing literature or books, discussing with experts, etc. This is unstructured and qualitative in nature. If a secondary source of data is unable to serve the purpose, a convenience sample of small size can be collected. Conclusive research is conducted to draw some conclusion about the problem. It is essentially, structured and quantitative research, and the output of this research is the input to management information systems (MIS).  Exploratory research is also conducted to simplify the findings of the conclusive or descriptive research, if the findings are very hard to interpret for the marketing managers. Methods  Methodologically, marketing research uses the following types of research designs:[8]  Based on questioning          Qualitative marketing research - generally used for exploratory purposes — small number of respondents — not generalizable to the whole population — statistical significance and confidence not calculated — examples include focus groups, in-depth interviews, and projective techniques         Quantitative marketing research - generally used to draw conclusions — tests a specific hypothesis - uses random sampling techniques so as to infer from the sample to the population — involves a large number of respondents — examples include surveys and questionnaires. Techniques include choice modelling, maximum difference preference scaling, and covariance analysis.  Based on observations          Ethnographic studies — by nature qualitative, the researcher observes social phenomena in their natural setting — observations can occur cross-sectionally (observations made at one time) or longitudinally (observations occur over several time-periods) - examples include product-use analysis and computer cookie traces. See also Ethnography and Observational techniques.         Experimental techniques - by nature quantitative, the researcher creates a quasi-artificial environment to try to control spurious factors, then manipulates at least one of the variables — examples include purchase laboratories and test markets  Researchers often use more than one research design. They may start with secondary research to get background information, then conduct a focus group (qualitative research design) to explore the issues. Finally they might do a full nationwide survey (quantitative research design) in order to devise specific recommendations for the client. Business to business  Business to business (B2B) research is inevitably more complicated than consumer research. The researchers need to know what type of multi-faceted approach will answer the objectives, since seldom is it possible to find the answers using just one method. Finding the right respondents is crucial in B2B research since they are often busy, and may not want to participate. Encouraging them to “open up” is yet another skill required of the B2B researcher. Last, but not least, most business research leads to strategic decisions and this means that the business researcher must have expertise in developing strategies that are strongly rooted in the research findings and acceptable to the client.  There are four key factors that make B2B market research special and different from consumer markets:[9]      The decision making unit is far more complex in B2B markets than in consumer markets     B2B products and their applications are more complex than consumer products     B2B marketers address a much smaller number of customers who are very much larger in their consumption of products than is the case in consumer markets     Personal relationships are of critical importance in B2B markets.  Small businesses and nonprofits \tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (April 2012) (Learn how and when to remove this template message)  Marketing research does not only occur in huge corporations with many employees and a large budget. Marketing information can be derived by observing the environment of their location and the competitions location. Small scale surveys and focus groups are low cost ways to gather information from potential and existing customers. Most secondary data (statistics, demographics, etc.) is available to the public in libraries or on the internet and can be easily accessed by a small business owner.  Below are some steps that could be done by SME (Small Medium Entreprise) to analyze the market:      Provide secondary and or primary data (if necessary);     Analyze Macro & Micro Economic data (e.g. Supply & Demand, GDP,Price change, Economic growth, Sales by sector/industries,interest rate, number of investment/ divestment, I/O, CPI, Social anlysis,etc.);     Implement the marketing mix concept, which is consist of: Place, Price, Product,Promotion, People, Process, Physical Evidence and also Political & social situation to analyze global market situation);     Analyze market trends, growth, market size, market share, market competition (e.g. SWOT analysis, B/C Analysis,channel mapping identities of key channels, drivers of customers loyalty and satisfaction, brand perception, satisfaction levels, current competitor-channel relationship analysis, etc.),etc.;     Determine market segment, market target, market forecast and market position;     Formulating market strategy & also investigating the possibility of partnership/ collaboration (e.g. Profiling & SWOT analysis of potential partners, evaluating business partnership.)     Combine those analysis with the SME's business plan/ business model analysis (e.g. Business description, Business process, Business strategy, Revenue model, Business expansion, Return of Investment, Financial analysis (Company History, Financial assumption, Cost/Benefit Analysis, Projected profit & Loss, Cashflow, Balance sheet & business Ratio,etc.).      Note as important : Overall analysis should be based on 6W+1H (What, When, Where, Which, Who, Why and How) question.  International plan  International Marketing Research follows the same path as domestic research, but there are a few more problems that may arise. Customers in international markets may have very different customs, cultures, and expectations from the same company. In this case, Marketing Research relies more on primary data rather than secondary information. Gathering the primary data can be hindered by language, literacy and access to technology. Basic Cultural and Market intelligence information will be needed to maximize the research effectiveness. Some of the steps that would help overcoming barriers include; 1. Collect secondary information on the country under study from reliable international source e.g. WHO and IMF 2. Collect secondary information on the product/service under study from available sources 3. Collect secondary information on product manufacturers and service providers under study in relevant country 4. Collect secondary information on culture and common business practices 5. Ask questions to get better understanding of reasons behind any recommendations for a specific methodology Common terms  Market research techniques resemble those used in political polling and social science research. Meta-analysis (also called the Schmidt-Hunter technique) refers to a statistical method of combining data from multiple studies or from several types of studies. Conceptualization means the process of converting vague mental images into definable concepts. Operationalization is the process of converting concepts into specific observable behaviors that a researcher can measure. Precision refers to the exactness of any given measure. Reliability refers to the likelihood that a given operationalized construct will yield the same results if re-measured. Validity refers to the extent to which a measure provides data that captures the meaning of the operationalized construct as defined in the study. It asks, “Are we measuring what we intended to measure?”      Applied research sets out to prove a specific hypothesis of value to the clients paying for the research. For example, a cigarette company might commission research that attempts to show that cigarettes are good for one's health. Many researchers have ethical misgivings about doing applied research.     Sugging (from SUG, for \"selling under the guise\" of market research) forms a sales technique in which sales people pretend to conduct marketing research, but with the real purpose of obtaining buyer motivation and buyer decision-making information to be used in a subsequent sales call.     Frugging comprises the practice of soliciting funds under the pretense of being a research organization.  Careers  Some of the positions available in marketing research include vice president of marketing research, research director, assistant director of research, project manager, field work director, statistician/data processing specialist, senior analyst, analyst, junior analyst and operational supervisor.[10]  The most common entry-level position in marketing research for people with bachelor's degrees (e.g., BBA) is as operational supervisor. These people are responsible for supervising a well-defined set of operations, including field work, data editing, and coding, and may be involved in programming and data analysis. Another entry-level position for BBAs is assistant project manager. An assistant project manager will learn and assist in questionnaire design, review field instructions, and monitor timing and costs of studies. In the marketing research industry, however, there is a growing preference for people with master's degrees. Those with MBA or equivalent degrees are likely to be employed as project managers.[10]  A small number of business schools also offer a more specialized Master of Marketing Research (MMR) degree. An MMR typically prepares students for a wide range of research methodologies and focuses on learning both in the classroom and the field.  The typical entry-level position in a business firm would be junior research analyst (for BBAs) or research analyst (for MBAs or MMRs). The junior analyst and the research analyst learn about the particular industry and receive training from a senior staff member, usually the marketing research manager. The junior analyst position includes a training program to prepare individuals for the responsibilities of a research analyst, including coordinating with the marketing department and sales force to develop goals for product exposure. The research analyst responsibilities include checking all data for accuracy, comparing and contrasting new research with established norms, and analyzing primary and secondary data for the purpose of market forecasting.  As these job titles indicate, people with a variety of backgrounds and skills are needed in marketing research. Technical specialists such as statisticians obviously need strong backgrounds in statistics and data analysis. Other positions, such as research director, call for managing the work of others and require more general skills. To prepare for a career in marketing research, students usually:      Take all the marketing courses.     Take courses in statistics and quantitative methods.     Acquire computer skills.     Take courses in psychology and consumer behavior.     Acquire effective written and verbal communication skills.     Think creatively.[10]  Corporate hierarchy      Vice-President of Marketing Research: This is the senior position in marketing research. The VP is responsible for the entire marketing research operation of the company and serves on the top management team. Sets the objectives and goals of the marketing research department.     Research Director: Also a senior position, the director has the overall responsibility for the development and execution of all the marketing research projects.     Assistant Director of Research: Serves as an administrative assistant to the director and supervises some of the other marketing research staff members.     (Senior) Project Manager: Has overall responsibility for design, implementation, and management of research projects.     Statistician/Data Processing Specialist: Serves as an expert on theory and application of statistical techniques. Responsibilities include experimental design, data processing, and analysis.     Senior Analyst: Participates in the development of projects and directs the operational execution of the assigned projects. Works closely with the analyst, junior analyst, and other personnel in developing the research design and data collection. Prepares the final report. The primary responsibility for meeting time and cost constraints rests with the senior analyst.     Analyst: Handles the details involved in executing the project. Designs and pretests the questionnaires and conducts a preliminary analysis of the data.     Junior Analyst: Handles routine assignments such as secondary data analysis, editing and coding of questionnaires, and simple statistical analysis.     Field Work Director: Responsible for the selection, training, supervision, and evaluation of interviewers and other field workers.[11]  See also      Ad Tracking     A/B testing     Advertising Research     AttentionTracking     Commercial eye tracking     Copy testing     Experimental techniques     Enterprise Feedback Management (EFM)     Global Marketing     Industry or market research     Integrated Marketing Communications     Journal of Marketing Research     Knowledge management     List of marketing research firms     Marketing     Marketing Research Association     Marketing research mix     Marketing research process     Master of Marketing Research     Observational techniques     Propaganda     Quantitative marketing research     Qualitative marketing research       Marketing management is the organizational discipline which focuses on the practical application of marketing orientation, techniques and methods inside enterprises and organizations and on the management of a firm's marketing resources and activities.  Globalization has led some firms to market beyond the borders of their home countries, making international marketing a part of those firms' marketing strategy.[1] Marketing managers are often responsible for influencing the level, timing, and composition of customer demand. In part, this is because the role of a marketing manager (or sometimes called managing marketer in small- and medium-sized enterprises) can vary significantly based on a business's size, corporate culture, and industry context. For example, in a small- and medium-sized enterprises, the managing marketer may contribute in both managerial and marketing operations roles for the company brands. In a large consumer products company, the marketing manager may act as the overall general manager of his or her assigned product.[2] To create an effective, cost-efficient marketing management strategy, firms must possess a detailed, objective understanding of their own business and the market in which they operate.[3] In analyzing these issues, the discipline of marketing management often overlaps with the related discipline of strategic planning.  Contents      1 Structure         1.1 Marketing strategy         1.2 Implementation planning         1.3 Project, process, and vendor management         1.4 Reporting, measurement, feedback and control systems     2 See also     3 References     4 Further reading     5 External links  Structure  Marketing management employs various tools from economics and competitive strategy to analyze the industry context in which the firm operates. These include Porter's five forces, analysis of strategic groups of competitors, value chain analysis and others.[4] Depending on the industry, the regulatory context may also be important to examine in detail.  In competitor analysis, marketers build detailed profiles of each competitor in the market, focusing especially on their relative competitive strengths and weaknesses using SWOT analysis. Marketing managers will examine each competitor's cost structure, sources of profits, resources and competencies, competitive positioning and product differentiation, degree of vertical integration, historical responses to industry developments, and other factors.  Marketing management often finds it necessary to invest in research to collect the data required to perform accurate marketing analysis. As such, they often conduct market research and marketing research to obtain this information. Marketers employ a variety of techniques to conduct market research, but some of the more common include:      Qualitative marketing research, such as focus groups and various types of interviews     Quantitative marketing research, such as statistical surveys     Experimental techniques such as test markets     Observational techniques such as ethnographic (on-site) observation  Marketing managers may also design and oversee various environmental scanning and competitive intelligence processes to help identify trends and inform the company's marketing analysis.  A brand audit is a thorough examination of a brand's current position in an industry compared to its competitors and the examination of its effectiveness. When it comes to brand auditing, five questions should be carefully examined and assessed. These five questions are how well the business’ current brand strategy is working, what are the company's established resource strengths and weaknesses, what are its external opportunities and threats, how competitive are the business’ prices and costs, how strong is the business’ competitive position in comparison to its competitors, and what strategic issues are facing the business.  Generally, when a business is conducting a brand audit, the main goal is to uncover business’ resource strengths, deficiencies, best market opportunities, outside threats, future profitability, and its competitive standing in comparison to existing competitors. A brand audit establishes the strategic elements needed to improve brand position and competitive capabilities within the industry. Once a brand is audited, any business that ends up with a strong financial performance and market position is more likely than not to have a properly conceived and effectively executed brand strategy.  A brand audit examines whether a business’ share of the market is increasing, decreasing, or stable. It determines if the company’s margin of profit is improving, decreasing, and how much it is in comparison to the profit margin of established competitors. Additionally, a brand audit investigates trends in a business’ net profits, the return on existing investments, and its established economic value. It determines whether or not the business’ entire financial strength and credit rating is improving or getting worse. This kind of audit also assesses a business’ image and reputation with its customers. Furthermore, a brand audit seeks to determine whether or not a business is perceived as an industry leader in technology, offering product or service innovations, along with exceptional customer service, among other relevant issues that customers use to decide on a brand of preference.  A brand audit usually focuses on a business’ strengths and resource capabilities because these are the elements that enhance its competitiveness. A business’ competitive strengths can exist in several forms. Some of these forms include skilled or pertinent expertise, valuable physical assets, valuable human assets, valuable organizational assets, valuable intangible assets, competitive capabilities, achievements and attributes that position the business into a competitive advantage, and alliances or cooperative ventures.  The basic concept of a brand audit is to determine whether a business’ resource strengths are competitive assets or competitive liabilities. This type of audit seeks to ensure that a business maintains a distinctive competence that allows it to build and reinforce its competitive advantage. What’s more, a successful brand audit seeks to establish what a business capitalizes on best, its level of expertise, resource strengths, and strongest competitive capabilities, while aiming to identify a business’ position and future performance. Marketing strategy Main article: Marketing strategy  Two customer segments are often selected as targets because they score highly on two dimensions:      The segment is attractive to serve because it is large, growing, makes frequent purchases, is not price sensitive (i.e. is willing to pay high prices), or other factors; and     The company has the resources and capabilities to compete for the segment's business, can meet their needs better than the competition, and can do so profitably.[3]  A commonly cited definition of marketing is simply \"meeting needs profitably\".[5]  The implication of selecting target segments is that the business will subsequently allocate more resources to acquire and retain customers in the target segment(s) than it will for other, non-targeted customers. In some cases, the firm may go so far as to turn away customers who are not in its target segment.The doorman at a swanky nightclub, for example, may deny entry to unfashionably dressed individuals because the business has made a strategic decision to target the \"high fashion\" segment of nightclub patrons.  In conjunction with targeting decisions, marketing managers will identify the desired positioning they want the company, product, or brand to occupy in the target customer's mind. This positioning is often an encapsulation of a key benefit the company's product or service offers that is differentiated and superior to the benefits offered by competitive products.[6] For example, Volvo has traditionally positioned its products in the automobile market in North America in order to be perceived as the leader in \"safety\", whereas BMW has traditionally positioned its brand to be perceived as the leader in \"performance\".  Ideally, a firm's positioning can be maintained over a long period of time because the company possesses, or can develop, some form of sustainable competitive advantage.[7] The positioning should also be sufficiently relevant to the target segment such that it will drive the purchasing behavior of target customers.[6] To sum up,the marketing branch of a company is to deal with the selling and popularity of its products among people and its customers,as the central and eventual goal of a company is customer satisfaction and the return of revenue. Implementation planning Main article: Marketing plan The Marketing Metrics Continuum provides a framework for how to categorize metrics from the tactical to strategic.  If the company has obtained an adequate understanding of the customer base and its own competitive position in the industry, marketing managers are able to make their own key strategic decisions and develop a marketing strategy designed to maximize the revenues and profits of the firm. The selected strategy may aim for any of a variety of specific objectives, including optimizing short-term unit margins, revenue growth, market share, long-term profitability, or other goals.  After the firm's strategic objectives have been identified, the target market selected, and the desired positioning for the company, product or brand has been determined, marketing managers focus on how to best implement the chosen strategy. Traditionally, this has involved implementation planning across the \"4 Ps\" of : product management, pricing (at what price slot does a producer position a product, e.g. low, medium or high price), place (the place or area where the products are going to be sold, which could be local, regional, countrywide or international) (i.e. sales and distribution channels), and Promotion.  Taken together, the company's implementation choices across the 4 Ps are often described as the marketing mix, meaning the mix of elements the business will employ to \"go to market\" and execute the marketing strategy. The overall goal for the marketing mix is to consistently deliver a compelling value proposition that reinforces the firm's chosen positioning, builds customer loyalty and brand equity among target customers, and achieves the firm's marketing and financial objectives.  In many cases, marketing management will develop a marketing plan to specify how the company will execute the chosen strategy and achieve the business' objectives. The content of marketing plans varies from firm to firm, but commonly includes:      An executive summary     Situation analysis to summarize facts and insights gained from market research and marketing analysis     The company's mission statement or long-term strategic vision     A statement of the company's key objectives, often subdivided into marketing objectives and financial objectives     The marketing strategy the business has chosen, specifying the target segments to be pursued and the competitive positioning to be achieved     Implementation choices for each element of the marketing mix (the 4 Ps)  Project, process, and vendor management  More broadly, marketing managers work to design and improve the effectiveness of core marketing processes, such as new product development, brand management, marketing communications, and pricing. Marketers may employ the tools of business process reengineering to ensure these processes are properly designed, and use a variety of process management techniques to keep them operating smoothly.  Effective execution may require management of both internal resources and a variety of external vendors and service providers, such as the firm's advertising agency. Marketers may therefore coordinate with the company's Purchasing department on the procurement of these services. Under the area of marketing agency management (i.e. working with external marketing agencies and suppliers) are techniques such as agency performance evaluation, scope of work, incentive compensation, RFx's and storage of agency information in a supplier database. Database is a critical thing to manage, but easy to allocate. While vendor allocation having complications to resolve but easy to handle. Reporting, measurement, feedback and control systems  Marketing management employs a variety of metrics to measure progress against objectives. It is the responsibility of marketing managers – in the marketing department or elsewhere – to ensure that the execution of marketing programs achieves the desired objectives and does so in a cost-efficient manner.  Marketing management therefore often makes use of various organizational control systems, such as sales forecasts, sales force and reseller incentive programs, sales force management systems, and customer relationship management tools (CRM). Some software vendors have begun using the term marketing operations management or marketing resource management to describe systems that facilitate an integrated approach for controlling marketing resources. In some cases, these efforts may be linked to various supply chain management systems, such as enterprise resource planning (ERP), material requirements planning (MRP), efficient consumer response (ECR), and inventory management systems. See also      Enterprise marketing management     Marketing effectiveness     Marketing performance measurement and management     Marketing resource management     Predictive analytics     Strategic management", "category": "Edison", "id": 135}
{"skillName": "DSDM01", "skillText": "Data management comprises all the disciplines related to managing data as a valuable resource.  Contents      1 Overview     2 Corporate Data Quality Management     3 Topics in Data Management     4 Body of Knowledge     5 Usage     6 Integrated data management     7 See also     8 References     9 External links  Overview  The official definition provided by DAMA International, the professional organization for those in the data management profession, is: \"Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise.\" This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as relational database management. The data lifecycle  Alternatively, the definition provided in the DAMA Data Management Body of Knowledge ([1]) is: \"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets.\"[2]  The concept of \"Data Management\" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to random access processing. Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that \"Data Management\" was more important than \"Process Management\" used arguments such as \"a customer's home address is stored in 75 (or some other large number) places in our computer systems.\" During this period, random access processing was not competitively fast, so those suggesting \"Process Management\" was more important than \"Data Management\" used batch processing time as their primary argument. As applications moved into real-time, interactive applications, it became obvious to most practitioners that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs. Corporate Data Quality Management  Corporate Data Quality Management (CDQM) is, according to the European Foundation for Quality Management and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.[3]      Strategy for Corporate Data Quality: As CDQM is affected by various business drivers and requires involvement of multiple divisions in an organization; it must be considered a company-wide endeavor.     Corporate Data Quality Controlling: Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.     Corporate Data Quality Organization: CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.     Corporate Data Quality Processes and Methods: In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.     Data Architecture for Corporate Data Quality: The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.     Applications for Corporate Data Quality: Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.  Topics in Data Management  Topics in Data Management, grouped by the DAMA DMBOK Framework,[4] include:      Data governance         Data asset         Data governance         Data steward     Data Architecture, Analysis and Design         Data analysis         Data architecture         Data modeling     Database Management         Data maintenance         Database administration         Database management system     Data Security Management         Data access         Data erasure         Data privacy         Data security     Data Quality Management         Data cleansing         Data integrity         Data enrichment         Data quality         Data quality assurance     Reference and Master Data Management         Data integration         Master data management         Reference data     Data Warehousing and Business Intelligence Management         Business intelligence         Data mart         Data mining         Data movement (Extract, transform, load )         Data warehouse     Document, Record and Content Management         Document management system         Records management     Meta Data Management         Meta-data management         Metadata         Metadata discovery         Metadata publishing         Metadata registry     Contact Data Management         Business continuity planning         Marketing operations         Customer data integration         Identity management         Identity theft         Data theft         ERP software         CRM software         Address (geography)         Postal code         Email address         Telephone number  Body of Knowledge  The DAMA Guide to the Data Management Body of Knowledge\" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009. Usage  In modern management usage, one can easily discern a trend away from the term \"data\" in composite expressions to the term \"information\" or even \"knowledge\" when talking in a non-technical context. Thus there exists not only data management, but also information management and knowledge management. This is a misleading trend as it obscures that traditional data are managed or somehow processed on second looks.[citation needed] The distinction between data and derived values can be seen in the information ladder.[citation needed] While data can exist as such, \"information\" and \"knowledge\" are always in the \"eye\" (or rather the brain) of the beholder and can only be measured in relative units.  Several organisations have established a data management centre (DMC)[5] for their operations. Integrated data management  Integrated data management (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its lifetime.[6][7][8][9] IDM's purpose is to:      Produce enterprise-ready applications faster     Improve data access, speed iterative testing     Empower collaboration between architects, developers and DBAs     Consistently achieve service level targets     Automate and simplify operations     Provide contextual intelligence across the solution stack     Support business growth     Accommodate new initiatives without expanding infrastructure     Simplify application upgrades, consolidation and retirement     Facilitate alignment, consistency and governance     Define business policies and standards up front; share, extend, and apply throughout the lifecycle  See also      Open data     Information architecture     Information management     Enterprise architecture     Information design     Information system     Controlled vocabulary     Data curation     Data retention     Data governance     Data quality     Data modeling     Data management plan     Information lifecycle management     Computer data storage     Data proliferation     Digital preservation     Digital perpetuation     Document management     Enterprise content management     Hierarchical storage management     Information repository     Records management     System integration   Data curation is a term used to indicate management activities related to organization and integration of data collected from various sources, annotation of the data, and publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes \"all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data\".[1] In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.[2] The term is also used in the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation.[3] In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component.[4]  Contents      1 Definition and practice     2 Projects and studies     3 See also     4 References     5 External links  Definition and practice  According to the University of Illinois' Graduate School of Library and Information Science, \"Data curation is the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education; curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time.\"[5]  Deep background on data libraries appeared in a 1982 issue of the Illinois journal, Library Trends.[6] For historical background on the data archive movement, see \"Social Scientific Information Needs for Numeric Data: The Evolution of the International Data Archive Infrastructure.\"[7]  This term is sometimes used in context of biological databases, where specific biological information is firstly obtained from a range of research articles and then stored within a specific category of database. For instance, information about anti-depressant drugs can be obtained from various sources and, after checking whether they are available as a database or not, they are saved under a drug's database's anti-depressive category. Enterprises are also utilizing data curation within their operational and strategic processes to ensure data quality and accuracy.[8] Projects and studies  The Dissemination Information Packages (DIPS) for Information Reuse (DIPIR) project is studying research data produced and used by quantitative social scientists, archaeologists, and zoologists. The intended audience is researchers who use secondary data and the digital curators, digital repository managers, data center staff, and others who collect, manage, and store digital information.[9] See also      Biocurator     Data archaeology     Data degradation     Data format management     Data governance     Data management     Data stewardship     Data wrangling, low-level activities to parse and reformat data     Informationist, an individual with extensive industry expertise, acute familiarity with organizational structures and processes, deep domain level information mastery and information systems technical savvy       Data management comprises all the disciplines related to managing data as a valuable resource.  Contents      1 Overview     2 Corporate Data Quality Management     3 Topics in Data Management     4 Body of Knowledge     5 Usage     6 Integrated data management     7 See also     8 References     9 External links  Overview  The official definition provided by DAMA International, the professional organization for those in the data management profession, is: \"Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise.\" This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as relational database management. The data lifecycle  Alternatively, the definition provided in the DAMA Data Management Body of Knowledge ([1]) is: \"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets.\"[2]  The concept of \"Data Management\" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to random access processing. Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that \"Data Management\" was more important than \"Process Management\" used arguments such as \"a customer's home address is stored in 75 (or some other large number) places in our computer systems.\" During this period, random access processing was not competitively fast, so those suggesting \"Process Management\" was more important than \"Data Management\" used batch processing time as their primary argument. As applications moved into real-time, interactive applications, it became obvious to most practitioners that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs. Corporate Data Quality Management  Corporate Data Quality Management (CDQM) is, according to the European Foundation for Quality Management and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.[3]      Strategy for Corporate Data Quality: As CDQM is affected by various business drivers and requires involvement of multiple divisions in an organization; it must be considered a company-wide endeavor.     Corporate Data Quality Controlling: Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.     Corporate Data Quality Organization: CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.     Corporate Data Quality Processes and Methods: In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.     Data Architecture for Corporate Data Quality: The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.     Applications for Corporate Data Quality: Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.  Topics in Data Management  Topics in Data Management, grouped by the DAMA DMBOK Framework,[4] include:      Data governance         Data asset         Data governance         Data steward     Data Architecture, Analysis and Design         Data analysis         Data architecture         Data modeling     Database Management         Data maintenance         Database administration         Database management system     Data Security Management         Data access         Data erasure         Data privacy         Data security     Data Quality Management         Data cleansing         Data integrity         Data enrichment         Data quality         Data quality assurance     Reference and Master Data Management         Data integration         Master data management         Reference data     Data Warehousing and Business Intelligence Management         Business intelligence         Data mart         Data mining         Data movement (Extract, transform, load )         Data warehouse     Document, Record and Content Management         Document management system         Records management     Meta Data Management         Meta-data management         Metadata         Metadata discovery         Metadata publishing         Metadata registry     Contact Data Management         Business continuity planning         Marketing operations         Customer data integration         Identity management         Identity theft         Data theft         ERP software         CRM software         Address (geography)         Postal code         Email address         Telephone number  Body of Knowledge  The DAMA Guide to the Data Management Body of Knowledge\" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009. Usage  In modern management usage, one can easily discern a trend away from the term \"data\" in composite expressions to the term \"information\" or even \"knowledge\" when talking in a non-technical context. Thus there exists not only data management, but also information management and knowledge management. This is a misleading trend as it obscures that traditional data are managed or somehow processed on second looks.[citation needed] The distinction between data and derived values can be seen in the information ladder.[citation needed] While data can exist as such, \"information\" and \"knowledge\" are always in the \"eye\" (or rather the brain) of the beholder and can only be measured in relative units.  Several organisations have established a data management centre (DMC)[5] for their operations. Integrated data management  Integrated data management (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its lifetime.[6][7][8][9] IDM's purpose is to:      Produce enterprise-ready applications faster     Improve data access, speed iterative testing     Empower collaboration between architects, developers and DBAs     Consistently achieve service level targets     Automate and simplify operations     Provide contextual intelligence across the solution stack     Support business growth     Accommodate new initiatives without expanding infrastructure     Simplify application upgrades, consolidation and retirement     Facilitate alignment, consistency and governance     Define business policies and standards up front; share, extend, and apply throughout the lifecycle     Data Management and Curation platform Data modelling and related technologies  ETL, OLAP, OLTP Data warehouses platform and related tools  In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered as a core component of business intelligence[1] environment. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.  The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.  Contents      1 Types of systems     2 Software tools     3 Benefits     4 Generic data warehouse environment     5 History     6 Information storage         6.1 Facts         6.2 Dimensional vs. normalized approach for storage of data     7 Design methods         7.1 Bottom-up design         7.2 Top-down design         7.3 Hybrid design     8 Data warehouses versus operational systems     9 Evolution in organization use     10 See also     11 References     12 Further reading     13 External links  Types of systems  Data mart     A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area) hence, they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.[2] Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.  The difference between data warehouse and data mart Data warehouse \tdata mart enterprise-wide data \tdepartment-wide data multiple subject areas \tsingle subject area difficult to build \teasy to build takes more time to build \tless time to build larger memory \tlimited memory  Types of data marts      Dependent data mart     Independent data mart     Hybrid data mart  Online analytical processing (OLAP)     OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day.The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.[3]  Online transaction processing (OLTP)     OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF).[4] Normalization is the norm for data modeling techniques in this system.  Predictive analysis     Predictive analysis is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM (customer relationship management).  Software tools  The typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.[5]  This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support.[6] However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata. Benefits  A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to :      Congregate data from multiple sources into a single database so a single query engine can be used to present data.     Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.     Maintain data history, even if the source transaction systems do not.     Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.     Improve data quality, by providing consistent codes and descriptions, flagging or even fixing bad data.     Present the organization's information consistently.     Provide a single common data model for all data of interest regardless of the data's source.     Restructure the data so that it makes sense to the business users.     Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the operational systems.     Add value to operational business applications, notably customer relationship management (CRM) systems.     Make decision–support queries easier to write.  Generic data warehouse environment  The environment for data warehouses and marts includes the following:      Source systems that provide data to the warehouse or mart;     Data integration technology and processes that are needed to prepare the data for use;     Different architectures for storing data in an organization's data warehouse or data marts;     Different tools and applications for the variety of users;     Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.  In regards to source systems listed above, Rainer[clarification needed] states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.[7]  Regarding data integration, Rainer states, “It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse”.[7]  Rainer discusses storing data in an organization’s data warehouse or data marts.[7]  Metadata are data about data. “IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures“.[7]  Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.[7] A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization.[7] Once data are stored in a data mart or warehouse, they can be accessed. History  The concept of data warehousing dates back to the late 1980s[8] when IBM researchers Barry Devlin and Paul Murphy developed the \"business data warehouse\". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from \"data marts\" that were tailored for ready access by users.  Key developments in early years of data warehousing were:      1960s — General Mills and Dartmouth College, in a joint research project, develop the terms dimensions and facts.[9]     1970s — ACNielsen and IRI provide dimensional data marts for retail sales.[9]     1970s — Bill Inmon begins to define and discuss the term: Data Warehouse.[citation needed]     1975 — Sperry Univac introduces MAPPER (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first 4GL. First platform designed for building Information Centers (a forerunner of contemporary Enterprise Data Warehousing platforms)     1983 — Teradata introduces a database management system specifically designed for decision support.     1984 — Metaphor Computer Systems, founded by David Liddle and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.     1988 — Barry Devlin and Paul Murphy publish the article An architecture for a business and information system where they introduce the term \"business data warehouse\".[10]     1990 — Red Brick Systems, founded by Ralph Kimball, introduces Red Brick Warehouse, a database management system specifically for data warehousing.     1991 — Prism Solutions, founded by Bill Inmon, introduces Prism Warehouse Manager, software for developing a data warehouse.     1992 — Bill Inmon publishes the book Building the Data Warehouse.[11]     1995 — The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.     1996 — Ralph Kimball publishes the book The Data Warehouse Toolkit.[12]     2012 — Bill Inmon developed and made public technology known as \"textual disambiguation\". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.  Information storage Facts  A fact is a value or measurement, which represents a fact about the managed entity or system.  Facts as reported by the reporting entity are said to be at raw level. E.g. if a BTS (business transformation service) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:      tch_req_total = 1000     tch_req_success = 820     tch_req_fail = 180  Facts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. These are called aggregates or summaries or aggregated facts.  E.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension. E.g.      t c h _ r e q _ s u c c e s s _ c i t y = t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 {\\displaystyle tch\\_req\\_success\\_city=tch\\_req\\_success\\_bts1+tch\\_req\\_success\\_bts2+tch\\_req\\_success\\_bts3} tch\\_req\\_success\\_city = tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3     a v g _ t c h _ r e q _ s u c c e s s _ c i t y = ( t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 ) / 3 {\\displaystyle avg\\_tch\\_req\\_success\\_city=(tch\\_req\\_success\\_bts1+tch\\_req\\_success\\_bts2+tch\\_req\\_success\\_bts3)/3} avg\\_tch\\_req\\_success\\_city = (tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3) / 3  Dimensional vs. normalized approach for storage of data  There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.  The dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.  In a dimensional approach, transaction data are partitioned into \"facts\", which are generally numeric transaction data, and \"dimensions\", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.  A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.[12] Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus,this type of modeling technique is very useful for end-user queries in data warehouse.[3]  The main disadvantages of the dimensional approach are the following:      In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.     It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.  In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by subject areas that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008)[citation needed]. The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.  Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).  In Information-Driven Business,[13] Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.[14] Design methods \tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2015) (Learn how and when to remove this template message) Bottom-up design  In the bottom-up approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of \"the bus\", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.[15] Top-down design  The top-down approach is designed using a normalized enterprise data model. \"Atomic\" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.[16] Hybrid design  Data warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.  The DW database in a hybrid solution is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.  The Data Vault Modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes. Data warehouses versus operational systems  Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.  Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse. Evolution in organization use  These terms refer to the level of sophistication of a data warehouse:  Offline operational data warehouse     Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data Offline data warehouse     Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting. On time data warehouse     Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data Integrated data warehouse     These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.[17]   OLTP stands for On Line Transaction Processing and is a data modeling approach typically used to facilitate and manage usual business applications. Most of applications you see and use are OLTP based. OLAP stands for On Line Analytic Processing and is an approach to answer multi-dimensional queries ETL Extract, Transform and Load is a process in data warehousing responsible for pulling data out of the source systems and placing it into a data warehouse. OLAP is an acronym for Online Analytical Processing. OLAP performs multidimensional analysis of business data and provides the capability for complex calculations, trend analysis, and sophisticated data modeling. Data curation platform, metadata management ETL, Curator's Workbench, DataUp, MIXED  Backup and storage management iRODS, XArch, Nesstar In computing, online analytical processing, or OLAP (/ˈoʊlæp/), is an approach to answering multi-dimensional analytical (MDA) queries swiftly.[1] OLAP is part of the broader category of business intelligence, which also encompasses relational database, report writing and data mining.[2] Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM),[3] budgeting and forecasting, financial reporting and similar areas, with new applications coming up, such as agriculture.[4] The term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).[5]  OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.[6] Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region’s sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints. These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)  Databases configured for OLAP use a multidimensional data model, allowing for complex analytical and ad hoc queries with a rapid execution time  In information technology, a backup, or the process of backing up, refers to the copying and archiving of computer data so it may be used to restore the original after a data loss event. The verb form is to back up in two words, whereas the noun is backup.[1]  Backups have two distinct purposes. The primary purpose is to recover data after its loss, be it by data deletion or corruption. Data loss can be a common experience of computer users; a 2008 survey found that 66% of respondents had lost files on their home PC.[2] The secondary purpose of backups is to recover data from an earlier time, according to a user-defined data retention policy, typically configured within a backup application for how long copies of data are required. Though backups represent a simple form of disaster recovery, and should be part of any disaster recovery plan, backups by themselves should not be considered a complete disaster recovery plan. One reason for this is that not all backup systems are able to reconstitute a computer system or other complex configuration such as a computer cluster, active directory server, or database server by simply restoring data from a backup.  Since a backup system contains at least one copy of all data considered worth saving, the data storage requirements can be significant. Organizing this storage space and managing the backup process can be a complicated undertaking. A data repository model may be used to provide structure to the storage. Nowadays, there are many different types of data storage devices that are useful for making backups. There are also many different ways in which these devices can be arranged to provide geographic redundancy, data security, and portability.  Before data are sent to their storage locations, they are selected, extracted, and manipulated. Many different techniques have been developed to optimize the backup procedure. These include optimizations for dealing with open files and live data sources as well as compression, encryption, and de-duplication, among others. Every backup scheme should include dry runs that validate the reliability of the data being backed up. It is important to recognize the limitations and human factors involved in any backup scheme.  Contents      1 Storage, the base of a backup system         1.1 Data repository models         1.2 Storage media         1.3 Managing the data repository     2 Selection and extraction of data         2.1 Files         2.2 Filesystems         2.3 Live data         2.4 Metadata     3 Manipulation of data and dataset optimization     4 Managing the backup process         4.1 Objectives         4.2 Limitations         4.3 Implementation         4.4 Measuring the process     5 See also     6 References     7 External links  Storage, the base of a backup system Data repository models  Any backup strategy starts with a concept of a data repository. The backup data needs to be stored, and probably should be organized to a degree. The organisation could be as simple as a sheet of paper with a list of all backup media (CDs etc.) and the dates they were produced. A more sophisticated setup could include a computerized index, catalog, or relational database. Different approaches have different advantages. Part of the model is the backup rotation scheme.  Unstructured      An unstructured repository may simply be a stack of or CD-Rs or DVD-Rs with minimal information about what was backed up and when. This is the easiest to implement, but probably the least likely to achieve a high level of recoverability as it lacks automation. Full only / System imaging      A repository of this type contains complete system images taken at one or more specific points in time. This technology is frequently used by computer technicians to record known good configurations. Imaging[3] is generally more useful for deploying a standard configuration to many systems rather than as a tool for making ongoing backups of diverse systems. Incremental      An incremental style repository aims to make it more feasible to store backups from more points in time by organizing the data into increments of change between points in time. This eliminates the need to store duplicate copies of unchanged data: with full backups a lot of the data will be unchanged from what has been backed up previously. Typically, a full backup (of all files) is made on one occasion (or at infrequent intervals) and serves as the reference point for an incremental backup set. After that, a number of incremental backups are made after successive time periods. Restoring the whole system to the date of the last incremental backup would require starting from the last full backup taken before the data loss, and then applying in turn each of the incremental backups since then.[4] Additionally, some backup systems can reorganize the repository to synthesize full backups from a series of incrementals. Differential      Each differential backup saves the data that has changed since the last full backup. It has the advantage that only a maximum of two data sets are needed to restore the data. One disadvantage, compared to the incremental backup method, is that as time from the last full backup (and thus the accumulated changes in data) increases, so does the time to perform the differential backup. Restoring an entire system would require starting from the most recent full backup and then applying just the last differential backup since the last full backup.          Note: Vendors have standardized on the meaning of the terms \"incremental backup\" and \"differential backup\". However, there have been cases where conflicting definitions of these terms have been used. The most relevant characteristic of an incremental backup is which reference point it uses to check for changes. By standard definition, a differential backup copies files that have been created or changed since the last full backup, regardless of whether any other differential backups have been made since then, whereas an incremental backup copies files that have been created or changed since the most recent backup of any type (full or incremental). Other variations of incremental backup include multi-level incrementals and incremental backups that compare parts of files instead of just the whole file.  Reverse delta      A reverse delta type repository stores a recent \"mirror\" of the source data and a series of differences between the mirror in its current state and its previous states. A reverse delta backup will start with a normal full backup. After the full backup is performed, the system will periodically synchronize the full backup with the live copy, while storing the data necessary to reconstruct older versions. This can either be done using hard links, or using binary diffs. This system works particularly well for large, slowly changing, data sets. Examples of programs that use this method are rdiff-backup and Time Machine. Continuous data protection      Instead of scheduling periodic backups, the system immediately logs every change on the host system. This is generally done by saving byte or block-level differences rather than file-level differences.[5] It differs from simple disk mirroring in that it enables a roll-back of the log and thus restoration of old images of data.  Storage media  Regardless of the repository model that is used, the data has to be stored on some data storage medium.  Magnetic tape      Magnetic tape has long been the most commonly used medium for bulk data storage, backup, archiving, and interchange. Tape has typically had an order of magnitude better capacity-to-price ratio when compared to hard disk, but recently the ratios for tape and hard disk have become a lot closer.[6] There are many formats, many of which are proprietary or specific to certain markets like mainframes or a particular brand of personal computer. Tape is a sequential access medium, so even though access times may be poor, the rate of continuously writing or reading data can actually be very fast. Some new tape drives are even faster than modern hard disks. Hard disk     The capacity-to-price ratio of hard disk has been rapidly improving for many years. This is making it more competitive with magnetic tape as a bulk storage medium. The main advantages of hard disk storage are low access times, availability, capacity and ease of use.[7] External disks can be connected via local interfaces like SCSI, USB, FireWire, or eSATA, or via longer distance technologies like Ethernet, iSCSI, or Fibre Channel. Some disk-based backup systems, such as Virtual Tape Libraries, support data deduplication which can dramatically reduce the amount of disk storage capacity consumed by daily and weekly backup data. The main disadvantages of hard disk backups are that they are easily damaged, especially while being transported (e.g., for off-site backups), and that their stability over periods of years is a relative unknown. Optical storage      Recordable CDs, DVDs, and Blu-ray Discs are commonly used with personal computers and generally have low media unit costs. However, the capacities and speeds of these and other optical discs are typically an order of magnitude lower than hard disk or tape. Many optical disk formats are WORM type, which makes them useful for archival purposes since the data cannot be changed. The use of an auto-changer or jukebox can make optical discs a feasible option for larger-scale backup systems. Some optical storage systems allow for cataloged data backups without human contact with the discs, allowing for longer data integrity. Solid state storage      Also known as flash memory, thumb drives, USB flash drives, CompactFlash, SmartMedia, Memory Stick, Secure Digital cards, etc., these devices are relatively expensive for their low capacity in comparison to hard disk drives, but are very convenient for backing up relatively low data volumes. A solid-state drive does not contain any movable parts unlike its magnetic drive counterpart, making it less susceptible to physical damage, and can have huge throughput in the order of 500Mbit/s to 6Gbit/s. The capacity offered from SSDs continues to grow and prices are gradually decreasing as they become more common. Remote backup service      As broadband Internet access becomes more widespread, remote backup services are gaining in popularity. Backing up via the Internet to a remote location can protect against some worst-case scenarios such as fires, floods, or earthquakes which would destroy any backups in the immediate vicinity along with everything else. There are, however, a number of drawbacks to remote backup services. First, Internet connections are usually slower than local data storage devices. Residential broadband is especially problematic as routine backups must use an upstream link that's usually much slower than the downstream link used only occasionally to retrieve a file from backup. This tends to limit the use of such services to relatively small amounts of high value data. Secondly, users must trust a third party service provider to maintain the privacy and integrity of their data, although confidentiality can be assured by encrypting the data before transmission to the backup service with an encryption key known only to the user. Ultimately the backup service must itself use one of the above methods so this could be seen as a more complex way of doing traditional backups. Floppy disk      During the 1980s and early 1990s, many personal/home computer users associated backing up mostly with copying to floppy disks. However, the data capacity of floppy disks failed to catch up with growing demands, rendering them effectively obsolete.  Managing the data repository  Regardless of the data repository model, or data storage media used for backups, a balance needs to be struck between accessibility, security and cost. These media management methods are not mutually exclusive and are frequently combined to meet the user's needs. Using on-line disks for staging data before it is sent to a near-line tape library is a common example.  On-line      On-line backup storage is typically the most accessible type of data storage, which can begin restore in milliseconds of time. A good example is an internal hard disk or a disk array (maybe connected to SAN). This type of storage is very convenient and speedy, but is relatively expensive. On-line storage is quite vulnerable to being deleted or overwritten, either by accident, by intentional malevolent action, or in the wake of a data-deleting virus payload. Near-line      Near-line storage is typically less accessible and less expensive than on-line storage, but still useful for backup data storage. A good example would be a tape library with restore times ranging from seconds to a few minutes. A mechanical device is usually used to move media units from storage into a drive where the data can be read or written. Generally it has safety properties similar to on-line storage. Off-line      Off-line storage requires some direct human action to provide access to the storage media: for example inserting a tape into a tape drive or plugging in a cable. Because the data are not accessible via any computer except during limited periods in which they are written or read back, they are largely immune to a whole class of on-line backup failure modes. Access time will vary depending on whether the media are on-site or off-site. Off-site data protection     To protect against a disaster or other site-specific problem, many people choose to send backup media to an off-site vault. The vault can be as simple as a system administrator's home office or as sophisticated as a disaster-hardened, temperature-controlled, high-security bunker with facilities for backup media storage. Importantly a data replica can be off-site but also on-line (e.g., an off-site RAID mirror). Such a replica has fairly limited value as a backup, and should not be confused with an off-line backup. Backup site or disaster recovery center (DR center)     In the event of a disaster, the data on backup media will not be sufficient to recover. Computer systems onto which the data can be restored and properly configured networks are necessary too. Some organizations have their own data recovery centers that are equipped for this scenario. Other organizations contract this out to a third-party recovery center. Because a DR site is itself a huge investment, backing up is very rarely considered the preferred method of moving data to a DR site. A more typical way would be remote disk mirroring, which keeps the DR data as up to date as possible.  Selection and extraction of data  A successful backup job starts with selecting and extracting coherent units of data. Most data on modern computer systems is stored in discrete units, known as files. These files are organized into filesystems. Files that are actively being updated can be thought of as \"live\" and present a challenge to back up. It is also useful to save metadata that describes the computer or the filesystem being backed up.  Deciding what to back up at any given time is a harder process than it seems. By backing up too much redundant data, the data repository will fill up too quickly. Backing up an insufficient amount of data can eventually lead to the loss of critical information. Files  Copying files      With file-level approach, making copies of files is the simplest and most common way to perform a backup. A means to perform this basic function is included in all backup software and all operating systems.  Partial file copying     Instead of copying whole files, one can limit the backup to only the blocks or bytes within a file that have changed in a given period of time. This technique can use substantially less storage space on the backup medium, but requires a high level of sophistication to reconstruct files in a restore situation. Some implementations require integration with the source file system.  Deleted files      To prevent the unintentional restoration of files that have been intentionally deleted, a record of the deletion must be kept.  Filesystems  Filesystem dump     Instead of copying files within a file system, a copy of the whole filesystem itself in block-level can be made. This is also known as a raw partition backup and is related to disk imaging. The process usually involves unmounting the filesystem and running a program like dd (Unix). Because the disk is read sequentially and with large buffers, this type of backup can be much faster than reading every file normally, especially when the filesystem contains many small files, is highly fragmented, or is nearly full. But because this method also reads the free disk blocks that contain no useful data, this method can also be slower than conventional reading, especially when the filesystem is nearly empty. Some filesystems, such as XFS, provide a \"dump\" utility that reads the disk sequentially for high performance while skipping unused sections. The corresponding restore utility can selectively restore individual files or the entire volume at the operator's choice.  Identification of changes     Some filesystems have an archive bit for each file that says it was recently changed. Some backup software looks at the date of the file and compares it with the last backup to determine whether the file was changed.  Versioning file system      A versioning filesystem keeps track of all changes to a file and makes those changes accessible to the user. Generally this gives access to any previous version, all the way back to the file's creation time. An example of this is the Wayback versioning filesystem for Linux.[8]  Live data  If a computer system is in use while it is being backed up, the possibility of files being open for reading or writing is real. If a file is open, the contents on disk may not correctly represent what the owner of the file intends. This is especially true for database files of all kinds. The term fuzzy backup can be used to describe a backup of live data that looks like it ran correctly, but does not represent the state of the data at any single point in time. This is because the data being backed up changed in the period of time between when the backup started and when it finished. For databases in particular, fuzzy backups are worthless.[citation needed]  Snapshot backup     A snapshot is an instantaneous function of some storage systems that presents a copy of the file system as if it were frozen at a specific point in time, often by a copy-on-write mechanism. An effective way to back up live data is to temporarily quiesce them (e.g. close all files), take a snapshot, and then resume live operations. At this point the snapshot can be backed up through normal methods.[9] While a snapshot is very handy for viewing a filesystem as it was at a different point in time, it is hardly an effective backup mechanism by itself.  Open file backup     Many backup software packages feature the ability to handle open files in backup operations. Some simply check for openness and try again later. File locking is useful for regulating access to open files.     When attempting to understand the logistics of backing up open files, one must consider that the backup process could take several minutes to back up a large file such as a database. In order to back up a file that is in use, it is vital that the entire backup represent a single-moment snapshot of the file, rather than a simple copy of a read-through. This represents a challenge when backing up a file that is constantly changing. Either the database file must be locked to prevent changes, or a method must be implemented to ensure that the original snapshot is preserved long enough to be copied, all while changes are being preserved. Backing up a file while it is being changed, in a manner that causes the first part of the backup to represent data before changes occur to be combined with later parts of the backup after the change results in a corrupted file that is unusable, as most large files contain internal references between their various parts that must remain consistent throughout the file.  Cold database backup     During a cold backup, the database is closed or locked and not available to users. The datafiles do not change during the backup process so the database is in a consistent state when it is returned to normal operation.[10]  Hot database backup     Some database management systems offer a means to generate a backup image of the database while it is online and usable (\"hot\"). This usually includes an inconsistent image of the data files plus a log of changes made while the procedure is running. Upon a restore, the changes in the log files are reapplied to bring the copy of the database up-to-date (the point in time at which the initial hot backup ended).[11]  Metadata  Not all information stored on the computer is stored in files. Accurately recovering a complete system from scratch requires keeping track of this non-file data too. [12]  System description     System specifications are needed to procure an exact replacement after a disaster. Boot sector      The boot sector can sometimes be recreated more easily than saving it. Still, it usually isn't a normal file and the system won't boot without it. Partition layout     The layout of the original disk, as well as partition tables and filesystem settings, is needed to properly recreate the original system. File metadata      Each file's permissions, owner, group, ACLs, and any other metadata need to be backed up for a restore to properly recreate the original environment. System metadata     Different operating systems have different ways of storing configuration information. Microsoft Windows keeps a registry of system information that is more difficult to restore than a typical file.  Manipulation of data and dataset optimization  It is frequently useful or required to manipulate the data being backed up to optimize the backup process. These manipulations can provide many benefits including improved backup speed, restore speed, data security, media usage and/or reduced bandwidth requirements.  Compression      Various schemes can be employed to shrink the size of the source data to be stored so that it uses less storage space. Compression is frequently a built-in feature of tape drive hardware. Deduplication      When multiple similar systems are backed up to the same destination storage device, there exists the potential for much redundancy within the backed up data. For example, if 20 Windows workstations were backed up to the same data repository, they might share a common set of system files. The data repository only needs to store one copy of those files to be able to restore any one of those workstations. This technique can be applied at the file level or even on raw blocks of data, potentially resulting in a massive reduction in required storage space. Deduplication can occur on a server before any data moves to backup media, sometimes referred to as source/client side deduplication. This approach also reduces bandwidth required to send backup data to its target media. The process can also occur at the target storage device, sometimes referred to as inline or back-end deduplication. Duplication      Sometimes backup jobs are duplicated to a second set of storage media. This can be done to rearrange the backup images to optimize restore speed or to have a second copy at a different location or on a different storage medium. Encryption      High capacity removable storage media such as backup tapes present a data security risk if they are lost or stolen.[13] Encrypting the data on these media can mitigate this problem, but presents new problems. Encryption is a CPU intensive process that can slow down backup speeds, and the security of the encrypted backups is only as effective as the security of the key management policy. Multiplexing      When there are many more computers to be backed up than there are destination storage devices, the ability to use a single storage device with several simultaneous backups can be useful. Refactoring     The process of rearranging the backup sets in a data repository is known as refactoring. For example, if a backup system uses a single tape each day to store the incremental backups for all the protected computers, restoring one of the computers could potentially require many tapes. Refactoring could be used to consolidate all the backups for a single computer onto a single tape. This is especially useful for backup systems that do incrementals forever style backups. Staging      Sometimes backup jobs are copied to a staging disk before being copied to tape. This process is sometimes referred to as D2D2T, an acronym for Disk to Disk to Tape. This can be useful if there is a problem matching the speed of the final destination device with the source device as is frequently faced in network-based backup systems. It can also serve as a centralized location for applying other data manipulation techniques.  Managing the backup process \tThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2014) (Learn how and when to remove this template message)  As long as new data are being created and changes are being made, backups will need to be performed at frequent intervals. Individuals and organizations with anything from one computer to thousands of computer systems all require protection of data. The scales may be very different, but the objectives and limitations are essentially the same. Those who perform backups need to know how successful the backups are, regardless of scale. Objectives  Recovery point objective (RPO)      The point in time that the restarted infrastructure will reflect. Essentially, this is the roll-back that will be experienced as a result of the recovery. The most desirable RPO would be the point just prior to the data loss event. Making a more recent recovery point achievable requires increasing the frequency of synchronization between the source data and the backup repository.[14][15] Recovery time objective (RTO)      The amount of time elapsed between disaster and restoration of business functions.[16] Data security      In addition to preserving access to data for its owners, data must be restricted from unauthorized access. Backups must be performed in a manner that does not compromise the original owner's undertaking. This can be achieved with data encryption and proper media handling policies. Data retention period      Regulations and policy can lead to situations where backups are expected to be retained for a particular period, but not any further. Retaining backups after this period can lead to unwanted liability and sub-optimal use of storage media.  Limitations  An effective backup scheme will take into consideration the limitations of the situation.  Backup window     The period of time when backups are permitted to run on a system is called the backup window. This is typically the time when the system sees the least usage and the backup process will have the least amount of interference with normal operations. The backup window is usually planned with users' convenience in mind. If a backup extends past the defined backup window, a decision is made whether it is more beneficial to abort the backup or to lengthen the backup window. Performance impact     All backup schemes have some performance impact on the system being backed up. For example, for the period of time that a computer system is being backed up, the hard drive is busy reading files for the purpose of backing up, and its full bandwidth is no longer available for other tasks. Such impacts should be analyzed. Costs of hardware, software, labor     All types of storage media have a finite capacity with a real cost. Matching the correct amount of storage capacity (over time) with the backup needs is an important part of the design of a backup scheme. Any backup scheme has some labor requirement, but complicated schemes have considerably higher labor requirements. The cost of commercial backup software can also be considerable. Network bandwidth     Distributed backup systems can be affected by limited network bandwidth.  Implementation  Meeting the defined objectives in the face of the above limitations can be a difficult task. The tools and concepts below can make that task more achievable.  Scheduling     Using a job scheduler can greatly improve the reliability and consistency of backups by removing part of the human element. Many backup software packages include this functionality. Authentication     Over the course of regular operations, the user accounts and/or system agents that perform the backups need to be authenticated at some level. The power to copy all data off of or onto a system requires unrestricted access. Using an authentication mechanism is a good way to prevent the backup scheme from being used for unauthorized activity. Chain of trust      Removable storage media are physical items and must only be handled by trusted individuals. Establishing a chain of trusted individuals (and vendors) is critical to defining the security of the data.  Measuring the process  To ensure that the backup scheme is working as expected, key factors should be monitored and historical data maintained.  Backup validation      (also known as \"backup success validation\") Provides information about the backup, and proves compliance to regulatory bodies outside the organization: for example, an insurance company in the USA might be required under HIPAA to demonstrate that its client data meet records retention requirements.[17] Disaster, data complexity, data value and increasing dependence upon ever-growing volumes of data all contribute to the anxiety around and dependence upon successful backups to ensure business continuity. Thus many organizations rely on third-party or \"independent\" solutions to test, validate, and optimize their backup operations (backup reporting). Reporting     In larger configurations, reports are useful for monitoring media usage, device status, errors, vault coordination and other information about the backup process. Logging     In addition to the history of computer generated reports, activity and change logs are useful for monitoring backup system events. Validation     Many backup programs use checksums or hashes to validate that the data was accurately copied. These offer several advantages. First, they allow data integrity to be verified without reference to the original file: if the file as stored on the backup medium has the same checksum as the saved value, then it is very probably correct. Second, some backup programs can use checksums to avoid making redundant copies of files, and thus improve backup speed. This is particularly useful for the de-duplication process. Monitored backup     Backup processes are monitored by a third party monitoring center, which alerts users to any errors that occur during automated backups. Monitored backup requires software capable of pinging[clarification needed] the monitoring center's servers in the case of errors. Some monitoring services also allow collection of historical meta-data, that can be used for Storage Resource Management purposes like projection of data growth, locating redundant primary storage capacity and reclaimable backup capacity.   Backup software are computer programs used to perform backup; they create supplementary exact copies of files, databases or entire computers. These programs may later use the supplementary copies to restore the original contents in the event of data loss.[1]  Contents      1 Key features         1.1 Volumes         1.2 Data compression         1.3 Access to open files         1.4 Differential and incremental backups         1.5 Schedules         1.6 Encryption         1.7 Transaction mechanism     2 See also     3 References  Key features  There are several features of backup software that make it more effective in backing up data. Volumes Main article: Volume (compression)  Voluming allows the ability to compress and split backup data into separate parts for storage on smaller, removable media such as CDs. It was often used because CDs were easy to transport off-site and inexpensive compared to hard drives or servers.  However, the recent increase in hard drive capacity and decrease in drive cost has made voluming a far less popular solution. The introduction of small, portable, durable USB drives, and the increase in broadband capacity has provided easier and more secure methods of transporting backup data off-site. Data compression Main articles: Data compression and Data deduplication  Since hard drive space has cost, compressing the data will reduce the size allowing for less drive space to be used to save money. Access to open files Main article: File locking  Many backup solutions offer a plug-in for access to exclusive, in use, and locked files. Differential and incremental backups Main article: Backup rotation scheme  Backup solutions generally support differential backups and incremental backups in addition to full backups, so only material that is newer or changed compared to the backed up data is actually backed up. The effect of these is to increase significantly the speed of the backup process over slow networks while decreasing space requirements. Schedules Main article: Job scheduler  Backup schedules are usually supported to reduce maintenance of the backup tool and increase the reliability of the backups. Encryption Main article: Encryption  To prevent data theft, some backup software offers cryptography features to protect the backup. Transaction mechanism Main article: Database management system § Transaction mechanism  To prevent loss of previously backed up data during a backup, some backup software (e.g. Areca Backup, Argentum Backup) offer Transaction mechanism (with commit / rollback management) for all critical processes (such as backups or merges) to guarantee the backups' integrity. A data management plan or DMP is a formal document that outlines how you will handle your data both during your research, and after the project is completed.[1] The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.  Contents      1 Importance     2 Major Components         2.1 Information about data & data format         2.2 Metadata content and format         2.3 Policies for access, sharing, and re-use         2.4 Long-term storage and data management         2.5 Budget     3 NSF Data Management Plan     4 ESRC Data Management Plan     5 References     6 Further reading     7 External links  Importance  Preparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.[2] This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future. One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers. It also allows the data collector to direct requests for data to the database, rather than address requests individually. Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.  Funding agencies are beginning to require data management plans as part of the proposal and evaluation process.[3] Major Components Information about data & data format      Include a description of data to be produced by the project.[4] This might include (but is not limited to) data that are:         Experimental         Observational         Raw or derived         Physical collections         Models         Simulations         Curriculum materials         Software         Images     How will the data be acquired? When and where will they be acquired?     After collection, how will the data be processed? Include information about         Software used         Algorithms         Scientific workflows     Describe the file formats that will be used, justify those formats, and describe the naming conventions used.     Identify the quality assurance & quality control measures that will be taken during sample collection, analysis, and processing.     If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?     How will the data be managed in the short-term? Consider the following:         Version control for files         Backing up data and data products         Security & protection of data and data products         Who will be responsible for management  Metadata content and format  Metadata are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as “data about data”.[5] Consider the following:      What metadata are needed? Include any details that make data meaningful.     How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.     What format will be used for the metadata? Consider the metadata standards commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.  Policies for access, sharing, and re-use      Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.     Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.     Address any ethical or privacy issues with data sharing     Address intellectual property & copyright issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?     Describe the intended future uses/users for the data     Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a digital object identifier (doi) assigned to it?  Long-term storage and data management      Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.     Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.     An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.  Budget  Data management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are      Personnel time for data preparation, management, documentation, and preservation     Hardware and/or software needed for data management, backing up, security, documentation, and preservation     Costs associated with submitting the data to an archive  The data management plan should include how these costs will be paid. NSF Data Management Plan  All grant proposals submitted to NSF must include a Data Management Plan that is no more than two pages.[6] This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:      The types of data     The standards to be used for data and metadata format and content     Policies for access and sharing     Policies and provisions for re-use     Plans for archiving data  Policy summarized from the NSF Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):[7]      Promptly publish with appropriate authorship     Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame     Share software and inventions     Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others     Policies will be implemented via         Proposal review         Award negotiations and conditions         Support/incentives  ESRC Data Management Plan  Since 1995, the UK's Economic and Social Research Council (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.[8]  ESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The UK Data Service, the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.[9][10]  ESRC has a longstanding arrangement with the UK Data Archive, based at the University of Essex, as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.[11] The Archive enables data re-use by preserving data and making them available to the research and teaching communities.  DATA MANAGEMENT SAMPLE PLAN  2  This study will only collect non-sensitive data. No pers onal identifiers will be recorded or retained by the  researchers  in  any  form.  There  are  no  copyright  or   licensing  issues  associated  with  the  data  being   submitted.  Access, Sharing and Re-use of Data  The  researchers  associated  with  this  study  are  no t  aware  of  any  reasons  which  might  prohibit  the   sharing  and  re-use  of  the  data  being  submitted.  Th e  researchers  are  not  required  to  make  this  data   available  publicly  but  have  elected  to  do  so.  The  da ta  being  submitted  will  be  made  publicly  available   through the Odum Institute for Research in Social Sc ience located at UNC-CH by January 1, 2013. There  will  be  no  additional  restrictions  or  permissions  required  for  accessing  the  data.  Findings  will  be   published by the researchers based on this data; the  estimated date of publication is June 1, 2012. There  is  an  agreement  regarding  the  right  of  the  original   data  collector,  creator  or   principal  investigator  for   first  use  of  the  data.  The  specified  embargo  period  associated  with  the  data  being  submitted  extends   from  the  projected  conclusion  date  for  initial   research  (December  31,  2011)  until  six  months  after   projected publication date for the fi ndings (June 1, 2012). The embargo  will be lifted by January 1, 2013.  Data Standards and Capture  The associated data types will be captured using Qual trics survey software and analyzed using SPSS data  analytics tools. The researchers are not aware of any  issues regarding the effects or limitations of these  formats regarding the data being submitted.  Metadata  General  metadata  related  to  the  survey  topic  wi ll  be  created  for  the  data  being  submitted.  The   associated  metadata  will  be  manually  created  in   XML  file  format.  DDI  metadata  standards  will  be   applied during the creation of the metadata.   Security, Storage, Management and Back-Up of Data  The Odum Institute’s experience with, and commitment  to, secure data archiving is well established and  is  in  keeping  with  established   UNC  Information  Security  Policies .  During  the  implementation  of  the   survey,  associated  research  data  will  be  physicall y  stored  on  a  password-protected  secure  server   maintained  by  UNC-CH  using  standard  SPSS  file  form ats.  No  data  will  reside  on  portable  or  laptop   devices, and no other external media/format(s) will be used for data storage.   Research  data  is  backed  up  on  a  daily  basis.  The   researchers  are  currently  responsible  for  storage,   maintenance  and  back-up  of  the  data.  The  specific  storage  volume  of  the  data  being  submitted  will  be   not  more  than  1GB  maximum.  The  long-term  strategy   for  the  maintenance,  curation  and  archiving  of   the  data  will  be  implemented  when  the  data  and  associated  research  are  migrated  to  the  Odum   Institute for archiving usin g the Odum Institute DVN.  Preservation, Review and Long-Term Management of Data  ODUM INSTITUTE  −  DATA MANAGEMENT SAMPLE PLAN  3  Data  collected  during  this  study  will  be  archived  with   the  Odum  Institute  at  UNC-CH.  The  data  will  be   stored in a specific virtual archive and will be ma de publicly available through  the Odum Institute DVN.  As  a  result  of  this  arrangement,  there  are  no  specif ic  financial  considerations  of  which  the  researchers   are  currently  aware  which  might  impact  the  long-term  management  of  the  data.  The  research  and   archival staff of the Odum Institute will review this  DMP upon accession of the data in order to ensure  and demonstrate compliance. The DMP will again be re viewed by Odum Institute research and archival  staff prior to ingest and release into the Odum DVN Write a data management plan  A data management plan (DMP) will help you manage your data, meet funder requirements, and help others use your data if shared.  The DMPTool is a web-based tool that helps you construct data management plans using templates that address specific funder requirements. From within this tool, you can save your plans, access MIT-specific information & resources, and request a review of your DMP by a member of our Data Management Services team. To access the MIT-customized DMPTool, choose “Massachusetts Institute of Technology” as your institution to log via Touchstone.  Alternatively, you can use the questions below and any specific data management requirements from your funding agency to write your data management plan. Additional resources for creating plans are also provided below.       Project, experiment, and data description         What’s the purpose of the research?         What is the data? How and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?         How much data will be generated for this research?         How long will the data be collected and how often will it change?         Are you using data that someone else produced? If so, where is it from?         Who is responsible for managing the data? Who will ensure that the data management plan is carried out?     Documentation, organization, and storage         What documentation will you be creating in order to make the data understandable by other researchers?         Are you using metadata that is standard to your field? How will the metadata be managed and stored?         What file formats will be used? Do these formats conform to an open standard and/or are they proprietary?         Are you using a file format that is standard to your field? If not, how will you document the alternative you are using?         What directory and file naming convention will be used?         What are your local storage and backup procedures? Will this data require secure storage?         What tools or software are required to read or view the data?     Access, sharing, and re-use         Who has the right to manage this data? Is it the responsibility of the PI, student, lab, MIT, or funding agency?         What data will be shared, when, and how?         Does sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?         Who holds intellectual property rights for the data and other information created by the project? Will any copyrighted or licensed material be used? Do you have permission to use/disseminate this material?         Are there any patent- or technology-licensing-related restrictions on data sharing associated with this grant? The Technology Licensing Office (TLO) can provide this information.         Will this research be published in a journal that requires the underlying data to accompany articles?         Will there be any embargoes on the data?         Will you permit re-use, redistribution, or the creation of new tools, services, data sets, or products (derivatives)? Will commercial use be allowed?     Archiving         How will you be archiving the data? Will you be storing it in an archive or repository for long-term access? If not, how will you preserve access to the data?         Is a discipline-specific repository available? If not, you could consider depositing your data into DSpace@MIT. Email us at data-management@mit.edu if you’re interested in using DSpace@MIT to store your data.         How will you prepare data for preservation or data sharing? Will the data need to be anonymized or converted to more stable file formats?         Are software or tools needed to use the data? Will these be archived?         How long should the data be retained? 3-5 years, 10 years, or forever?", "category": "Edison", "id": 136}
{"skillName": "DSENG04", "skillText": "A NoSQL (originally referring to \"non SQL\" or \"non relational\")[1] database provides a mechanism for storage and retrieval of data which is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but did not obtain the \"NoSQL\" moniker until a surge of popularity in the early twenty-first century,[2] triggered by the needs of Web 2.0 companies such as Facebook, Google and Amazon.com.[3][4][5] NoSQL databases are increasingly used in big data and real-time web applications.[6] NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages.[7][8]  Motivations for this approach include: simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases),[2] and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve. Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.[9]  Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.[10] Most NoSQL stores lack true ACID transactions, although a few databases, such as MarkLogic, Aerospike, FairCom c-treeACE, Google Spanner (though technically a NewSQL database), Symas LMDB and OrientDB have made them central to their designs. (See ACID and JOIN Support.)  Instead, most NoSQL databases offer a concept of \"eventual consistency\" in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.[11] Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss.[12] Fortunately, some NoSQL systems provide concepts such as write-ahead logging to avoid data loss.[13] For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases \"do not allow referential integrity constraints to span databases.\"[14] There are few systems that maintain both ACID transactions and X/Open XA standards for distributed transaction processing.  Contents      1 History     2 Types and examples of NoSQL databases         2.1 Key-value stores         2.2 Document store         2.3 Graph         2.4 Object database         2.5 Tabular         2.6 Tuple store         2.7 Triple/quad store (RDF) database         2.8 Hosted         2.9 Multivalue databases         2.10 Multimodel database     3 Performance     4 Handling relational data         4.1 Multiple queries         4.2 Caching/replication/non-normalized data         4.3 Nesting data     5 ACID and JOIN Support     6 See also     7 References     8 Further reading     9 External links  History  The term NoSQL was used by Carlo Strozzi in 1998 to name his lightweight, Strozzi NoSQL open-source relational database that did not expose the standard SQL interface, but was still relational.[15] His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases. Strozzi suggests that, as the current NoSQL movement \"departs from the relational model altogether; it should therefore have been called more appropriately 'NoREL'\",[16] referring to 'No Relational'.  Johan Oskarsson of Last.fm reintroduced the term NoSQL in early 2009 when he organized an event to discuss \"open source distributed, non relational databases\".[17] The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide atomicity, consistency, isolation and durability guarantees, contrary to the prevailing practice among relational database systems.[18]  Based on 2014 revenue, the NoSQL market leaders are MarkLogic, MongoDB, and Datastax.[19] Based on 2015 popularity rankings, the most popular NoSQL databases are MongoDB, Apache Cassandra, and Redis.[20] Types and examples of NoSQL databases  There have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:      Column: Accumulo, Cassandra, Druid, HBase, Vertica     Document: Apache CouchDB, Clusterpoint, Couchbase, DocumentDB, HyperDex, Lotus Notes, MarkLogic, MongoDB, OrientDB, Qizx, RethinkDB     Key-value: Aerospike, Couchbase, Dynamo, FairCom c-treeACE, FoundationDB, HyperDex, MemcacheDB, MUMPS, Oracle NoSQL Database, OrientDB, Redis, Riak, Berkeley DB     Graph: AllegroGraph, InfiniteGraph,Giraph, MarkLogic, Neo4J, OrientDB, Virtuoso, Stardog     Multi-model: Alchemy Database, ArangoDB, CortexDB, FoundationDB, MarkLogic, OrientDB  A more detailed classification is the following, based on one from Stephen Yen:[21] Type \tExamples of this type Key-Value Cache \tCoherence, eXtreme Scale, GigaSpaces, GemFire, Hazelcast, Infinispan, JBoss Cache, Memcached, Repcached, Terracotta, Velocity Key-Value Store \tFlare, Keyspace, RAMCloud, SchemaFree, Hyperdex, Aerospike Key-Value Store (Eventually-Consistent) \tDovetailDB, Oracle NoSQL Database, Dynamo, Riak, Dynomite, MotionDb, Voldemort, SubRecord Key-Value Store (Ordered) \tActord, FoundationDB, Lightcloud, LMDB, Luxio, MemcacheDB, NMDB, Scalaris, TokyoTyrant Data-Structures Server \tRedis Tuple Store \tApache River, Coord, GigaSpaces Object Database \tDB4O, Objectivity/DB, Perst, Shoal, ZopeDB Document Store \tClusterpoint, Couchbase, CouchDB, DocumentDB, Lotus Notes, MarkLogic, MongoDB, Qizx, RethinkDB, XML-databases Wide Column Store \tBigTable, Cassandra, Druid, HBase, Hypertable, KAI, KDI, OpenNeptune, Qbase  Correlation databases are model-independent, and instead of row-based or column-based storage, use value-based storage. Key-value stores Main article: Key-value database  Key-value (KV) stores use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.[22][23]  The key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in lexicographic order. This extension is computationally powerful, in that it can efficiently retrieve selective key ranges.[24]  Key-value stores can use consistency models ranging from eventual consistency to serializability. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ solid-state drives or rotating disks.  Examples include Oracle NoSQL Database, Redis, and dbm. Document store Main articles: Document-oriented database and XML database  The central concept of a document store is the notion of a \"document\". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, YAML, and JSON as well as binary forms like BSON. Documents are addressed in the database via a unique key that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents  Different implementations offer different ways of organizing and/or grouping documents:      Collections     Tags     Non-visible metadata     Directory hierarchies  Compared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different. Graph Main article: Graph database  This kind of database is designed for data whose relations are well represented as a graph consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.  Graph databases and their query language  Name \tLanguage(s) \tNotes AllegroGraph \tSPARQL \tRDF triple store DEX/Sparksee \tC++, Java, .NET, Python \tGraph database FlockDB \tScala \tGraph database IBM DB2 \tSPARQL \tRDF triple store added in DB2 10 InfiniteGraph \tJava \tGraph database MarkLogic \tJava, JavaScript, SPARQL, XQuery \tMulti-model document database and RDF triple store Neo4j \tCypher \tGraph database OWLIM \tJava, SPARQL 1.1 \tRDF triple store Oracle \tSPARQL 1.1 \tRDF triple store added in 11g OrientDB \tJava \tMulti-model document and graph database Sqrrl Enterprise \tJava \tGraph database OpenLink Virtuoso \tC++, C#, Java, SPARQL \tMiddleware and database engine hybrid Stardog \tJava, SPARQL \tGraph database Object database Main article: Object database      db4o     GemStone/S     InterSystems Caché     JADE     NeoDatis ODB     ObjectDatabase++     ObjectDB     Objectivity/DB     ObjectStore     ODABA     Perst     OpenLink Virtuoso     Versant Object Database     ZODB  Tabular      Apache Accumulo     BigTable     Apache Hbase     Hypertable     Mnesia     OpenLink Virtuoso  Tuple store      Apache River     GigaSpaces     Tarantool     TIBCO ActiveSpaces     OpenLink Virtuoso  Triple/quad store (RDF) database Main articles: Triplestore and Named graph      AllegroGraph     Apache JENA (It is a framework, not a database)     MarkLogic     Ontotext-OWLIM     Oracle NoSQL database     SparkleDB     Virtuoso Universal Server     Stardog  Hosted      Amazon DynamoDB     Amazon SimpleDB     Datastore on Google Appengine     Clusterpoint database     Cloudant Data Layer (CouchDB)     Freebase     Microsoft Azure Tables[25]     Microsoft Azure DocumentDB[26]     OpenLink Virtuoso     Drenel Hosted MongoDB  Multivalue databases      D3 Pick database     Extensible Storage Engine (ESE/NT)     InfinityDB     InterSystems Caché     jBASE Pick database     Northgate Information Solutions Reality, the original Pick/MV Database     OpenQM     Revelation Software's OpenInsight     Rocket U2  Multimodel database      OrientDB     FoundationDB     ArangoDB     MarkLogic  Performance  Ben Scofield rated different categories of NoSQL databases as follows:[27] Data Model \tPerformance \tScalability \tFlexibility \tComplexity \tFunctionality Key–Value Store \thigh \thigh \thigh \tnone \tvariable (none) Column-Oriented Store \thigh \thigh \tmoderate \tlow \tminimal Document-Oriented Store \thigh \tvariable (high) \thigh \tlow \tvariable (low) Graph Database \tvariable \tvariable \thigh \thigh \tgraph theory Relational Database \tvariable \tvariable \tlow \tmoderate \trelational algebra  Performance and scalability comparisons are sometimes done with the YCSB benchmark. See also: Comparison of structured storage software Handling relational data  Since most NoSQL databases lack ability for joins in queries, the database schema generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.) Multiple queries  Instead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate. Caching/replication/non-normalized data  Instead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.[28] Nesting data  With document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task. ACID and JOIN Support  If a database is marked as supporting ACID or joins, then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess. Database \tACID \tJoins Aerospike \tYes \tNo ArangoDB \tYes \tYes CouchDB \tYes \tYes c-treeACE \tYes \tYes HyperDex \tYes[nb 1] \tYes InfinityDB \tYes \tNo LMDB \tYes \tNo MarkLogic \tYes \tYes[nb 2] OrientDB \tYes \tYes  HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.      Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.[29]  See also      CAP theorem     Comparison of object database management systems     Comparison of structured storage software     Correlation database     Distributed cache     Faceted search     MultiValue database     Multi-model database     Triplestore    In theoretical computer science, the CAP theorem, also named Brewer's theorem after computer scientist Eric Brewer, states that it is impossible for a distributed computer system to simultaneously provide all three of the following guarantees:[1][2][3]      Consistency (all nodes see the same data at the same time)     Availability (every request receives a response about whether it succeeded or failed)     Partition tolerance (the system continues to operate despite arbitrary partitioning due to network failures)  In 2012 Brewer clarified some of his positions, including why the often-used \"two out of three\" concept can be misleading or misapplied, and the different definition of consistency used in CAP relative to the one used in ACID.[4]  Contents      1 History         1.1 Brewer’s 2012 article     2 See also     3 References     4 External links  History  According to University of California, Berkeley computer scientist Eric Brewer, the theorem first appeared in autumn 1998.[4] It was published as the CAP principle in 1999[5] and presented as a conjecture by Brewer at the 2000 Symposium on Principles of Distributed Computing (PODC).[6] In 2002, Seth Gilbert and Nancy Lynch of MIT published a formal proof of Brewer's conjecture, rendering it a theorem.[1] This last claim has been criticized, however, this reference does not offer a peer-reviewed formal proof - just an informal assertion on a blog posting.[7] Brewer’s 2012 article  CAP Twelve Years Later: How the \"Rules\" Have Changed  See also      Consistency model     Fallacies of Distributed Computing     Paxos (computer science)     Project management triangle     Raft (computer science)     Trilemma    A correlation database is a database management system (DBMS) that is data-model-independent and designed to efficiently handle unplanned, ad hoc queries in an analytical system environment.  Unlike row-oriented relational database management systems, which use a records-based storage approach, or column-oriented databases which use a column-based storage method, a correlation database uses a value-based storage (VBS) architecture in which each unique data value is stored only once and an auto-generated indexing system maintains the context for all values.[1]  Contents      1 Structure     2 Comparison of DBMS storage structures         2.1 Storage in RDBMS         2.2 Storage in column-oriented databases         2.3 Storage in CDBMS     3 Advantages and disadvantages     4 References  Structure  Because a correlation DBMS stores each unique data value only once, the physical database size is significantly smaller than relational or column-oriented databases, without the use of data compression techniques. Above approximately 30GB, a correlation DBMS may become smaller than the raw data set.[citation needed]  The VBS model used by a CDBMS consists of three primary physical sets of objects that are stored and managed:      a data dictionary (metadata);     an indexing and linking data set (additional metadata); and     the actual data values that comprise the stored information.  In the VBS model, each unique value in the raw data is stored only once; therefore, the data is always normalized at the level of unique values.[2] This eliminates the need to normalize data sets in the logical schema.  Data values are stored together in ordered sets based on data types: all integers in one set, characters in another, etc. This optimizes the data handling processes that access the values.  In addition to typical data values, the data value store contains a special type of data for storing relationships between tables. This functions similarly to foreign keys in RDBMS structures, but with a CDBMS, the relationship is known by the dictionary and stored as a data value, making navigation between tables completely automatic.  The data dictionary contains typical metadata plus additional statistical data about the tables, columns and occurrences of values in the logical schema. It also maintains information about the relationships between the logical tables. The index and linking storage includes all of the data used to locate the contents of a record from the ordered values in the data store.  While not a RAM-based storage system, a CDBMS is designed to use as much RAM as the operating system can provide. For large databases, additional RAM improves performance. Generally, 4GB of RAM will provide optimized access times up to about 100 million records. 8GB of RAM is adequate for databases up to 10 times that size.[3] Because the incremental RAM consumed decreases as the database grows, 16GB of RAM will generally support databases containing up to approximately 20 billion records. Comparison of DBMS storage structures  The sample records shown below illustrate the physical differences in the storage structures used in relational, column-oriented and correlation databases. Cust ID \tName \tCity \tState 12222 \tABC Corp \tMinneapolis \tMN 19434 \tA1 Mfg \tDuluth \tMN 20523 \tJ&J Inc \tSt. Paul \tMN Storage in RDBMS  The record-based structure used in an RDBMS stores elements in the same row adjacent to each other. Variations like clustered indexing may change the sequence of the rows, but all rows, columns and values will be stored as in the table. The above table might be stored as:        12222,ABC Corp,Minneapolis,MN;19434,A1 Mfg,Duluth,MN;20523,J&J Inc,St. Paul,MN  Storage in column-oriented databases  In the column-based structure, elements of the same column are stored adjacent to each other. Consecutive duplicates within a single column may be automatically removed or compressed efficiently.        12222,19434,20523;ABC Corp,A1 Mfg,J&J Inc;Minneapolis,Duluth,St.Paul;MN,MN,MN  Storage in CDBMS  In the VBS structure used in a CDBMS, each unique value is stored once and given an abstract (numeric) identifier, regardless of the number of occurrences or locations in the original data set. The original dataset is then constructed by referencing those logical identifiers. The correlation index may resemble the storage below. Note that the value \"MN\" which occurs multiple times in the data above is only included once. As the amount of repeat data grows, this benefit multiplies.        1:12222,2:19434,3:20523,4:ABC Corp,5:A1 Mfg,6:J&J Inc,7:Minneapolis,8:Duluth,9:St.Paul,10:MN  The records in our example table above can then be expressed as:        11:[1,4,7,10],12:[2,5,8,10],13:[3,6,9,10]  This correlation process is a form of database normalization. Just as one can achieve some benefits of column-oriented storage within an RDBMS, so too can one achieve some benefits of the correlation database through database normalization. However, in a traditional RDBMS this normalization process requires work in the form of table configuration, stored procedures, and SQL statements. We say that a database is a correlation database when it naturally expresses a fully normalized schema without this extra configuration. As a result, a correlation database may have more focused optimizations for this fully normalized structure.  This correlation process is similar to what occurs in a text-search oriented Inverted index. Advantages and disadvantages  For analytical data warehouse applications, a CDBMS has several advantages over alternative database structures. First, because the database engine itself indexes all data and auto-generates its own schema on the fly while loading, it can be implemented quickly and is easy to update. There is no need for physical pre-design and no need to ever restructure the database. Second, a CDBMS enables creation and execution of complex queries such as associative queries (\"show everything that is related to x\") that are difficult if not impossible to model in SQL. The primary advantage of the CDBMS is that it is optimized for executing ad hoc queries - queries not anticipated during the data warehouse design phase.[4]  A CDBMS has two drawbacks in comparison to database alternatives. Unlike relational databases, which can be used in a wide variety of applications, a correlation database is designed specifically for analytical applications and does not provide transaction management features; it cannot be used for transactional processing. Second, because it indexes all data during the load process, the physical load speed of a CDBMS is slower than relational or column-oriented structures. However, because it eliminates the need for logical or physical pre-design, the overall \"time to use\" of a CDBMS is generally similar to or somewhat faster than alternative structures. A triplestore or RDF store is a purpose-built database for the storage and retrieval of triples[1] through semantic queries. A triple is a data entity composed of subject-predicate-object, like \"Bob is 35\" or \"Bob knows Fred\".   Much like a relational database, one stores information in a triplestore and retrieves it via a query language. Unlike a relational database, a triplestore is optimized for the storage and retrieval of triples. In addition to queries, triples can usually be imported/exported using Resource Description Framework (RDF) and other formats.  Contents      1 Implementations     2 Related database types     3 See also     4 References     5 External links  Implementations Main article: List of subject-predicate-object databases  Some triplestores have been built as database engines from scratch, while others have been built on top of existing commercial relational database engines (e.g., SQL-based),[2] or NoSQL document-oriented database engines.[3][4] Like the early development of online analytical processing (OLAP) databases, this intermediate approach allowed large and powerful database engines to be constructed for little programming effort in the initial phases of triplestore development. Long-term though it seems likely that native triplestores will have the advantage for performance. A difficulty with implementing triplestores over SQL is that although triples may thus be stored, implementing efficient querying of a graph-based RDF model (e.g., mapping from SPARQL) onto SQL queries is difficult.[5] Related database types  Adding a name to the triple makes a \"quad store\" or named graph.  A graph database has a more generalized structure than triplestore. Uses graph structures with nodes, edges, and properties to represent and store data. Provides index-free adjacency, meaning every element contains a direct pointer to its adjacent elements and no index lookups are necessary. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases. See also \tThis article may require cleanup to meet Wikipedia's quality standards. The specific problem is: prose in \"See also\" section Please help improve this article if you can. (August 2015) (Learn how and when to remove this template message)      Dataspaces - notes that fact-based, subject-predicate-object triples (data entities) rely on existing matching and mapping generation techniques. The triple data structure allows a pay-as-you-go approach to data integration which effectively postpones the labor-intensive aspects of integration to the very end, just before the integrated data is absolutely needed.     Entity–relationship model - covers entities (things) and the relationships that can exist among them.     ISO/IEC 19788 - Metadata for learning resources (MLR). In a MLR triple, the subject is always the literal of an identifier of the learning resource, such as a URI or ISBN. The predicate is also a literal, the MLR data element specification identifier. Finally, the object can be a literal or a resource class (a set of accepted values, such as a list of terms identifiers from a controlled vocabulary list).     Metaweb's Graphd      tuple store (owned by Google) used in Freebase and Knowledge Graph     Metadata - syntax section - subject-predicate-object triple a/k/a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. \"metacontent = metadata + master data\". All these elements can be thought of as vocabulary. Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by ISO-25964: If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved.     Outline of databases     Semantic data model - covers semantic information, symbols (instance data), meaning from instances, facts as binary relations between data elements. Object-RelationType-Object'     RDFLib - a Python library for working with RDF including both in-memory and persistent Graph backends. Supports subject-predicate-object triple pattern matching.     Semantic wiki and Semantic MediaWiki - illustrates subject-predicate-object support for Wikis, advanced query support, and implementations by organizations including: Pfizer, Harvard Pilgrim Health Care, Johnson & Johnson Pharmaceutical Research and Development, Pacific Northwest National Laboratory,Metropolitan Museum of Art, and the U.S. Department of Defense.     SPARQL W3C specification involving subject-predicate-object triples and List of SPARQL implementations  Relational databases like MySQL, PostgreSQL and SQLite3 represent and store data in tables and rows. They're based on a branch of algebraic set theory known as relational algebra. Meanwhile, non-relational databases like MongoDB represent data in collections of JSON documents. The Mongo import utility can import JSON, CSV and TSV file formats. Mongo query targets of data are technically represented as BSON (binary JASON).  Relational databases use Structured Querying Language (SQL), making them a good choice for applications that involve the management of several transactions. The structure of a relational database allows you to link information from different tables through the use of foreign keys (or indexes), which are used to uniquely identify any atomic piece of data within that table. Other tables may refer to that foreign key, so as to create a link between their data pieces and the piece pointed to by the foreign key. This comes in handy for applications that are heavy into data analysis.  If you want your application to handle a lot of complicated querying, database transactions and routine analysis of data, you’ll probably want to stick with a relational database. And if your application is going to focus on doing many database transactions, it’s important that those transactions are processed reliably. This is where ACID (the set of properties that guarantee database transactions are processed reliably) really matters, and where referential integrity comes into play. Referential integrity (and minimizing ORM Impedance Mismatch)  Referential integrity is the concept in which multiple database tables share a relationship based on the data stored in the tables, and that relationship must remain consistent. This is usually enforced with cascading actions of adding, deleting and updating. To illustrate an example of enforcing referential integrity, let’s consider an app that helps victims of human trafficking locate a safe house and access victim services in real time.  Suppose city or county X has two tables; a Trafficking Victim Shelter table and a Trafficking Shelter Funding table. In the Trafficking Shelter table we have two columns; the Shelter ID (which could be its EIN/FEIN number) and the name of the shelter. In the Trafficking Shelter Funding table, we also have two columns; the Shelter ID and the amount of funding received for that given Shelter ID. Now, suppose a dearth in funding forced Shelter A in city/county X to close its doors. We would need to remove that shelter from locale X since it’s no longer in existence. And since Shelter A also exists in the Shelter Funding table, we need to remove it from there as well. By enforcing referential integrity, we can make this accurate -- and with minimal headaches.  Here’s how:  First, define the Shelter ID column in the Shelter table to be our primary key. Then, define the Shelter ID column in the Shelter Funding table to be a foreign key that points to a primary key (that is the Shelter ID column in the Shelter table). Once we define our foreign-to-primary key relationship, we need to add constraints.  One constraint in particular is known as cascading delete. This means that anytime a shelter is deleted from the Shelter table in our database, all entries for that same shelter would be automatically removed from the Shelter Funding table.  Relate_01  Now, take note of what was designated as the primary key, and why. In our little example of anti-trafficking charities, every non-profit NGO with 501(3)c status is issued an EIN, much like an individual’s social security number. So, in tables where other data is linked to any particular trafficking victim’s shelter in the shelter table, it makes sense to have that unique identifier serve as the primary key and to have the foreign keys point to it.  Relate_02  Keep in mind, there are three rules that referential integrity enforces:       We may not add a record to the Shelter Funding table unless the foreign key for that record points to an existing shelter in the Shelter table. You can think of this as a “No Unattended Child” rule or a “No Orphans” rule.      If a record in the shelter table is deleted, all corresponding records in the Shelter Funding table must also be deleted. The best way to handle this is by using cascade delete.      If the primary key for a record in the Shelter table changes, all corresponding records in the Shelter Funding (and other possible future tables with data relating to the Shelter table) must also be modified using something called a cascade update.   The burden of instilling and maintaining referential integrity rests on the person who designs the database schema. If designing a database schema seems like a daunting task, consider this: Prior to 1970 (when the relational database was born) all databases were flat; data was stored in a long text file called a tab delimited file where each entry was separated by the pipe character (“|”). Searching for specific information to compare and analyze was a difficult, tedious, time-consuming endeavor. With relational databases you can easily search, sort and analyze (usually for comparison to other data purposes) specific pieces of data without having to search sequentially through an entire file (or database), including all the data pieces you’re not interested in.  In the previous example of a relational database (Postgresql), we don’t need to search through an entire database worth of information just to find the information on a shelter that either had its funding slashed or that was forced to close for lack of funds. We can use a simple SQL query to find which shelters closed in a particular region or locale without having to traverse all of the data, including shelters not in that specific area, by using a an SQL SELECT * FROM statement.  Object Relational Mapping (ORM) refers to the programmatic process of converting data between incompatible type systems in object-oriented programming languages (like Ruby). In the context of a Ruby program (a Rails app in particular), the concept of ORM libraries was briefly discussed in our tutorial on Getting started with Rails. When to non-relate  While relational databases are great, they do come with trade-offs. One of those is ORM Impedence Mismatching, because relational databases were not initially created with the OOP languages in mind. The best way to avoid this issue is to create your database schema with referential integrity at its core. So, when using a relational database with an OOP (like Ruby), you have to think about how to set up your primary and foreign keys, the use of constraints (including the cascade delete and update), and how you write your migrations.  But, if you’re dealing with a phenomenally huge amount of data, it can be way too tedious, and the probability of error (in the form of an ORM Impedance Mismatch issue) increases. In that situation you may need to consider going with a non-relational database. A non-relational database just stores data without explicit and structured mechanisms to link data from different tables (or buckets) to one another.  Mongo is a popular non-relational database for MongoDB Ember Angular and Node.js (MEAN) stack developers because it’s basically written in JavaScript; JSON is JavaScript Object Notation, which is a lightweight data interchange format. If your data model turns out to be very complex, or if you find yourself having to de-normalize your database schema, non-relational databases like Mongo may be the best way to go. Other reasons for choosing a non-relational database include:       The need to store serialized arrays in JSON objects      Storing records in the same collection that have different fields or attributes      Finding yourself de-normalizing your database schema or coding around performance and horizontal scalability issues      Problems easily pre-defining your schema because of the nature of your data model   Suppose we were developing an app, and our example for the trafficking victim safe houses was part of a data model that was too complex and had too many tables, making referential integrity extremely difficult. We might handle the representation of our trafficking victim service-providing NGO’s like this instead, using Mongo:  Relate_03  Note the nice, easily readable output. Mongo is accessible with JavaScript, and from a MEAN stack developer’s point of view, it wouldn’t make sense to go with any database that wasn’t easily accessible. Additionally, the MongoDB site is well documented and provides clear, concise examples for how to set up a Mongo database and make the most of it. As a NoSQL database, MongoDB allows developers to define the application’s flow entirely on the code side. One of the biggest issues MEAN stack developers have with relational databases is facing the unavoidable fact that the objects represented in the database are stored in a format that is unable to be easily used by the frontend and vice-versa.  But it isn’t only MEAN stack developers who decided that a non-relational database was the best way to go. Steve Klabnik (a well-known member of the Ruby/Ruby on Rails community and the maintainer of the open source project Hackety-Hack) also chose MongoDB. Of course, he had to make trade-offs in taking this route. This included difficulty in getting Hackety-Hack refactored to be set up for user authentication with Facebook, Twitter, Linkedin and Github accounts. But other Rails developers also like Mongo for its superior horizontal scalability.  One of the biggest advantages in going with a non-relational database is that your database is not at risk for SQL injection attacks, because non-relational databases don’t use SQL and are, for the most part, schema-less. Another major advantage, at least with Mongo, is that you can theoretically shard it forever (although that does bring up replication issues). Sharding distributes the data across partitions to overcome hardware limitations. Non-relational database disadvantages  In non-relational databases like Mongo, there are no joins like there would be in relational databases. This means you need to perform multiple queries and join the data manually within your code -- and that can get very ugly, very fast.  Since Mongo doesn’t automatically treat operations as transactions the way a relational database does, you must manually choose to create a transaction and then manually verify it, manually commit it or roll it back. Even the documentation on the MongoDB site warns you that without taking some potentially time-consuming precautions, and since documents can be fairly complex and nested, the success or failure of a database operation cannot be all or nothing. To put it simply, some operations will succeed while others fail.  Of course, this all brings us back to the beginning; knowing how to ask exactly the right questions in order to effectively whiteboard your data model. It's this key step that will allow you to determine the best route for you regarding your application’s flow. Taking the time to pinpoint the right questions will serve as a solid guide when choosing the programming language to write your application in, and the use of one particular database over another. A hierarchical database model is a data model in which the data is organized into a tree-like structure. The data is stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The entity type of a record defines which fields the record contains. Example of a hierarchical model  A record in the hierarchical database model corresponds to a row (or tuple) in the relational database model and an entity type corresponds to a table (or relation).    The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. In order to retrieve data from a hierarchical database the whole tree needs to be traversed starting from the root node. This model is recognized as the first database model created by IBM in the 1960s[citation needed].  Contents      1 History     2 Examples of hierarchical data represented as relational tables     3 See also     4 References     5 External links  History  The hierarchical structure was developed by IBM in the 1960s, and used in early mainframe DBMS. Records' relationships form a treelike model. This structure is simple but inflexible because the relationship is confined to a one-to-many relationship. The IBM Information Management System (IMS) and the RDM Mobile are examples of a hierarchical database system with multiple hierarchies over the same data. RDM Mobile is a newly designed embedded database for a mobile computer system.[citation needed]  The hierarchical data model lost traction as Codd's relational model became the de facto standard used by virtually all mainstream database management systems. A relational-database implementation of a hierarchical model was first discussed in published form in 1992[1] (see also nested set model). Hierarchical data organization schemes resurfaced with the advent of XML in the late 1990s[2] (see also XML database). The hierarchical structure is used primarily today for storing geographic information and file systems.[citation needed]  Currently hierarchical databases are still widely used especially in applications that require very high performance and availability such as banking and telecommunications. One of the most widely used commercial hierarchical databases is IMS.[3] Another example of the use of hierarchical databases is Windows Registry in the Microsoft Windows operating systems.[4] Examples of hierarchical data represented as relational tables  An organization could store employee information in a table that contains attributes/columns such as employee number, first name, last name, and department number. The organization provides each employee with computer hardware as needed, but computer equipment may only be used by the employee to which it is assigned. The organization could store the computer hardware information in a separate table that includes each part's serial number, type, and the employee that uses it. The tables might look like this: employee table EmpNo \tFirst Name \tLast Name \tDept. Num 100 \tsweet sonija \tMohommad \t10-L 101 \tkhalid ayub \tHashim \t10-L 102 \tAbd e Wahab \tAW \t20-B 103 \tsaadoo \tSandakelum \t20-B \t computer table Serial Num \tType \tUser EmpNo 3009734-4 \tComputer \t100 3-23-283742 \tMonitor \t100 2-22-723423 \tMonitor \t100 232342 \tPrinter \t100  In this model, the employee data table represents the \"parent\" part of the hierarchy, while the computer table represents the \"child\" part of the hierarchy. In contrast to tree structures usually found in computer software algorithms, in this model the children point to the parents. As shown, each employee may possess several pieces of computer equipment, but each individual piece of computer equipment may have only one employee owner.  Consider the following structure: EmpNo \tDesignation \tReportsTo 10 \tDirector \t 20 \tSenior Manager \t10 30 \tTypist \t20 40 \tProgrammer \t20  In this, the \"child\" is the same type as the \"parent\". The hierarchy stating EmpNo 10 is boss of 20, and 30 and 40 each report to 20 is represented by the \"ReportsTo\" column. In Relational database terms, the ReportsTo column is a foreign key referencing the EmpNo column. If the \"child\" data type were different, it would be in a different table, but there would still be a foreign key referencing the EmpNo column of the employees table.  This simple model is commonly known as the adjacency list model, and was introduced by Dr. Edgar F. Codd after initial criticisms surfaced that the relational model could not model hierarchical data. See also      Tree structure     Hierarchical query     Hierarchical clustering    In mathematics graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices, nodes, or points which are connected by edges, arcs, or lines. A graph may be undirected, meaning that there is no distinction between the two vertices associated with each edge, or its edges may be directed from one vertex to another; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.  Refer to the glossary of graph theory for basic definitions in graph theory.  Contents      1 Definitions         1.1 Graph     2 Applications     3 History     4 Graph drawing     5 Graph-theoretic data structures     6 Problems in graph theory         6.1 Enumeration         6.2 Subgraphs, induced subgraphs, and minors         6.3 Graph coloring         6.4 Subsumption and unification         6.5 Route problems         6.6 Network flow         6.7 Visibility problems         6.8 Covering problems         6.9 Decomposition problems         6.10 Graph classes     7 See also         7.1 Related topics         7.2 Algorithms         7.3 Subareas         7.4 Related areas of mathematics         7.5 Generalizations         7.6 Prominent graph theorists     8 Notes     9 References     10 External links         10.1 Online textbooks  Definitions  Definitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures. Graph  In the most common sense of the term,[1] a graph is an ordered pair G = (V, E) comprising a set V of vertices or nodes or points together with a set E of edges or arcs or lines, which are 2-element subsets of V (i.e. an edge is related with two vertices, and the relation is represented as an unordered pair of the vertices with respect to the particular edge). To avoid ambiguity, this type of graph may be described precisely as undirected and simple.  Other senses of graph stem from different conceptions of the edge set. In one more generalized notion,[2] V is a set together with a relation of incidence that associates with each edge two vertices. In another generalized notion, E is a multiset of unordered pairs of (not necessarily distinct) vertices. Many authors call this type of object a multigraph or pseudograph.  All of these variants and others are described more fully below.  The vertices belonging to an edge are called the ends or end vertices of the edge. A vertex may exist in a graph and not belong to an edge.  V and E are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. The order of a graph is |V|, its number of vertices. The size of a graph is |E|, its number of edges. The degree or valency of a vertex is the number of edges that connect to it, where an edge that connects a vertex to itself (a loop) is counted twice.  For an edge {x, y}, graph theorists usually use the somewhat shorter notation xy. Applications The network graph formed by Wikipedia editors (edges) contributing to different Wikipedia language versions (vertices) during one month in summer 2013[3]  Graphs can be used to model many types of relations and processes in physical, biological,[4] social and information systems. Many practical problems can be represented by graphs. Emphasizing their application to real-world systems, the term network is sometimes defined to mean a graph in which attributes (e.g. names) are associated with the nodes and/or edges.  In computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of a website can be represented by a directed graph, in which the vertices represent web pages and directed edges represent links from one page to another. A similar approach can be taken to problems in social media,[5] travel, biology, computer chip design, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. The transformation of graphs is often formalized and represented by graph rewrite systems. Complementary to graph transformation systems focusing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.  Graph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. Within lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs  , as well as various 'Net' projects, such as WordNet, VerbNet, and others.  Graph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such systems. Similarly, in computational neuroscience graphs can be used to represent functional connections between brain areas that interact to give rise to various cognitive processes, where the vertices represent different areas of the brain and the edges represent the connections between those areas. Graphs are also used to represent the micro-scale channels of porous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores.  Graph theory is also widely used in sociology as a way, for example, to measure actors' prestige or to explore rumor spreading, notably through the use of social network analysis software. Under the umbrella of social networks are many different types of graphs.[6] Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.  Likewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths, or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.  In mathematics, graphs are useful in geometry and certain parts of topology such as knot theory. Algebraic graph theory has close links with group theory.  A graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road. History The Königsberg Bridge problem  The paper written by Leonhard Euler on the Seven Bridges of Königsberg and published in 1736 is regarded as the first paper in the history of graph theory.[7] This paper, as well as the one written by Vandermonde on the knight problem, carried on with the analysis situs initiated by Leibniz. Euler's formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized by Cauchy[8] and L'Huillier,[9] and represents the beginning of the branch of mathematics known as topology.  More than one century after Euler's paper on the bridges of Königsberg and while Listing was introducing the concept of topology, Cayley was led by an interest in particular analytical forms arising from differential calculus to study a particular class of graphs, the trees.[10] This study had many implications for theoretical chemistry. The techniques he used mainly concern the enumeration of graphs with particular properties. Enumerative graph theory then arose from the results of Cayley and the fundamental results published by Pólya between 1935 and 1937. These were generalized by De Bruijn in 1959. Cayley linked his results on trees with contemporary studies of chemical composition.[11] The fusion of ideas from mathematics with those from chemistry began what has become part of the standard terminology of graph theory.  In particular, the term \"graph\" was introduced by Sylvester in a paper published in 1878 in Nature, where he draws an analogy between \"quantic invariants\" and \"co-variants\" of algebra and molecular diagrams:[12]      \"[…] Every invariant and co-variant thus becomes expressible by a graph precisely identical with a Kekuléan diagram or chemicograph. […] I give a rule for the geometrical multiplication of graphs, i.e. for constructing a graph to the product of in- or co-variants whose separate graphs are given. […]\" (italics as in the original).  The first textbook on graph theory was written by Dénes Kőnig, and published in 1936.[13] Another book by Frank Harary, published in 1969, was \"considered the world over to be the definitive textbook on the subject\",[14] and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund the Pólya Prize.[15]  One of the most famous and stimulating problems in graph theory is the four color problem: \"Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?\" This problem was first posed by Francis Guthrie in 1852 and its first written record is in a letter of De Morgan addressed to Hamilton the same year. Many incorrect proofs have been proposed, including those by Cayley, Kempe, and others. The study and the generalization of this problem by Tait, Heawood, Ramsey and Hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus. Tait's reformulation generated a new class of problems, the factorization problems, particularly studied by Petersen and Kőnig. The works of Ramsey on colorations and more specially the results obtained by Turán in 1941 was at the origin of another branch of graph theory, extremal graph theory.  The four color problem remained unsolved for more than a century. In 1969 Heinrich Heesch published a method for solving the problem using computers.[16] A computer-aided proof produced in 1976 by Kenneth Appel and Wolfgang Haken makes fundamental use of the notion of \"discharging\" developed by Heesch.[17][18] The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later by Robertson, Seymour, Sanders and Thomas.[19]  The autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of Jordan, Kuratowski and Whitney. Another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicist Gustav Kirchhoff, who published in 1845 his Kirchhoff's circuit laws for calculating the voltage and current in electric circuits.  The introduction of probabilistic methods in graph theory, especially in the study of Erdős and Rényi of the asymptotic probability of graph connectivity, gave rise to yet another branch, known as random graph theory, which has been a fruitful source of graph-theoretic results. Graph drawing Main article: Graph drawing  Graphs are represented visually by drawing a dot or circle for every vertex, and drawing an arc between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow.  A graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.  The pioneering work of W. T. Tutte was very influential in the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.  Graph drawing also can be said to encompass problems that deal with the crossing number and its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For a planar graph, the crossing number is zero by definition.  Drawings on surfaces other than the plane are also studied. Graph-theoretic data structures Main article: Graph (abstract data type)  There are different ways to store graphs in a computer system. The data structure used depends on both the graph structure and the algorithm used for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred for sparse graphs as they have smaller memory requirements. Matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory.  List structures include the incidence list, an array of pairs of vertices, and the adjacency list, which separately lists the neighbors of each vertex: Much like the incidence list, each vertex has a list of which vertices it is adjacent to.  Matrix structures include the incidence matrix, a matrix of 0's and 1's whose rows represent vertices and whose columns represent edges, and the adjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. The Laplacian matrix is a modified form of the adjacency matrix that incorporates information about the degrees of the vertices, and is useful in some calculations such as Kirchhoff's theorem on the number of spanning trees of a graph. The distance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of a shortest path between two vertices. Problems in graph theory Enumeration  There is a large literature on graphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973). Subgraphs, induced subgraphs, and minors  A common problem, called the subgraph isomorphism problem, is finding a fixed graph as a subgraph in a given graph. One reason to be interested in such a question is that many graph properties are hereditary for subgraphs, which means that a graph has the property if and only if all subgraphs have it too. Unfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem. For example:      Finding the largest complete subgraph is called the clique problem (NP-complete).  A similar problem is finding induced subgraphs in a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:      Finding the largest edgeless induced subgraph or independent set is called the independent set problem (NP-complete).  Still another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. A minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example, Wagner's Theorem states:      A graph is planar if it contains as a minor neither the complete bipartite graph K3,3 (see the Three-cottage problem) nor the complete graph K5.  A similar problem, the subdivision containment problem, is to find a fixed graph as a subdivision of a given graph. A subdivision or homeomorphism of a graph is any graph obtained by subdividing some (or no) edges. Subdivision containment is related to graph properties such as planarity. For example, Kuratowski's Theorem states:      A graph is planar if it contains as a subdivision neither the complete bipartite graph K3,3 nor the complete graph K5.  Another problem in subdivision containment is Kelmans-Seymour conjecture:      Every 5-vertex-connected graph that is not planar contains a subdivision of the 5-vertex complete graph K5.  Another class of problems has to do with the extent to which various species and generalizations of graphs are determined by their point-deleted subgraphs. For example:      The reconstruction conjecture  Graph coloring  Many problems have to do with various ways of coloring graphs, for example:      Four-color theorem     Strong perfect graph theorem     Erdős–Faber–Lovász conjecture (unsolved)     Total coloring conjecture, also called Behzad's conjecture (unsolved)     List coloring conjecture (unsolved)     Hadwiger conjecture (graph theory) (unsolved)  Subsumption and unification  Constraint modeling theories concern families of directed graphs related by a partial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs—which are more specific and thus contain a greater amount of information—are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.  For constraint frameworks which are strictly compositional, graph unification is the sufficient satisfiability and combination function. Well-known applications include automatic theorem proving and modeling the elaboration of linguistic structure. Route problems      Hamiltonian path problem     Minimum spanning tree     Route inspection problem (also called the \"Chinese postman problem\")     Seven bridges of Königsberg     Shortest path problem     Steiner tree     Three-cottage problem     Traveling salesman problem (NP-hard)  Network flow  There are numerous problems arising especially from applications that have to do with various notions of flows in networks, for example:      Max flow min cut theorem  Visibility problems      Museum guard problem  Covering problems  Covering problems in graphs are specific instances of subgraph-finding problems, and they tend to be closely related to the clique problem or the independent set problem.      Set cover problem     Vertex cover problem  Decomposition problems  Decomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of question. Often, it is required to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graph Kn into n − 1 specified trees having, respectively, 1, 2, 3, …, n − 1 edges.  Some specific decomposition problems that have been studied include:      Arboricity, a decomposition into as few forests as possible     Cycle double cover, a decomposition into a collection of cycles covering each edge exactly twice     Edge coloring, a decomposition into as few matchings as possible     Graph factorization, a decomposition of a regular graph into regular subgraphs of given degrees  Graph classes  Many problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:      Enumerating the members of a class     Characterizing a class in terms of forbidden substructures     Ascertaining relationships among classes (e.g. does one property of graphs imply another)     Finding efficient algorithms to decide membership in a class     Finding representations for members of a class  See also      Gallery of named graphs     Glossary of graph theory     List of graph theory topics     List of unsolved problems in graph theory     Publications in graph theory  Related topics      Algebraic graph theory     Citation graph     Conceptual graph     Data structure     Disjoint-set data structure     Dual-phase evolution     Entitative graph     Existential graph     Graph algebra     Graph automorphism     Graph coloring     Graph database     Graph data structure     Graph drawing     Graph equation     Graph rewriting     Graph sandwich problem     Graph property     Intersection graph     Logical graph     Loop     Network theory     Null graph     Pebble motion problems     Percolation     Perfect graph     Quantum graph     Random regular graphs     Semantic networks     Spectral graph theory     Strongly regular graphs     Symmetric graphs     Transitive reduction     Tree data structure  Algorithms      Bellman–Ford algorithm     Dijkstra's algorithm     Ford–Fulkerson algorithm     Kruskal's algorithm     Nearest neighbour algorithm     Prim's algorithm     Depth-first search     Breadth-first search  Subareas      Algebraic graph theory     Geometric graph theory     Extremal graph theory     Probabilistic graph theory     Topological graph theory  Related areas of mathematics      Combinatorics     Group theory     Knot theory     Ramsey theory  Generalizations      Hypergraph     Abstract simplicial complex  Prominent graph theorists      Alon, Noga     Berge, Claude     Bollobás, Béla     Bondy, Adrian John     Brightwell, Graham     Chudnovsky, Maria     Chung, Fan     Dirac, Gabriel Andrew     Erdős, Paul     Euler, Leonhard     Faudree, Ralph     Golumbic, Martin     Graham, Ronald     Harary, Frank     Heawood, Percy John     Kotzig, Anton     Kőnig, Dénes     Lovász, László     Murty, U. S. R.     Nešetřil, Jaroslav     Rényi, Alfréd     Ringel, Gerhard     Robertson, Neil     Seymour, Paul     Szemerédi, Endre     Thomas, Robin     Thomassen, Carsten     Turán, Pál     Tutte, W. T.     Whitney, Hassler Databases SQL and relational databases NoSQL Databases NoSQL, Mongo, Redis NoSQL, Teradata Excel MongoDB sql nosql relatinal database nonrelational database Oracle MySQL  Microsoft SQL Server MongoDB  PostgreSQL DB2 Cassandra  Microsoft Access SQLite Redis  Elasticsearch  Teradata SAP Adaptive Server Solr HBase FileMaker Hive Splunk SAP HANA  MariaDB Neo4j  Informix Memcached Couchbase  Amazon DynamoDB  CouchDB Microsoft Azure SQL Database Netezza Vertica Firebird Riak KV  Ingres MarkLogic dBASE Greenplum Amazon Redshift  Impala DB2 MySQL Oracle PostgreSQL SQLite SQL Server Sybase RethinkDB Berkeley DB memcached redis couchDB mongoDB  With non-relational databases you can store any type of content. Incorporate any kind of data in a single database. Build any feature. Faster. With less money.   Relational (SQL)\tNon-Relational (NoSQL) Stuck. Data now includes rich data types – tweets, videos, podcasts, animated gifs – which are hard, if not impossible, to store in a relational database. Development slows to a crawl, and ops is caught playing whack-a-mole.\tDo the Impossible. NoSQL can incorporate literally any type of data, while providing all the features needed to build content-rich apps. Can’t Scale. Your audience is global, in many countries, speaking many languages, accessing content on many devices. Scaling a relational database is not trivial. And it isn’t cheap.\tScale Big. Scaling is built into the database. It is automatic and transparent. You can scale as your audience grows, both within a data center and across regions. $$$$. Large teams tied up for long periods of time make these applications expensive to build and maintain. Proprietary software and hardware, plus separate databases and file systems needed to manage your content, add to the cost.\t$. More productive teams, plus commodity hardware, make your projects cost 10% what they would with a relational database.   Download the white paper to understand in depth      Why organizations of all sizes are seeking alternatives to legacy relational databases like MySQL, SQL and PostgreSQL     The differences and similarities between NoSQL databases and relational databases     How to evaluate commercial support and community strength when selecting a NoSQL database  1) Relational databases, which can also be called relational database management systems (RDBMS) or SQL databases.  The most popular of these are Microsoft SQL Server, Oracle Database, MySQL, and IBM DB2.  These RDBMS’s are mostly used in large enterprise scenarios, with the exception of MySQL, which is mostly used to store data for web applications, typically as part of the popular LAMP stack (Linux, Apache, MySQL, PHP/ Python/ Perl).  2) Non-relational databases, also called NoSQL databases, the most popular being MongoDB, DocumentDB, Cassandra, Coachbase, HBase, Redis, and Neo4j.  These databases are usually grouped into four categories: Key-value stores, Graph stores, Column stores, and Document stores (see Types of NoSQL databases).  All relational databases can be used to manage transaction-oriented applications (OLTP), and most non-relational databases that are in the categories Document stores and Column stores can also be used for OLTP, adding to the confusion.  OLTP databases can be thought of as “Operational” databases, characterized by frequent, short transactions that include updates and that touch a small amount of data and where concurrency of thousands of transactions is very important (examples including banking applications and online reservations).  Integrity of data is very important so they support ACID transactions (Atomicity, Consistency, Isolation, Durability).  This is opposed to data warehouses, which are considered “Analytical” databases characterized by long, complex queries that touch a large amount of data and require a lot of resources.  Updates are infrequent.  An example is analysis of sales over the past year.  Relational databases usually work with structured data, while non-relational databases usually work with semi-structured data (i.e. XML, JSON).  Let’s look at each group in more detail: Relational Databases  A relational database is organized based on the relational model of data, as proposed by E.F. Codd in 1970.  This model organizes data into one or more tables (or “relations”) of rows and columns, with a unique key for each row.  Generally, each entity type that is described in a database has its own table with the rows representing instances of that type of entity and the columns representing values attributed to that instance.  Since each row in a table has its own unique key, rows in a table can be linked to rows in other tables by storing the unique key of the row to which it should be linked (where such unique key is known as a “foreign key”).  Codd showed that data relationships of arbitrary complexity can be represented using this simple set of concepts.  Virtually all relational database systems use SQL (Structured Query Language) as the language for querying and maintaining the database.  The reasons for the dominance of relational databases are: simplicity, robustness, flexibility, performance, scalability and compatibility in managing generic data.  But to offer all of this, relational databases have to be incredibly complex internally.  For example, a relatively simple SELECT statement could have dozens of potential query execution paths, which a query optimizer would evaluate at run time.  All of this is hidden to users, but under the hood, the RDBMS determines the best “execution plan” to answer requests by using things like cost-based algorithms.  For large databases, especially ones used for web applications, the main concern is scalability.  As more and more applications are created in environments that have massive workloads (i.e. Amazon), their scalability requirements can change very quickly and grow very large.  Relational databases scale well, but usually only when that scaling happens on a single server (“scale-up”).  When the capacity of that single server is reached, you need to “scale-out” and distribute that load across multiple servers, moving into so-called distributed computing.  This is when the complexity of relational databases starts to cause problems with their potential to scale.  If you try to scale to hundreds or thousands of servers the complexities become overwhelming.  The characteristics that make relational databases so appealing are the very same that also drastically reduce their viability as platforms for large distributed systems. Non-relational databases  A NoSQL database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases.  Motivations for this approach include:      Simplicity of design.  Not having to deal with the “impedance mismatch” between the object-oriented approach to write applications and the schema-based tables and rows of a relational database.  For example, storing all the customer order info in one document as opposed to having to join many tables together, resulting in less code to write, debug, and maintain     Better “horizontal” scaling to clusters of machines, which solves the problem when the number of concurrent users skyrockets for applications that are accessible via the web and mobile devices.  Using documents makes it much easier to scale-out as all the info for that customer order is contained in one place as opposed to being spread out on multiple tables.  NoSQL databases automatically spread data across servers without requiring application changes (auto-sharding), meaning that they natively and automatically spread data across an arbitrary number of servers, without requiring the application to even be aware of the composition of the server pool.  Data and query load are automatically balanced across servers, and when a server goes down, it can be quickly and transparently replaced with no application disruption     Finer control over availability.  Servers can be added or removed without application downtime.  Most NoSQL databases support data replication, storing multiple copies of data across the cluster or even across data centers, to ensure high availability and disaster recovery     To easily capture all kinds of data “Big Data” which include unstructured and semi-structured data.  Allowing for a flexible database that can easily and quickly accommodate any new type of data and is not disrupted by content structure changes.  This is because document database are schemaless, allowing you to freely add fields to JSON documents without having to first define changes (schema-on-read instead of schema-on-write).  You can have documents with a different number of fields than other documents.  For example, a patient record that may or may not contain fields that list allergies     Speed.  The data structures used by NoSQL databases (i.e. JSON documents) differ from those used by default in relational databases, making many operations faster in NoSQL than relational databases due to not having to join tables (at the cost of increased storage space due to duplication of data – but storage space is so cheap nowadays this is usually not an issue).  In fact, most NoSQL databases do not even support joins     Cost.  NoSQL databases usually use clusters of cheap commodity servers, while RDBMS tend to rely on expensive proprietary servers and storage systems.  Also, the licenses for RDBMS systems can be quite expensive while many NoSQL databases are open source and therefore free  The particular suitability of a given NoSQL database depends on the problem it must solve.  NoSQL databases are increasingly used in big data and real-time web applications.  They became popular with the introduction of the web, when databases went from a max of a few hundred users on an internal company application to thousands or millions of users on a web application.  NoSQL systems are also called “Not only SQL” to emphasize that they may also support SQL-like query languages.  Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability and partition tolerance.  Some reasons that block adoption of NoSQL stores include the use of low-level query languages, the lack of standardized interfaces, and huge investments in existing SQL.  Also, most NoSQL stores lack true ACID transactions or only support transactions in certain circumstances and at certain levels (e.g., document level).  Finally, RDBMS’s are usually much simpler to use as they have GUI’s where many NoSQL solution use a command-line interface. Comparing the two  One of the most severe limitations of relational databases is that each item can only contain one attribute.  If we use a bank example, each aspect of a customer’s relationship with a bank is stored as separate row items in separate tables.  So the customer’s master details are in one table, the account details are in another table, the loan details in yet another, investments in a different table, and so on.  All these tables are linked to each other through the use of relations such as primary keys and foreign keys.  Non-relational databases, specifically a database’s key-value stores or key-value pairs, are radically different from this model.  Key-value pairs allow you to store several related items in one “row” of data in the same table.  We place the word “row” in quotes because a row here is not really the same thing as the row of a relational table.  For instance, in a non-relational table for the same bank, each row would contain the customer’s details as well as their account, loan and investment details.  All data relating to one customer would be conveniently stored together as one record.  This seems an obviously superior method of storing data, but it has a major drawback: key-value stores, unlike relational databases, cannot enforce relationships between data items.  For instance, in our key-value database, the customer details (name, social security, address, account number, loan processing number, etc.) would all be stored as one data record (instead of being stored in several tables, as in the relational model).  The customer’s transactions (account withdrawals, account deposits, loan repayments, bank charges, etc.) would also be stored as another single data record.  In the relational model, there is an built-in and foolproof method of ensuring and enforcing business logic and rules at the database layer, for instance that a withdrawal is charged to the correct bank account, through primary keys and foreign keys.  In key-value stores, this responsibility falls squarely on the application logic and many people are very uncomfortable leaving this crucial responsibility just to the application.  This is one reason why relational databases will continued to be used.  However, when it comes to web-based applications that use databases, the aspect of rigorously enforcing business logic is often not a top priorities.  The highest priority is the ability to service large numbers of user requests, which are typically read-only queries.  For example, on a site like eBay, the majority of users simply browse and look through posted items (read-only operations).  Only a fraction of these users actually place bids or reserve the items (read-write operations).  And remember, we are talking about millions, sometimes billions, of page views per day.  The eBay site administrators are more interested in quick response time to ensure faster page loading for the site’s users, rather than the traditional priorities of enforcing business rules or ensuring a balance between reads and writes.  Relational-model databases can be tweaked and set up to run large-scale read-only operations through data warehousing, and thus potentially serve a large amount of users who are querying a large amount of data, especially when using relational MPP architectures like Analytics Platform System, Teradata, Oracle Exadata, or IBM Netezza, which all support scaling.  As mentioned before, data warehouses are distinct from typical databases in that they are used for more complex analysis of data.  This differs from the transactional (OLTP) database, whose main use is to support operational systems and offer day-to-day, small scale reporting.  However, the real challenge is the relational model’s lack of scalability when dealing with OLTP applications, or any solution with a lot of individual writes, which is the domain of relational SMP architectures.  This is where non-relational models can really shine.  They can easily distribute their data loads across dozens, hundreds and in extreme cases (think Google search) even thousands of servers.  With each server handling only a small percentage of the total requests from users, response time is very good for each individual user.  Although this distributed computing model can be built for relational databases, it is a real pain to implement, especially when there are a lot of writes (i.e OLTP), requiring techniques like sharding which usually requires significant coding outside of the application’s business logic.  This is because the relational model insists on data integrity at all levels, which must be maintained, even as the data is accessed and modified by several different servers.  This is the reason for the non-relational model as the architecture of choice for web applications such as cloud-computing and social networking.  So in summary, RDBMS’s suffer from no horizontal scaling for high transaction loads (millions of read-writes), while NoSQL databases solve high transaction loads but at the cost of data integrity and joins.  Keep in mind many solutions will use a combination of relational and non-relational databases (see What is Polyglot Persistence?).  Also keep in mind that you may not need the performance of a non-relational database and instead just going with storing files in HDFS and using Apache Hive will be enough (Apache Hive is a data warehouse infrastructure built on top of Hadoop for providing data summarization, query, and analysis that it provides via an SQL-like language called HiveQL).  And to end on a note that adds to the confusion, we have a another category forming called NewSQL: NewSQL is a class of modern RDBMS’s that seek to provide the same scalable performance of NoSQL systems for OLTP read-write workloads while still maintaining the ACID guarantees of a traditional relational database system.  The disadvantages is they are not for OLAP-style queries, and they are inappropriate for databases over a few terabytes.  Examples include VoltDB, NuoDB, MemSQL, SAP HANA, Splice Machine, Clustrix, and Altibase.", "category": "Edison", "id": 137}
{"skillName": "DSRMP01", "skillText": "The selection of the research method is crucial for what conclusions you can make about a phenomenon. It affects what you can say about the cause and factors influencing the phenomenon.  It is also important to choose a research method which is within the limits of what the researcher can do. Time, money, feasibility, ethics and availability to measure the phenomenon correctly are examples of issues constraining the research. Choosing the Measurement  Choosing the scientific measurements are also crucial for getting the correct conclusion. Some measurements might not reflect the real world, because they do not measure the phenomenon as it should. Results Significance Test  To test a hypothesis, quantitative research uses significance tests to determine which hypothesis is right.  The significance test can show whether the null hypothesis is more likely correct than the research hypothesis. Research methodology in a number of areas like social sciences depends heavily on significance tests.  A significance test may even drive the research process in a whole new direction, based on the findings.  The t-test (also called the Student's T-Test) is one of many statistical significance tests, which compares two supposedly equal sets of data to see if they really are alike or not. The t-test helps the researcher conclude whether a hypothesis is supported or not. Drawing Conclusions  Drawing a conclusion is based on several factors of the research process, not just because the researcher got the expected result. It has to be based on the validity and reliability of the measurement, how good the measurement was to reflect the real world and what more could have affected the results.  The observations are often referred to as 'empirical evidence' and the logic/thinking leads to the conclusions. Anyone should be able to check the observation and logic, to see if they also reach the same conclusions.  Errors of the observations may stem from measurement-problems, misinterpretations, unlikely random events etc.  A common error is to think that correlation implies a causal relationship. This is not necessarily true. Generalization  Generalization is to which extent the research and the conclusions of the research apply to the real world. It is not always so that good research will reflect the real world, since we can only measure a small portion of the population at a time.  Generalization in Research   Validity and Reliability  Validity refers to what degree the research reflects the given research problem, while Reliability refers to how consistent a set of measurements are.  Validity and Reliability  Types of validity:      External Validity     Population Validity     Ecological Validity     Internal Validity     Content Validity     Face Validity     Construct Validity     Convergent and Discriminant Validity     Test Validity     Criterion Validity     Concurrent Validity     Predictive Validity  A definition of reliability may be \"Yielding the same or compatible results in different clinical experiments or statistical trials\" (the free dictionary). Research methodology lacking reliability cannot be trusted. Replication studies are a way to test reliability.  Types of Reliability:      Test-Retest Reliability     Interrater Reliability     Internal Consistency Reliability     Instrument Reliability     Statistical Reliability     Reproducibility  Both validity and reliability are important aspects of the research methodology to get better explanations of the world. Errors in Research  Logically, there are two types of errors when drawing conclusions in research:  Type 1 error is when we accept the research hypothesis when the null hypothesis is in fact correct.  Type 2 error is when we reject the research hypothesis even if the null hypothesis is wrong. Research comprises \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\"[1] It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects, or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life,technological,etc.  Contents      1 Forms of research     2 Etymology     3 Definitions     4 Steps in conducting research     5 Scientific research     6 Historical method     7 Research methods         7.1 Research method controversies             7.1.1 Quantitative vs. Qualitative war             7.1.2 Anti-methodology             7.1.3 Methodological academic imperialism     8 Professionalisation         8.1 In Russia     9 Publishing     10 Research funding     11 Original research         11.1 Different forms     12 Artistic research     13 See also     14 References     15 Further reading     16 External links  Forms of research  Scientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, such as business schools, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).[2]  Research in the humanities involves different methods such as for example hermeneutics and semiotics, and a different, more relativist epistemology. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past.  Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth. Etymology Aristotle, 384 BC – 322 BC, - one of the early figures in the development of the scientific method.[3]  The word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'.[4] The earliest recorded use of the term was in 1577.[4] Definitions  Research has been defined in a number of different ways.  A broad definition of research is given by Martyn Shuttleworth - \"In the broadest sense of the word, the definition of research includes any gathering of data, information and facts for the advancement of knowledge.\"[5]  Another definition of research is given by Creswell who states that - \"Research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: Pose a question, collect data to answer the question, and present an answer to the question.[6]  The Merriam-Webster Online Dictionary defines research in more detail as \"a studious inquiry or examination; especially investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".[4] Steps in conducting research  Research is often conducted using the hourglass model structure of research.[7] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[8]      Identification of research problem     Literature review     Specifying the purpose of research     Determine specific research questions     Specification of a conceptual framework, usually a set of hypotheses[9]     Choice of a methodology (for data collection)     Data collection     Verify data     Analyzing and interpreting the data     Reporting and evaluating research     Communicating the research findings and, possibly, recommendations  The steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[10] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[11] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in confirming or failing to reject the Null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the flip approach: starting with articulating findings and discussion of them, moving \"up\" to identification research problem that emerging in the findings and literature review introducing the findings. The flip approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings fully emerged and interpreted.  Rudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"[12]  Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrase in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"[13] Scientific research Main article: Scientific method Primary scientific research being carried out at the Microscopy Laboratory of the Idaho National Laboratory. Scientific research equipment at MIT.  Generally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:      Observations and Formation of the topic: Consists of the subject area of ones interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.     Hypothesis: A testable prediction which designates the relationship between two or more variables.     Conceptual definition: Description of a concept by relating it to other concepts.     Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.     Gathering of data: Consists of identifying a population and selecting samples, gathering information from and/or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.     Analysis of data: Involves breaking down the individual pieces of data in order to draw conclusions about it.     Data Interpretation: This can be represented through tables, figures and pictures, and then described in words.     Test, revising of hypothesis     Conclusion, reiteration if necessary  A common misconception is that a hypothesis will be proven (see, rather, Null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.  A useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which state no relationship or difference between the independent or dependent variables. A null hypothesis uses a sample of all possible people to make a conclusion about the population.[14] Historical method Main article: Historical method German historian Leopold von Ranke (1795-1886), considered to be one of the founders of modern source-based history.  The historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[15]      Identification of origin date     Evidence of localization     Recognition of authorship     Analysis of data     Identification of integrity     Attribution of credibility  Research methods The research room at the New York Public Library, an example of secondary research in progress. Maurice Hilleman is credited with saving more lives than any other scientist of the 20th century.[16]  The goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):      Exploratory research, which helps to identify and define a problem or question.     Constructive research, which tests theories and proposes solutions to a problem or question.     Empirical research, which tests the feasibility of a solution using empirical evidence.  There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:  Qualitative research     Understanding of human behavior and the reasons that govern such behavior. Asking a broad question and collecting data in the form of words, images, video etc that is analyzed and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time-consuming, and typically limited to a single set of research subjects.[citation needed] Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses.[citation needed] Qualitative research is linked with the philosophical and theoretical stance of social constructionism.  Quantitative research     Systematic empirical investigation of quantitative properties and phenomena and their relationships. Asking a narrow question and collecting numerical data to analyze utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[17] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.  The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories.[citation needed] These methods produce results that are easy to summarize, compare, and generalize.[citation needed] Quantitative research is concerned with testing hypotheses derived from theory and/or being able to estimate the size of a phenomenon of interest. Depending on the research question, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics in order to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[18]  In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[19]  Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[20]  Big data has brought big impacts on research methods that now researchers do not put much effort on data collection, and also methods to analyze easily available huge amount of data have also changed.[21]  Nonempirical refers to an approach that is grounded in theory as opposed to using observation and experimentation to achieve the outcome. As such, nonempirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool existing and established knowledge. Nonempirical is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose within life and in science. A simple example of a nonempirical task could the prototyping of a new drug using a differentiated application of existing knowledge; similarly, it could be the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Empirical research on the other hand seeks to create new knowledge through observations and experiments in which established knowledge can either be contested or supplements. Research method controversies  There have been many controversies about research methods stemmed from a philosophical positivism promise to distinguish the science from other practices (especially religion) by its method. This promise leads to methodological hegemony and methodology wars where diverse researchers, often coming from opposing paradigms, try to impose their own methodology on the entire field or even on the science practice in general as the only legitimate one.[citation needed] Quantitative vs. Qualitative war Anti-methodology  According to this view, general scientific methodology does not exist and attempts to impose it on scientists is counterproductive. Each particular research with its emerging particular inquiries requires and should produce its own way (method) of researching. Similar to the art practice, the notion of methodology has to be replaced with the notion of research mastery.[22] Methodological academic imperialism  Epistemologies of different national sciences and cultural communities may differ and, thus, they may produce different methods of research. For example, psychological research in Russia tends to be rooted in philosophy while in the US and UK in empirism.[23][24][25] Rich countries (and dominant cultural communities within them) and their national sciences may dominate scientific discourse through funding and publications. This academic hegemony can translate into impositions of certain research methodologies through the gatekeeping process of international academic publications, conference presentation selection, institutional review boards, and funding.[26] Professionalisation Globe icon. \tThe examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (January 2014) (Learn how and when to remove this template message) See also: Academic ranks, Academics, and Scientists  In several national and private academic systems, the professionalization of research has resulted in formal job titles. In Russia  In present-day Russia, the former Soviet Union and in some Post-Soviet states the term researcher (Russian: Научный сотрудник, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc.  The following ranks are known:      Junior Researcher (Junior Research Associate)     Researcher (Research Associate)     Senior Researcher (Senior Research Associate)     Leading Researcher (Leading Research Associate)[27]     Chief Researcher (Chief Research Associate)  Publishing Cover of the first issue of Nature, 4 November 1869.  Academic publishing describes a system that is necessary in order for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field, and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.  Most established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields; from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently.[28] It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its factors in order to prevent the publication of unproven findings.[29] Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access.[30] There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web. Research funding Main article: Funding of science  Most funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA[31] and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research, but also as a source of merit.  The Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources. Original research Original research redirects here, for the Wikipedia policy see Wikipedia:No original research  Original research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).[32][33] Different forms  Original research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[34]  The degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review.[35] Graduate students are commonly required to perform original research as part of a dissertation.[36] Artistic research  The controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[37] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[38]  Artistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods and criticality. Through presented documentation, the insights gained shall be placed in a context.\"[39] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[40] For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.[41]  According to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\".[42] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[43]  The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[44][45] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[46][47][48] a searchable, documentary database of artistic research, to which anyone can contribute.  Patricia Leavy addresses eight arts-based research (ABR) genres, they are: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[49] See also      European Charter for Researchers     Undergraduate research     Internet research     List of countries by research and development spending     Open research     Operations research     Participatory action research     Primary research     Psychological research methods     Research-intensive cluster     Scholarly research     Secondary research     Society for Artistic Research     Timeline of the history of scientific method   The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge.[2] To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.[3] The Oxford Dictionaries Online define the scientific method as \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses.\"[4]  The scientific method is an ongoing process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.[1]  Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions.[5][6] A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.[7]  The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis.[8] Experiments can take place in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars, and so on. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles.[9] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order.[10] Some philosophers and scientists have argued that there is no scientific method. For example, Lee Smolin[11] and Paul Feyerabend (in his Against Method). Nola and Sankey remark that \"For some, the whole idea of a theory of scientific method is yester-year’s debate ...\".[12] Contents      1 Overview         1.1 Process         1.2 DNA example         1.3 Other components     2 Scientific inquiry         2.1 Properties of scientific inquiry         2.2 Beliefs and biases     3 Elements of the scientific method         3.1 Characterizations         3.2 Hypothesis development         3.3 Predictions from the hypothesis         3.4 Experiments         3.5 Evaluation and improvement         3.6 Confirmation     4 Models of scientific inquiry         4.1 Classical model         4.2 Pragmatic model     5 Communication and community         5.1 Peer review evaluation         5.2 Documentation and replication         5.3 Dimensions of practice     6 Philosophy and sociology of science         6.1 Role of chance in discovery     7 History     8 Relationship with mathematics     9 Relationship with statistics     10 See also         10.1 Problems and issues         10.2 History, philosophy, sociology     11 Notes     12 References     13 Further reading     14 External links  Overview      The DNA example below is a synopsis of this method  Ibn al-Haytham (Alhazen), 965–1039 Iraq. A polymath, considered by some to be the father of modern scientific methodology, due to his emphasis on experimental data and reproducibility of its results.[13][14] Johannes Kepler (1571–1630). \"Kepler shows his keen logical sense in detailing the whole process by which he finally arrived at the true orbit. This is the greatest piece of Retroductive reasoning ever performed.\" – C. S. Peirce, c. 1896, on Kepler's reasoning through explanatory hypotheses[15] According to Morris Kline,[16] \"Modern science owes its present flourishing state to a new scientific method which was fashioned almost entirely by Galileo Galilei\" (1564−1642). Dudley Shapere[17] takes a more measured view of Galileo's contribution.  The scientific method is the process by which science is carried out.[18] As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.[19][20][21][22][23][24] This model can be seen to underlay the scientific revolution.[25] One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them,[26] an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences.[27] The current method is based on a hypothetico-deductive model[28] formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below). Process  The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct.[5] There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles.[29] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), \"invention, sagacity, [and] genius\"[10] are required at every step. Formulation of a question  The question can refer to the explanation of a specific observation, as in \"Why is the sky blue?\", but can also be open-ended, as in \"How can I design a drug to cure this particular disease?\" This stage frequently involves looking up and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.[30] Hypothesis  A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's \"DNA makes RNA makes protein\",[31] or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested. Prediction  This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).[32] Testing  This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk.[8] Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually. Analysis  This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question. DNA example DNA icon (25x25).png \tThe basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:      Question: Previous investigation of DNA had determined its chemical composition (the four nucleotides), the structure of each individual nucleotide, and other properties. It had been identified as the carrier of genetic information by the Avery–MacLeod–McCarty experiment in 1944,[33] but the mechanism of how genetic information was stored in DNA was unclear.     Hypothesis: Linus Pauling, Francis Crick and James D. Watson hypothesized that DNA had a helical structure.[34]     Prediction: If DNA had a helical structure, its X-ray diffraction pattern would be X-shaped.[35][36] This prediction was determined using the mathematics of the helix transform, which had been derived by Cochran, Crick and Vand[37] (and independently by Stokes). This prediction was a mathematical construct, completely independent from the biological problem at hand.     Experiment: Rosalind Franklin crystallized pure DNA and performed X-ray diffraction to produce photo 51. The results showed an X-shape.     Analysis: When Watson saw the detailed diffraction pattern, he immediately recognized it as a helix.[38][39] He and Crick then produced their model, using this information along with the previously known information about DNA's composition and about molecular interactions such as hydrogen bonds.[40]  The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article. Other components  The scientific method also includes other components required even when all the iterations of the steps above have been completed:[41] Replication  If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.[42] External review  The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.[43] Data recording and sharing  Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others.[44] Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.[45] Scientific inquiry  Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that can be used to predict the results of future experiments. This allows scientists to gain a better understanding of the topic being studied, and later be able to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it is to continue explaining a body of evidence better than its alternatives. The most successful explanations, which explain and make accurate predictions in a wide range of circumstances, are often called scientific theories.  Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding is typically the result of a gradual process of development over time, sometimes across different domains of science.[46] Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question is more powerful than its alternatives at explaining the evidence. Often the explanations are altered over time, or explanations are combined to produce new explanations. Properties of scientific inquiry  Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to be related to how long it has persisted without major alteration to its core principles.  Theories can also subject to subsumption by other theories. For example, thousands of years of scientific observations of the planets were explained almost perfectly by Newton's laws. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicting and explaining other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.[47]  Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors.[47] For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world;[48][49] its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology. Beliefs and biases Flying gallop falsified; see image below Muybridge's photographs of The Horse in Motion, 1878, were used to answer the question whether all four feet of a galloping horse are ever off the ground at the same time. This demonstrates a use of photography in science.  Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).  A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.[50] Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true.[2] In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic,[51] sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe.[52][53] Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.[54] Elements of the scientific method  There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.      Four essential elements[55][56][57] of the scientific method[58] are iterations,[59][60] recursions,[61] interleavings, or orderings of the following:          Characterizations (observations,[62] definitions, and measurements of the subject of inquiry)         Hypotheses[63][64] (theoretical, hypothetical explanations of observations and measurements of the subject)[65]         Predictions (reasoning including deductive reasoning[66] from the hypothesis or theory)         Experiments[67] (tests of all of the above)  Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as \"the scientific method\".[68]  The scientific method is not a single recipe: it requires intelligence, imagination, and creativity.[69] In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.  A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:[70]      Define a question     Gather information and resources (observe)     Form an explanatory hypothesis     Test the hypothesis by performing an experiment and collecting data in a reproducible manner     Analyze the data     Interpret the data and draw conclusions that serve as a starting point for new hypothesis     Publish results     Retest (frequently done by other scientists)  The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.  While this schema outlines a typical hypothesis/testing method,[71] it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced. Characterizations  The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.  The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.      I am not accustomed to saying anything with certainty after only one or two observations.     — Andreas Vesalius, (1546)[72]  Uncertainty  Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken. Definition  Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or \"idealized\" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of \"mass\" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.  The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.  New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, \"I do not define time, space, place and motion, as being well known to all.\" Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood.[73] In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them. DNA-characterizations DNA icon (25x25).png  The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle).[33] But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle.[74] ..2. DNA-hypotheses Another example: precession of Mercury Precession of the perihelion (exaggerated)  The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century. Hypothesis development Main article: Hypothesis formation  A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.  Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.  Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the \"irritation of doubt\" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a \"flash of inspiration\", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.  William Glen observes that      the success of a hypothesis, or its service to science, lies not simply in its perceived \"truth\", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.[75]  In general scientists tend to look for theories that are \"elegant\" or \"beautiful\". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses. DNA-hypotheses DNA icon (25x25).png  Linus Pauling proposed that DNA might be a triple helix.[76] This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong[77] and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions Predictions from the hypothesis Main article: Prediction in science  Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.  It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.  If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science. DNA-predictions DNA icon (25x25).png  James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'.[36][78] This prediction followed from the work of Cochran, Crick and Vand[37] (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.  In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, \"It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material\".[79] ..4. DNA-experiments Another example: general relativity Einstein's prediction (1907): Light bends in a gravitational field  Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.[80] Experiments Main article: Experiment  Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is.[81] Factor analysis is one technique for discovering the important factor in an effect.  Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.  Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).[82] DNA-experiments DNA icon (25x25).png  Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape  and was able to confirm the structure was helical.[38][39] This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations Evaluation and improvement  The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.  Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility. DNA-iterations DNA icon (25x25).png  After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts,[83][84][85] Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it.[40][86] They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example Confirmation  Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.[87]  To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center. Models of scientific inquiry Main article: Models of scientific inquiry Classical model  The classical model of scientific inquiry derives from Aristotle,[88] who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy. Pragmatic model See also: Pragmatic theory of truth  In 1877,[19] Charles Sanders Peirce (/ˈpɜːrs/ like \"purse\"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless.[89] He outlined four methods of settling opinion, ordered from least to most successful:      The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.[90]     The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.     The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of \"what is agreeable to reason.\" Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.     The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.  Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research,[91] which in turn should not be trammeled by the other methods and practical ends; reason's \"first rule\" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry.[92] The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.[19][22]  For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points.[93] In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to \"Do the science.\" Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge.[94] As inference, \"logic is rooted in the social principle\" since it depends on a standpoint that is, in a sense, unlimited.[95]  Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in \"A Neglected Argument\"[5] except as otherwise noted):      Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the \"facile and natural\", as by Galileo's natural light of reason and as distinct from \"logical simplicity\". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths.[96] Coordinative method leads from abducing a plausible hypothesis to judging it for its testability[97] and for how its trial would economize inquiry itself.[98] Peirce calls his pragmatism \"the logic of abduction\".[99] His pragmatic maxim is: \"Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object\".[93] His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity.[100] One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.[98]     Deduction. Two stages:         Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.         Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, Theorematic.     Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general[93]) that the real is only the object of the final opinion to which adequate investigation would lead;[101] anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:         Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.         Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters;[102] if quantitative, then dependent on measurements, or on statistics, or on countings.         Sentential Induction. \"...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result\".  Communication and community See also: Scientific community and Scholarly communication  Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment. Peer review evaluation  Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of \"groupthink\" can interfere with open and fair deliberation of some new research.[103] Documentation and replication Main article: Reproducibility  Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results. Archiving  Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery. Data sharing  When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research. Limitations  Since it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'. Dimensions of practice Further information: Rhetoric of science  The primary constraints on contemporary science are:      Publication, i.e. Peer review     Resources (mostly funding)  It has not always been like this: in the old days of the \"gentleman scientist\" funding (and to a lesser extent publication) were far weaker constraints.  Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to \"good scientific practice\" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines  for Nature. Philosophy and sociology of science See also: Philosophy of science and Sociology of science  Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world.[104] These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.[105]  Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.  Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the \"theory laden\" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description.[106] He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a \"different\" sun rise despite the same physiological phenomenon. Kuhn[107] and Feyerabend[108] acknowledge the pioneering significance of his work.  Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the \"route from theory to measurement can almost never be traveled backward\". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that \"once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed\".[109]  Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'.[110] Criticisms such as his led to the strong programme, a radical approach to the sociology of science.  The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.[111] Role of chance in discovery Main article: Role of chance in scientific discoveries  Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky.[112] Louis Pasteur is credited with the famous saying that \"Luck favours the prepared mind\", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected.[112][113] This is what Nassim Nicholas Taleb calls \"Anti-fragility\"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.[23]  Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.[112][113] History Main article: History of scientific method See also: Timeline of the history of scientific method  Accuracy dispute \tThis article appears to contradict the article History of scientific method. Please see discussion on the linked talk page. Please do not remove this message until the contradictions are resolved. (June 2015) \tThis section may contain an excessive amount of intricate detail that may only interest a specific audience. Please help by spinning off or relocating any relevant information, and removing excessive detail that may be against Wikipedia's inclusion policy. (June 2015) (Learn how and when to remove this template message) Aristotle, 384 BCE – 322 BCE. \"As regards his method, Aristotle is recognized as the inventor of scientific method because of his refined analysis of logical implications contained in demonstrative discourse, which goes well beyond natural logic and does not owe anything to the ones who philosophized before him.\" – Riccardo Pozzo[114]  The development of the scientific method emerges in the history of science itself. Ancient Egyptian documents describe empirical methods in astronomy,[115] mathematics,[116] and medicine.[117] The Greeks made contributions to the scientific method, most notably through Aristotle in his six works of logic collected as the Organon. Aristotle's inductive-deductive method used inductions from observations to infer general principles, deductions from those principles to check against further observations, and more cycles of induction and deduction to continue the advance of knowledge[118]  According to Karl Popper, Parmenides (fl. 5th century BCE) had conceived an axiomatic-deductive method.[119] According to David Lindberg, Aristotle (4th century BCE) wrote about the scientific method even if he and his followers did not actually follow what he said.[67] Lindberg also notes that Ptolemy (2nd century CE) and Ibn al-Haytham (11th century CE) are among the early examples of people who carried out scientific experiments.[120] Also, John Losee writes that \"the Physics and the Metaphysics contain discussions of certain aspects of scientific method\", of which, he says \"Aristotle viewed scientific inquiry as a progression from observations to general principles and back to observations.\"[121]  Early Christian leaders such as Clement of Alexandria (150–215) and Basil of Caesarea (330–379) encouraged future generations to view the Greek wisdom as \"handmaidens to theology\" and science was considered a means to more accurate understanding of the Bible and of God.[122]:pp.4–5 Augustine of Hippo (354–430) who contributed great philosophical wealth to the Latin Middle Ages, advocated the study of science and was wary of philosophies that disagreed with the Bible, such as astrology and the Greek belief that the world had no beginning.[122]:p.5 This Christian accommodation with Greek science \"laid a foundation for the later widespread, intensive study of natural philosophy during the Late Middle Ages.\"[122]:pp.8,9 However, the division of Latin-speaking Western Europe from the Greek-speaking East,[122]:p.18 followed by barbarian invasions, the Plague of Justinian, and the Islamic conquests,[123] resulted in the West largely losing access to Greek wisdom.  By the 8th century Islam had conquered the Christian lands[124] of Syria, Iraq, Iran and Egypt.[125] This swift conquest further severed Western Europe from many of the great works of Aristotle, Plato, Euclid and others, many of which were housed in the great library of Alexandria. Having come upon such a wealth of knowledge, the Arabs, who viewed non-Arab languages as inferior, even as a source of pollution,[126] employed conquered Christians and Jews to translate these works from the native Greek and Syriac into Arabic.[127]  Thus equipped, Arab philosopher Alhazen (Ibn al-Haytham) performed optical and physiological experiments, reported in his manifold works, the most famous being Book of Optics (1021).[128] He was thus a forerunner of scientific method, having understood that a controlled environment involving experimentation and measurement is required in order to draw educated conclusions. Other Arab polymaths of the same era produced copious works on mathematics, philosophy, astronomy and alchemy. Most stuck closely to Aristotle, being hesitant to admit that some of Aristotle's thinking was errant,[129] while others strongly criticized him.  During these years, occasionally a paraphrased translation from the Arabic, which itself had been translated from Greek and Syriac, might make its way to the West for scholarly study. It was not until 1204, during which the Latins conquered and took Constantinople from the Byzantines in the name of the fourth Crusade, that a renewed scholarly interest in the original Greek manuscripts began to grow. Due to the new easier access to the libraries of Constantinople by Western scholars, a certain revival in the study and analysis of the original Greek texts by Western scholars began.[130] From that point a functional scientific method that would launch modern science was on the horizon.  Grosseteste (1175–1253), an English statesman, scientist and Christian theologian, was \"the principal figure\" in bringing about \"a more adequate method of scientific inquiry\" by which \"medieval scientists were able eventually to outstrip their ancient European and Muslim teachers\" (Dales 1973, p. 62). ... His thinking influenced Roger Bacon, who spread Grosseteste's ideas from Oxford to the University of Paris during a visit there in the 1240s. From the prestigious universities in Oxford and Paris, the new experimental science spread rapidly throughout the medieval universities: \"And so it went to Galileo, William Gilbert, Francis Bacon, William Harvey, Descartes, Robert Hooke, Newton, Leibniz, and the world of the seventeenth century\" (Crombie 1953, p. 15). \"So it went to us as well \" (Gauch 2003, pp. 52–53). Roger Bacon (c. 1214 – c. 1292) is sometimes credited as one of the earliest European advocates of the modern scientific method inspired by the works of Aristotle.[131]  Roger Bacon (c. 1214 – c. 1292), an English thinker and experimenter, is recognized by many to be the father of modern scientific method. His view that mathematics was essential to a correct understanding of natural philosophy was considered to be 400 years ahead of its time.[132]:2 He was viewed as \"a lone genius proclaiming the truth about time,\" having correctly calculated the calendar[132]:3 His work in optics provided the platform on which Newton, Descartes, Huygens and others later transformed the science of light. Bacon's groundbreaking advances were due largely to his discovery that experimental science must be based on mathematics. (186–187) His works Opus Majus and De Speculis Comburentibus contain many \"carefully drawn diagrams showing Bacon's meticulous investigations into the behavior of light.\"[132]:66 He gives detailed descriptions of systematic studies using prisms and measurements by which he shows how a rainbow functions.[132]:200  Others who advanced scientific method during this era included Albertus Magnus (c. 1193 – 1280), Theodoric of Freiberg, (c. 1250 – c. 1310), William of Ockham (c. 1285 – c. 1350), and Jean Buridan (c. 1300 – c. 1358). These were not only scientists but leaders of the church – Christian archbishops, friars and priests.  By the late 15th century, the physician-scholar Niccolò Leoniceno was finding errors in Pliny's Natural History. As a physician, Leoniceno was concerned about these botanical errors propagating to the materia medica on which medicines were based.[133] To counter this, a botanical garden was established at Orto botanico di Padova, University of Padua (in use for teaching by 1546), in order that medical students might have empirical access to the plants of a pharmacopia. The philosopher and physician Francisco Sanches was led by his medical training at Rome, 1571–73, and by the philosophical skepticism recently placed in the European mainstream by the publication of Sextus Empiricus' \"Outlines of Pyrrhonism\", to search for a true method of knowing (modus sciendi), as nothing clear can be known by the methods of Aristotle and his followers[134] – for example, syllogism fails upon circular reasoning. Following the physician Galen's method of medicine, Sanches lists the methods of judgement and experience, which are faulty in the wrong hands,[135] and we are left with the bleak statement That Nothing is Known (1581). This challenge was taken up by René Descartes in the next generation (1637), but at the least, Sanches warns us that we ought to refrain from the methods, summaries, and commentaries on Aristotle, if we seek scientific knowledge. In this, he is echoed by Francis Bacon, also influenced by skepticism; Sanches cites the humanist Juan Luis Vives who sought a better educational system, as well as a statement of human rights as a pathway for improvement of the lot of the poor.  The modern scientific method crystallized no later than in the 17th and 18th centuries. In his work Novum Organum (1620) – a reference to Aristotle's Organon – Francis Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism.[136] Then, in 1637, René Descartes established the framework for scientific method's guiding principles in his treatise, Discourse on Method. The writings of Alhazen, Bacon and Descartes are considered critical in the historical development of the modern scientific method, as are those of John Stuart Mill.[137]  In the late 19th century, Charles Sanders Peirce proposed a schema that would turn out to have considerable influence in the development of current scientific methodology generally. Peirce accelerated the progress on several fronts. Firstly, speaking in broader context in \"How to Make Our Ideas Clear\" (1878)  , Peirce outlined an objectively verifiable method to test the truth of putative knowledge on a way that goes beyond mere foundational alternatives, focusing upon both deduction and induction. He thus placed induction and deduction in a complementary rather than competitive context (the latter of which had been the primary trend at least since David Hume, who wrote in the mid-to-late 18th century). Secondly, and of more direct importance to modern method, Peirce put forth the basic schema for hypothesis/testing that continues to prevail today. Extracting the theory of inquiry from its raw materials in classical logic, he refined it in parallel with the early development of symbolic logic to address the then-current problems in scientific reasoning. Peirce examined and articulated the three fundamental modes of reasoning that, as discussed above in this article, play a role in inquiry today, the processes that are currently known as abductive, deductive, and inductive inference. Thirdly, he played a major role in the progress of symbolic logic itself – indeed this was his primary specialty.  Beginning in the 1930s, Karl Popper argued that there is no such thing as inductive reasoning.[138] All inferences ever made, including in science, are purely[139] deductive according to this view. Accordingly, he claimed that the empirical character of science has nothing to do with induction – but with the deductive property of falsifiability that scientific hypotheses have. Contrasting his views with inductivism and positivism, he even denied the existence of the scientific method: \"(1) There is no method of discovering a scientific theory (2) There is no method for ascertaining the truth of a scientific hypothesis, i.e., no method of verification; (3) There is no method for ascertaining whether a hypothesis is 'probable', or probably true\".[140] Instead, he held that there is only one universal method, a method not particular to science: The negative method of criticism, or colloquially termed trial and error. It covers not only all products of the human mind, including science, mathematics, philosophy, art and so on, but also the evolution of life. Following Peirce and others, Popper argued that science is fallible and has no authority.[140] In contrast to empiricist-inductivist views, he welcomed metaphysics and philosophical discussion and even gave qualified support to myths[141] and pseudosciences.[142] Popper's view has become known as critical rationalism.  Although science in a broad sense existed before the modern era, and in many historical civilizations (as described above), modern science is so distinct in its approach and successful in its results that it now defines what science is in the strictest sense of the term.[143] Relationship with mathematics  Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.[144]  Mathematical work and scientific work can inspire each other.[145] For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).  Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.  George Pólya's work on problem solving,[146] the construction of mathematical proofs, and heuristic[147][148] show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps. \tMathematical method \tScientific method 1 \tUnderstanding \tCharacterization from experience and observation 2 \tAnalysis \tHypothesis: a proposed explanation 3 \tSynthesis \tDeduction: prediction from the hypothesis 4 \tReview/Extend \tTest and experiment  In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus,[149] involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details[150] of the proof; review involves reconsidering and re-examining the result and the path taken to it.  Gauss, when asked how he came about his theorems, once replied \"durch planmässiges Tattonieren\" (through systematic palpable experimentation).[151]  Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work.[152] In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)  Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.[153] Relationship with statistics  The scientific method has been extremely successful in bringing the world out of medieval times, especially once it was combined with industrial processes.[154] However, when the scientific method employs statistics as part of its arsenal, there are a number of both mathematical and practical issues that can have a deleterious effect on the reliability of the output of the scientific methods. This is outlined in detail in the most downloaded 2005 scientific paper \"Why Most Published Research Findings Are False\"[155] ever by John Ioannidis.  The particular points raised are statistical (\"The smaller the studies conducted in a scientific field, the less likely the research findings are to be true\" and \"The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\") and economical (\"The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true\" and \"The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\") Hence: \"Most research findings are false for most research designs and for most fields\" and \"As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings.\" However: \"Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds,\" which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries. See also      Armchair theorizing     Confirmability     Contingency     Empirical limits in science     Evidence-based medicine     Fuzzy logic     Inquiry     Information theory     Logic     Methodology         Historical         Philosophical         Phronetic         Scholarly     Operationalization     Quantitative research     Replication crisis     Social research     Statistical hypothesis testing     Strong inference     Testability          Science is a systematic and logical approach to discovering how things in the universe work. It is also the body of knowledge accumulated through the discoveries about all the things in the universe.   The word \"science\" is derived from the Latin word scientia, which is knowledge based on demonstrable and reproducible data, according to the Merriam-Webster Dictionary. True to this definition, science aims for measurable results through testing and analysis. Science is based on fact, not opinion or preferences. The process of science is designed to challenge ideas through research. One important aspect of the scientific process is that it is focuses only on the natural world, according to the University of California. Anything that is considered supernatural does not fit into the definition of science. The scientific method  When conducting research, scientists use the scientific method to collect measurable, empirical evidence in an experiment related to a hypothesis (often in the form of an if/then statement), the results aiming to support or contradict a theory.  The steps of the scientific method go something like this:      Make an observation or observations.     Ask questions about the observations and gather information.     Form a hypothesis — a tentative description of what’s been observed, and make predictions based on that hypothesis.     Test the hypothesis and predictions in an experiment that can be reproduced.     Analyze the data and draw conclusions; accept or reject the hypothesis or modify the hypothesis if necessary.     Reproduce the experiment until there are no discrepancies between observations and theory. “Replication of methods and results is my favorite step in the scientific method,\" Moshe Pritsker, a former post-doctoral researcher at Harvard Medical School and CEO of JoVE, told Live Science. \"The reproducibility of published experiments is the foundation of science. No reproducibility – no science.\"  Some key underpinnings to the scientific method:      The hypothesis must be testable and falsifiable, according to North Carolina State University. Falsifiable means that there must be a possible negative answer to the hypothesis.     Research must involve deductive reasoning and inductive reasoning. Deductive reasoning is the process of using true premises to reach a logical true conclusion while inductive reasoning takes the opposite approach.     An experiment should include a dependent variable (which does not change) and an independent variable (which does change).     An experiment should include an experimental group and a control group. The control group is what the experimental group is compared against.  Scientific theories and laws  The scientific method and science in general can be frustrating. A theory is almost never proven, though a few theories do become scientific laws. One example would be the laws of conservation of energy, which is the first law of thermodynamics. Dr. Linda Boland, a neurobiologist and chairperson of the biology department at the University of Richmond, Virginia, told Live Science that this is her favorite scientific law. \"This is one that guides much of my research on cellular electrical activity and it states that energy cannot be created nor destroyed, only changed in form. This law continually reminds me of the many forms of energy,\" she said.  Laws are generally considered to be without exception, though some laws have been modified over time after further testing found discrepancies. This does not mean theories are not meaningful. For a hypothesis to become a theory, rigorous testing must occur, typically across multiple disciplines by separate groups of scientists. Saying something is “just a theory” is a layperson’s term that has no relationship to science. To most people a theory is a hunch. In science a theory is the framework for observations and facts, Jaime Tanner, a professor of biology at Marlboro College, told Live Science Key Concepts of the Scientific Method  Research Methodology  There are several important aspects to research methodology. This is a summary of the key concepts in scientific research and an attempt to erase some common misconceptions in science.  Steps of the scientific method are shaped like an hourglass - starting from general questions, narrowing down to focus on one specific aspect, and designing research where we can observe and analyze this aspect. At last, we conclude and generalize to the real world. Formulating a Research Problem  Researchers organize their research by formulating and defining a research problem. This helps them focus the research process so that they can draw conclusions reflecting the real world in the best possible way.   Hypothesis  In research, a hypothesis is a suggested explanation of a phenomenon.  A null hypothesis is a hypothesis which a researcher tries to disprove. Normally, the null hypothesis represents the current view/explanation of an aspect of the world that the researcher wants to challenge.  Research methodology involves the researcher providing an alternative hypothesis, a research hypothesis, as an alternate way to explain the phenomenon.  The researcher tests the hypothesis to disprove the null hypothesis, not because he/she loves the research hypothesis, but because it would mean coming closer to finding an answer to a specific problem. The research hypothesis is often based on observations that evoke suspicion that the null hypothesis is not always correct.  In the Stanley Milgram Experiment, the null hypothesis was that the personality determined whether a person would hurt another person, while the research hypothesis was that the role, instructions and orders were much more important in determining whether people would hurt others.  Reasoning Cycle - Scientific Research Variables  A variable is something that changes. It changes according to different factors. Some variables change easily, like the stock-exchange value, while other variables are almost constant, like the name of someone. Researchers are often seeking to measure variables.  The variable can be a number, a name, or anything where the value can change.  An example of a variable is temperature. The temperature varies according to other variable and factors. You can measure different temperature inside and outside. If it is a sunny day, chances are that the temperature will be higher than if it's cloudy. Another thing that can make the temperature change is whether something has been done to manipulate the temperature, like lighting a fire in the chimney.  In research, you typically define variables according to what you're measuring. The independent variable is the variable which the researcher would like to measure (the cause), while the dependent variable is the effect (or assumed effect), dependent on the independent variable. These variables are often stated in experimental research, in a hypothesis, e.g. \"what is the effect of personality on helping behavior?\"  In explorative research methodology, e.g. in some qualitative research, the independent and the dependent variables might not be identified beforehand. They might not be stated because the researcher does not have a clear idea yet on what is really going on.  Confounding variables are variables with a significant effect on the dependent variable that the researcher failed to control or eliminate - sometimes because the researcher is not aware of the effect of the confounding variable. The key is to identify possible confounding variables and somehow try to eliminate or control them. Operationalization  Operationalization is to take a fuzzy concept (conceptual variables), such as 'helping behavior', and try to measure it by specific observations, e.g. how likely are people to help a stranger with problems.  Operationalization in Research  See also:  Conceptual Variables Choosing the Research Method", "category": "Edison", "id": 138}
{"skillName": "DSDA01", "skillText": "Predictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2][3]  In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.[4]  The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.  Predictive analytics is used in actuarial science,[5] marketing,[1][6] financial services,[7] insurance, telecommunications,[8] retail,[9] travel,[10] healthcare,[11] child protection,[12][13] pharmaceuticals,[14] capacity planning[citation needed] and other fields.  One of the most well known applications is credit scoring,[2] which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.  Contents      1 Definition     2 Types         2.1 Predictive models         2.2 Descriptive models         2.3 Decision models     3 Applications         3.1 Analytical customer relationship management (CRM)         3.2 Child protection         3.3 Clinical decision support systems         3.4 Collection analytics         3.5 Cross-sell         3.6 Customer retention         3.7 Direct marketing         3.8 Fraud detection         3.9 Portfolio, product or economy-level prediction         3.10 Risk management         3.11 Underwriting     4 Technology and big data influences     5 Analytical Techniques         5.1 Regression techniques             5.1.1 Linear regression model             5.1.2 Discrete choice models             5.1.3 Logistic regression             5.1.4 Multinomial logistic regression             5.1.5 Probit regression             5.1.6 Logit versus probit             5.1.7 Time series models             5.1.8 Survival or duration analysis             5.1.9 Classification and regression trees (CART)             5.1.10 Multivariate adaptive regression splines         5.2 Machine learning techniques             5.2.1 Neural networks             5.2.2 Multilayer Perceptron (MLP)             5.2.3 Radial basis functions             5.2.4 Support vector machines             5.2.5 Naïve Bayes             5.2.6 k-nearest neighbours             5.2.7 Geospatial predictive modeling     6 Tools         6.1 PMML     7 Criticism     8 See also     9 References     10 Further reading  Definition  Predictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns.[1] Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[15] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.  Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\"[16] In the future industrial systems the value of Predictive Analytics is to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement[17] which is the vision of Industrial Internet Consortium. Types  Generally, the term predictive analytics is used to mean predictive modeling, \"scoring\" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary. Predictive models  Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.  The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other samples, with known attributes but unknown performances, are referred to as “out of [training] sample” units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time. Descriptive models  Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions. Decision models  Decision models describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance. Applications  Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years. Analytical customer relationship management (CRM)  Analytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements. Child protection  Over the last 5 years, some Child Welfare Agencies have started using predictive analytics to flag high risk cases.[18] The approach has been called \"innovative\" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF),[19] and in Hillsborough County, FL, where the Lead Child Welfare Agency uses a predictive modeling tool called Eckerd Rapid Safety Feedback®, there have been no abuse-related child deaths in the target population as of this writing.[20] Clinical decision support systems  Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: \"Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.\"[citation needed] Collection analytics  Many portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs. Cross-sell  Often corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.[3] This directly leads to higher profitability per customer and stronger customer relationships. Customer retention  With the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.[21] Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.[8] An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity. Direct marketing  When marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The  of predictive analytics is typically to lower the cost per order or cost per action. Fraud detection  Fraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and ), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies,[22] retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the \"bads\" and reduce a business's exposure to fraud.  Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.[23]  The Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.[22]  Recent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts. Portfolio, product or economy-level prediction  Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[24][25] Risk management  When employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAP-M) \"predicts\" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.[26] These are three examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method. Underwriting  Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.[5] Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default. Technology and big data influences  Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.[27] Thanks to technological advances in computer hardware — faster CPUs, cheaper memory, and MPP architectures — and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.[22] It is also possible to run predictive algorithms on streaming data.[28] Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed [29] [30] Analytical Techniques  The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques. Regression techniques  Regression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below. Linear regression model  The linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.  The  of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.  Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is \"explained\" (accounted for) by variation in the independent variables. Discrete choice models  Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary. Logistic regression For more details on this topic, see logistic regression.  In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).  The Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the \"percentage correctly predicted\". Multinomial logistic regression  An extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit. Probit regression  Probit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.  A good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.  We do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1. Logit versus probit  The Probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.  Practical reasons for choosing the probit model over the logistic model would be:      There is a strong belief that the underlying distribution is normal     The actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).  Time series models  Time series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.  Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box–Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.  Box and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.  In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models. Survival or duration analysis  Survival analysis is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).  Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.  The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.  An important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.  Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.  Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric). Classification and regression trees (CART) Main article: decision tree learning  Globally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.  Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.  Decision trees are formed by a collection of rules based on variables in the modeling data set:      Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable     Once a rule is selected and splits a node into two, the same process is applied to each \"child\" node (i.e. it is a recursive procedure)     Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)  Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.  A very popular method for predictive analytics is Leo Breiman's Random forests. Multivariate adaptive regression splines  Multivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.  An important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.  In multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.  Multivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions. Machine learning techniques  Machine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.  A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997). Neural networks  Neural networks are nar sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.  Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.  Some examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc. Multilayer Perceptron (MLP)  The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of narly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule). Radial basis functions  A radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron. Support vector machines  Support Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc. Naïve Bayes  Naïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘curse of dimensionality’ i.e. when the number of predictors is very high. k-nearest neighbours  The nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al. Geospatial predictive modeling  Conceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution – there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence. Tools  Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.[31] Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.[3] For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.[32]  There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.  Notable open source predictive analytic tools include:      Apache Mahout     GNU Octave     KNIME     OpenNN     Orange     R     RiskAoA     scikit-learn     Weka  Notable commercial predictive analytic tools include:      Alpine Data Labs     Angoss KnowledgeSTUDIO     BIRT Analytics     IBM SPSS Statistics and IBM SPSS Modeler     KXEN Modeler     Mathematica     MATLAB     Minitab     LabVIEW[33]     Neural Designer     Oracle Advanced Analytics     Pervasive     Predixion Software     RapidMiner     RCASE     Revolution Analytics     SAP HANA[34] and SAP BusinessObjects Predictive Analytics[35]     SAS and SAS Enterprise Miner     STATA     Statgraphics     STATISTICA     TeleRetail     TIBCO  Beside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LABVIEW[36][37]  The most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html. PMML  In an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009. Criticism  There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard  and the director of the Institute for Quantitative Social Science. [38] People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. \"People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.\" [39] See also      Criminal Reduction Utilising Statistical History     Data mining     Learning analytics     Odds algorithm     Pattern recognition     Prescriptive analytics     Predictive modeling     RiskAoA a predictive tool for discriminating future decisions.", "category": "Edison", "id": 139}
{"skillName": "DSDA06", "skillText": "Unique data visualizations are more memorable, and add variety for the audience — even the most clear and straightforward visualization types lose their appeal when repeated over and over again. As visual literacy increases in the general population, data visualization designers will need to continually extend their knowledge of and proficiency across a widening range of visualization approaches to grow their skills alongside audience familiarity and expectations. Even more importantly, broad visualization know-how is essential for matching the data visualization type to the data available, the story to be told, and the question being answered.  In this article, I review 7 less-common (though certainly not unheard-of) yet very useful data visualization approaches:      Slopegraphs     Parallel Coordinates     Alluvial Diagrams     Sunbursts     Circle Packing     Horizon Charts     Streamgraphs  Though these alternative visualization types are somewhat well-established and have proven their worth for many applications, they’re much less ubiquitous than bar charts, line graphs, scatter plots, and unfortunately, pie charts. Without a doubt, basic and common visualization types can still be best for certain, straightforward data stories. However, communicating complex topics — hierarchies, longitudinal data, and multi-variable comparisons, and so on — often involves more advanced visualizations with corresponding depth.  Overviews for each of the alternative visualization types include:      A Brief Description     When to Use It     Two “In the Wild” Examples (that is, where others have used it effectively with real-world data)     A Place to Read More about It     One (Code-Free) Way to Make It — All of these visualizations can be made in programs like R and D3.js, as well as many commercial programs…but those avenues involve coding, have a longer ramp-up time and/or higher expense, and as a result, may not always be the best option. Each alternative visualization can also be created fairly quickly using free or commonly-available tools (e.g., Excel), and without needing to code: I focus on those methods here.  1 Slopegraphs — Slopegraphs are a special type of a line chart where two (or more) sets of values are compared by connecting each group’s values on one scale to their values on the second scale, with labels shown next to the group values for easy interpretation. The two scales have identical maximum and minimum values to make it very easy see whether each group increases, decreases, or remains similar between the two categories. Following a best practice, designers often highlight the lines of greatest interest (for example, the groups increasing or decreasing the most), graying out the rest.  When to Use It — To compare groups’ rate of change and rank-order switches between categories (often sequential years). Slopegraphs generally require that values be available for each group and for each category to show the full extent of changes from one category to the other.  Two “In the Wild” Examples      New York Times’s Infant Death Rates Ranks by Country     NPR’s How Your State Generates Power  A Place to Read More about It — by Cole Nussbaumer Knaflic  One (Code-Free) Way to Make It— by Jon Peltier  2 Parallel Coordinates — A parallel coordinates graph arrays multiple variables alongside one another with each scaled from highest to lowest value (highest at the top, lowest at the bottom) and with lines connecting each entity’s position for each variable, horizontally across the graph. Due to the large number of cases represented, it is often presented using an interactive view where individual lines can be selected and highlighted.  When to Use It — To reveal how groups show similar or different profiles across many quantitative variables. Parallel coordinates visualizations are among the best visualization types for large-scale, Big Data.  Two “In the Wild” Examples      USDA’s Nutrient Database     Domestic Data Streamer’s Physical Parallel Coordinates  A Place to Read More about It — by Stephen Few  One (Code-Free) Way to Make It— Use RAW: Paste in your own data directly from Excel or a similar spreadsheet program, or use their Cars sample dataset (then select Parallel Coordinates and drag each numerical dimension into the Dimensions box and Name into the Color box).  3 Alluvial Diagrams — Alluvial diagrams (closely related to Sankey diagrams) show how various entities (or nodes) flow together or apart across stages representing multiple groups or time periods. In these diagrams, width of the streams shows size or proportion within each category, similar to how tributaries join to form larger streams or how rivers split to form various branches.  When to Use It — To show how multiple groups relate to one another (shown when their streams flow together) or differ from one another (shown when their streams split apart), across several variables. Alluvial diagrams are especially useful for literal flows: of money, goods, time, votes, and so on, but also versatile for many other purposes. They can also show which variables are more clustered (fewer, wider streams) and which are more distributed (more, narrower streams).  Two “In the Wild” Examples      InfoCaptor’s Presidents of USA and their Birth Signs     Lawrence Livermore’s Estimated U.S. Energy Use in 2011  A Place to Read More about It — by DigitalSplashMedia  One (Code-Free) Way to Make It — Use SankeyMatic: Paste in your own data or use their sample data).  4 Sunbursts — Sunbursts show a hierarchical structure in a circular layout, with each ring outward representing a deeper level of the hierarchy. Ring segments are usually sized by the number of members within that segment. While sunbursts share some of the disadvantages of pie charts and are not well-suited to precise size comparisons, they do allow notable segments of a complex, multi-layered hierarchy to be quickly identified to guide further action.  When to Use It — To show how a multiple-level structure subdivides into subgroups, and which subgroups are bigger than others. Sunbursts are essentially hierarchical pie charts, allowing the pie slices to be split up in progressively more precise ways as the graph moves from the center to the outside.  Two “In the Wild” Examples      Marcin Ignac’s Visualization of Search Results for Nature Photography     Co.Design’s How to Tell the Difference Between 66 Varieties of Cheese  A Place to Read More about It — by Bime Analytics  One (Code-Free) Way to Make It — by BeatExcel  5Circle Packing — Circle packing diagrams show groups as tightly-organized circles, and are often used to show hierarchies where smaller groups are either colored similarly to others in the same category, or nested within larger groups.  When to Use It — To show how various groups and hierarchical structures vary in size and other properties (for example, budget allocation used to show size of circles and performance against budget represented by color of circles). While similar to the better-known visualization method Treemaps, circle packing diagrams are often more appealing to an audience due to an inherent preference many people have for circular graphics.  Two “In the Wild” Examples      StanfordKay Studio’s Global Carbon Emissions     Music Popcorn’s Visualization of the Music Genre Space  A Place to Read More about It — by Datavizcatalogue  One (Code-Free) Way to Make It — Use RAW: Paste in your own data directly from Excel or a similar spreadsheet program, or use their Movies sample dataset (then select Circle Packing and drag Genre then Movie into the Hierarchy box, Total Domestic Box Office into the Size box, and Genre into the Color box).  6 Horizon Charts — Horizon charts show time-series data with both negative and positive values on the vertical scale, using coloring or shading to show negative values while transposing them above the baseline “horizon”.  When to Use It — To show over-time data for one or more entities (e.g., countries, products, industries), especially useful when the data include both positive/growth and negative/contraction values, which would be difficult to represent clearly across many groups.  Two “In the Wild” Examples      Flowing Data’s Food Pricing Patterns     Warwickshire Observatory’s County Unemployment Rates  A Place to Read More about It — by Stephen Few  One (Code-Free) Way to Make It — by Superuser.com  7 Streamgraph — Streamgraphs show how the size or proportions of groups vary over time, with vertical width of the “stream” representing the size of that entity. Streamgraphs can use either a fixed scale, where change in the overall size of all groups can be seen, or a relative scale, where all groups consistently add to 100% (similar to an area chart).  When to Use It — To show group size or proportion over time, usually spanning at least 6 time periods, though the approach is scalable to extend much further. Streamgraphs can be extremely impactful to visually depict major shifts in cultural influences, technological trends, and economic forces over time — how quickly certain groups emerge while others fade away.  Two “In the Wild” Examples      Datagraphic’s Who’s Been Hogging the Road Since 1973?     Google’s Music Timeline  A Place to Read More about It — by Andy Kirk  One (Code-Free) Way to Make Them — Use RAW: Paste in your own data directly from Excel or a similar spreadsheet program, or use their Music sample dataset (then select Streamgraph and drag Media into the Group box, Total Domestic Box Office into the Size box, and Genre into the Color box). The Benefits of Experimenting with Alternative Data Visualization Types  The statement “data visualizations simplify the complex” is itself an oversimplification. Certainly, complexity needs to be conveyed clearly; graphical rather than numerical displays of information aid greatly in pursuing this goal. However, too much emphasis on merely “simplifying” complicated information — and using a limited set of tried and true visualization methods to do so — often isn’t the right answer either. Rather, designers should see visualization as a tool to preserve sophistication where it’s warranted, which often involves using graphics that match the depth of the content, rather than the other way around.  The more designers expand and experiment with their dataviz toolbox by trying out less-popular types when their questions and data allow it, the more effectively they’ll be able to match their message to their visualization medium — in addition to drawing on the engagement and memorability advantages of graphical types most members of your audience probably haven’t seen dozens of times before Software visualization[1][2] or software visualisation refers to the visualization of information of and related to software systems—either the architecture of its source code or metrics of their runtime behavior- and their development process by means of static, interactive or animated 2-D or 3-D[3] visual representations of their structure,[4] execution,[5] behavior,[6] and evolution.  Contents      1 Software system information     2 Objectives     3 Types     4 See also     5 References     6 Further reading     7 External links         7.1 Research groups  Software system information  Software visualization uses a variety of information available about software systems. Key information categories include:      implementation artifacts such as source codes,     software metric data from measurements or from reverse engineering,     traces that record execution behavior,     software testing data (e.g., test coverage)     software repository data that tracks changes.  Objectives  The objectives of software visualization are to support the understanding of software systems (i.e., its structure) and algorithms (e.g., by animating the behavior of sorting algorithms) as well as the analysis and exploration of software systems and their anomalies (e.g., by showing classes with high coupling) and their development and evolution. One of the strengths of software visualization is to combine and relate information of software systems that are not inherently linked, for example by projecting code changes onto software execution traces.[7]  Software visualization can be used as tool and technique to explore and analyze software system information, e.g., to discover anomalies similar to the process of visual data mining.[8] For example, software visualization is used to monitoring activities such as for code quality or team activity.[9] Visualization is inherently not a method for software quality assurance. Types  Tools for software visualization might be used to visualize source code and quality defects during software development and maintenance activities. There are different approaches to map source code to a visual representation such as by software maps[10] Their objective includes, for example, the automatic discovery and visualization of quality defects in object-oriented software systems and services. Commonly, they visualize the direct relationship of a class and its methods with other classes in the software system and mark potential quality defects. A further benefit is the support for visual navigation through the software system.  More or less specialized graph drawing software is used for software visualization. A small-scale 2003 survey of researchers active in the reverse engineering and software maintenance fields found that a wide variety of visualization tools were used, including general purpose graph drawing packages like GraphViz and GraphEd, UML tools like Rational Rose and Borland Together, and more specialized tools like Visualization of Compiler Graphs (VCG) and Rigi.[11]:99–100 The range of UML tools that can act as a visualizer by reverse engineering source is by no means short; a 2007 book noted that besides the two aforementioned tools, ESS-Model, BlueJ, and Fujaba also have this capability, and that Fujaba can also identify design patterns.[12] See also  Programs      Imagix 4D     NDepend     SonarJ     Sotoarc  Related concepts      Software maintenance     Software maps     Software diagnosis     Cognitive dimensions of notations     Software archaeology", "category": "Edison", "id": 140}
{"skillName": "DSENG05", "skillText": "The security management process has relations with almost all other ITIL-processes. However, in this particular section the most obvious relations will be the relations to the service level management process, the incident management process and the Change Management process.  Contents      1 The security management process         1.1 Control         1.2 Plan         1.3 Implementation         1.4 Evaluation         1.5 Maintenance         1.6 Complete process-data model     2 Relations with other ITIL processes     3 Example     4 See also     5 References     6 See also     7 External links  The security management process  The security management process consists of activities that are carried out by the security management itself or activities that are controlled by the security management.  Because organizations and their information systems constantly change, the activities within the security management process must be revised continuously, in order to stay up-to-date and effective. Security management is a continuous process and it can be compared to W. Edwards Deming's Quality Circle (Plan, Do, Check, Act).  The inputs are the requirements which are formed by the clients. The requirements are translated into security services, security quality that needs to be provided in the security section of the service level agreements. As you can see in the picture there are arrows going both ways; from the client to the SLA; from the SLA to the client and from the SLA to the plan sub-process; from the plan sub-process to the SLA. This means that both the client and the plan sub-process have inputs in the SLA and the SLA is an input for both the client and the process. The provider then develops the security plans for his/her organization. These security plans contain the security policies and the operational level agreements. The security plans (Plan) are then implemented (Do) and the implementation is then evaluated (Check). After the evaluation then both the plans and the implementation of the plan are maintained (Act).  The activities, results/products and the process are documented. External reports are written and sent to the clients. The clients are then able to adapt their requirements based on the information received through the reports. Furthermore, the service provider can adjust their plan or the implementation based on their findings in order to satisfy all the requirements stated in the SLA (including new requirements). Control  The first activity in the security management process is the “Control” sub-process. The Control sub-process organizes and manages the security management process itself. The Control sub-process defines the processes, the allocation of responsibility for the policy statements and the management framework.  The security management framework defines the sub-processes for: the development of security plans, the implementation of the security plans, the evaluation and how the results of the evaluations are translated into action plans. Furthermore, the management framework defines how should be reported to clients.  The activities that take place in the Control process are summed up in the following table, which contains the name of the (sub) activity and a short definition of the activity. Activities \tSub-Activities \tDescriptions Control \tImplement policies \tThis process outlines the specific requirements and rules that have to be met in order to implement security management. The process ends with policy statement. Set up the security organization \tThis process sets up the organizations for information security. For example in this process the structure the responsibilities are set up. This process ends with security management framework. Reporting \tIn this process the whole targeting process is documented in a specific way. This process ends with reports.  The meta-modeling technique was used in order to model the activities of the control sub-process. The following figure is the meta-process model of the control sub-process. It is based on a UML activity diagram and it gives an overview of the activities of the Control sub-process. The grey rectangle represents the control sub-process and the smaller beam shapes inside of the grey rectangle represent the activities that take place inside the control sub-process. The beams with a black shadow indicate that the activity is a closed (complex) activity. This means that the activity consists of a collection of (sub) activities but these activities are not expanded because they are not relevant in this particular context. The white beam without shadow indicates that the reporting activity is a standard activity. This means that reporting does not contain (sub) activities.  Control Process model.jpg  Figure 2.1.1: Meta-process model Control sub-process  Furthermore, it is noticeable that the first two activities are not linked with an arrow and that there is a black stripe with an arrow leading to the reporting activity. This means that the two first activities are not sequential. They are unordered activities and after these two activities have taken place the reporting activity will sequentially follow. For a more extensive explanation of the meta-modeling technique consult the Meta-modeling wiki.  The following table (table 2.1.2) is a concept definition table. Concept \tDescription CONTROL DOCUMENTS \tCONTROL is a description of how SECURITY MANAGEMENT will be organized and how it will be managed. POLICY STATEMENTS \tPOLICY STATEMENTS are documents that outlines specific requirements or rules that must be met. In the information security realm, policies are usually point-specific, covering a single area. For example, an “Acceptable Use” policy would cover the rules and regulations for appropriate use of the computing facilities. SECURITY MANAGEMENT FRAMEWORK \tSECURITY MANAGEMENT FRAMEWORK is an established management framework to initiate and control the implementation of information security within your organization and to manage ongoing information security provision.  Table 2.1.2: Concept and definition control sub-process Security management  The meta-data model of the control sub-process is based on a UML class diagram. In figure 2.1.2 is the meta-data model of the control sub-process.  Control Data model.JPG  Figure 2.1.2: Meta-process model control sub-process  The CONTROL rectangle with a white shadow is an open complex concept. This means that the CONTROL rectangle consists of a collection of (sub) concepts and these concepts are expanded in this particular context.  The following picture (figure 2.1.3) is the process-data model of the control sub-process. This picture shows the integration of the two models. The dotted arrows indicate which concepts are created or adjusted in the corresponding activities.  Control Process data model.JPG  Figure 2.1.3: Process-data model control sub-process Plan  The Plan sub-process contains activities that in cooperation with the Service Level Management lead to the (information) Security section in the SLA. Furthermore, the Plan sub-process contains activities that are related to the underpinning contracts which are specific for (information) security.  In the Plan sub-process the goals formulated in the SLA are specified in the form of Operational Level Agreements (OLA). These OLA’s can be defined as security plans for a specific internal organization entity of the service provider.  Besides the input of the SLA, the Plan sub-process also works with the policy statements of the service provider itself. As said earlier these policy statements are defined in the control sub-process.  The Operational Level Agreements for information security are set up and implemented based on the ITIL process. This means that there has to be cooperation with other ITIL processes. For example if the security management wishes to change the IT infrastructure in order to achieve maximum security, these changes will only be done through the Change Management process. The Security Management will deliver the input (Request for change) for this change. The change Manager is responsible for the Change Management Process itself.  Table 2.3.1 shows the activity plan the (sub) activities and their definition. Activities \tSub-Activities \tDescriptions Plan \tCreate Security section for SLA \tThis process contains activities that lead to the security agreements paragraph in the service level agreements. At the end of this process the Security section of the service level agreement is created. Create underpinning Contracts \tThis process contains activities that lead to UNDERPINNING CONTRACTS. These contracts are specific for security. Create Operational level agreements \tThe general formulated goals in the SLA are specified in operational level agreements. These agreements can be seen as security  plans for specific organization units. Reporting \tIn this process the whole Create plan process is documented in a specific way. This process ends with REPORTS.  Table 2.2.1: (Sub) activities and descriptions Plan sub-process ITIL Security Management  As well as for the Control sub-process the Plan sub-process has been modeled using the meta-modeling technique. On the right side of figure 2.2.1 the meta-process model of the Plan sub-process is given.  As you can see the Plan sub-process consists of a combination of unordered and ordered (sub) activities. Furthermore, it is noticeable that the sub-process contains three complex activities which are all closed activities and one standard activity. Table 2.2.1 consists of concepts that are created or adjusted during the plan sub-process. The table also gives a definition of these concepts. Concept \tDescription PLAN \tFormulated schemes for the security agreements. Security section of the security level agreements \tThe security agreements paragraph in the written agreements between a Service Provider and the customer(s) that documents agreed Service Levels for a service. UNDERPINNING CONTRACTS \tA contract with an external supplier covering delivery of services that support the IT organisation in their delivery of services. OPERATIONAL LEVEL AGREEMENTS \tAn internal agreement covering the delivery of services which support the IT organization in their delivery of services.  Table 2.2.2: Concept and definition Plan sub-process Security management  Just as the Control sub-process the Plan sub-process is modeled using the meta-modeling technique. The left side of figure 2.2.1 is the meta-data model of the Plan sub-process.  The Plan rectangle is an open (complex) concept which has an aggregation type of relationship with two closed (complex) concepts and one standard concept. The two closed concepts are not expanded in this particular context.  The following picture (figure 2.2.1) is the process-data diagram of the Plan sub-process. This picture shows the integration of the two models. The dotted arrows indicate which concepts are created or adjusted in the corresponding activities of the Plan sub-process.  Plan process data model.jpg  Figure 2.2.1: Process-data model Plan sub-process Implementation  The Implementation sub-process makes sure that all measures, as specified in the plans, are properly implemented. During the Implementation sub-process no (new) measures are defined nor changed. The definition or change of measures will take place in the Plan sub-process in cooperation with the Change Management Process.  The activities that take place in the implementation sub-process are summed up in the following table (table 2.3.1). The table contains the name of the (sub) activity and a short definition of the activity. Activities \tSub-Activities \tDescriptions Implement \tClassifying and managing of IT applications \tProcess of formally grouping configuration items by type, e.g., software, hardware, documentation, environment, application.  Process of formally identifying changes by type e.g., project scope change request, validation change request, infrastructure change request this process leads to asset classification and control documents. Implement personnel security \tHere measures are adopted in order to give personnel safety and confidence and measures to prevent a crime/fraud. The process ends with personnel security. Implement security management \tIn this process specific security requirements and/or security rules that must be met are outlined and documented. The process ends with security policies. Implement access control \tIn this process specific access security requirements and/or access security rules that must be met are outlined and documented. The process ends with access control. Reporting \tIn this process the whole implement as planned process is documented in a specific way. This process ends with reports.  Table 2.3.1: (Sub) activities and descriptions Implementation sub-process ITIL Security Management  The left side of figure 2.3.1 is the meta-process model of the Implementation phase. The four labels with a black shadow mean that these activities are closed concepts and they are not expanded in this context. It is also noticeable that there are no arrows connecting these four activities this means that these activities are unordered and the reporting will be carried out after the completion of al the four activities.  During the implementation phase there are a number of concepts that are created and /or adjusted. See table 2.3.2 for an overview of the most common concepts and their description. Concept \tDescription Implementation \tAccomplished security management according to the security management plan. Asset classification and control documents \tA comprehensive inventory of assets with responsibility assigned to ensure that effective security protection is maintained. Personnel security \tWell defined job descriptions for all staff outlining security roles and responsibilities. Security policies \tSecurity policies are documents that outlines specific security requirements or security rules that must be met. Access control \tNetwork management to ensure that only those with the appropriate responsibility have access to information in the networks and the protection of the supporting infrastructure.  Table 2.3.2: Concept and definition Implementation sub-process Security management  The concepts created and/or adjusted are modeled using the meta-modeling technique. The right side of figure 2.3.1 is the meta-data model of the implementation sub-process.  The implementation documents are an open concept and is expanded upon in this context. It consists of four closed concepts which are not expanded because they are irrelevant in this particular context.  In order to make the relations between the two models clearer the integration of the two models are illustrated in figure 2.3.1. The dotted arrows running from the activities to the concepts illustrate which concepts are created/ adjusted in the corresponding activities.  Implementation process data model.jpg  Figure 2.3.1: Process-data model Implementation sub-process Evaluation  The evaluation of the implementation and the plans is very important. The evaluation is necessary to measure the success of the implementation and the Security plans. The evaluation is also very important for the clients (and possibly third parties). The results of the Evaluation sub-process are used to maintain the agreed measures and the implementation itself. Evaluation results can lead to new requirements and so lead to a Request for Change. The request for change is then defined and it is then send to the Change Management process.  Mainly there are three sorts of evaluation; the Self-assessment; internal audit, and external audit.  The self-assessment is mainly carried out in the organization of the processes. The internal audits are carried out by internal IT-auditors and the external audits are carried out by external independent IT-auditors. Besides, the evaluations already mentioned an evaluation based on the communicated security incidents will also take place. The most important activities for this evaluation are the security monitoring of IT-systems; verify if the security legislation and the implementation of the security plans are complied; trace and react to undesirable use of the IT-supplies.  The activities that take place in the evaluation sub-process are summed up in the following table (Table 2.4.1). The table contains the name of the (sub) activity and a short definition of the activity. Activities \tSub-Activities \tDescriptions Evaluate \tSelf-assessment \tIn this process an examination of the implemented security agreements is done by the organization of the process itself. The result of this process is SELF ASSESSMENT DOCUMENTS. Internal Audit \tIn this process an examination of the implemented security agreements is done by an internal EDP auditor. The result of this process is INTERNAL AUDIT. External audit \tIn this process an examination of the implemented security agreements is done by an external EDP auditor. The result of this process is EXTERNAL AUDIT. Evaluation based on security incidents \tIn this process an examination of the implemented security agreements is done  based on security events which is not part of the standard operation of a service and which causes, or may cause, an interruption to, or a reduction in, the quality of that service. The result of this process is SECURITY INCIDENTS. Reporting \tIn this process the whole Evaluate implementation process is documented in a specific way. This process ends with REPORTS.  Table 2.4.1: (Sub) activities and descriptions Evaluation sub-process ITIL Security Management  Evaluation process data model.jpg  Figure 2.4.1: Process-data model Evaluation sub-process  The process-data diagram illustrated in the figure 2.4.1 consists of a meta-process model and a meta-data model. The Evaluation sub-process was modeled using the meta-modeling technique. The dotted arrows running from the meta-process diagram (left) to the meta-data diagram (right) indicate which concepts are created/ adjusted in the corresponding activities. All of the activities in the evaluation phase are standard activities. For a short description of the Evaluation phase concepts see Table 2.4.2 where the concepts are listed and defined. Concept \tDescription EVALUATION \tEvaluated/checked implementation. RESULTS \tThe outcome of the evaluated implementation. SELF ASSESSMENT DOCUMENTS \tResult of the examination of the security management by the organization of the process itself. INTERNAL AUDIT \tResult of the examination of the security management by the internal EDP auditor. EXTERNAL AUDIT \tResult of the examination of the security management by the external EDP auditor. SECURITY INCIDENTS DOCUMENTS \tResults of evaluating security events which is not part of the standard operation of a service and which causes, or may cause, an interruption to, or a reduction in, the quality of that service.  Table 2.4.2: Concept and definition evaluation sub-process Security management Maintenance  It is necessary for the security to be maintained. Because of changes in the IT-infrastructure and changes in the organization itself security risks are bound to change over time. The maintenance of the security concerns both the maintenance of the security section of the service level agreements and the more detailed security plans.  The maintenance is based on the results of the Evaluation sub-process and insight in the changing risks. These activities will only produce proposals. The proposals serve as inputs for the plan sub-process and will go through the whole cycle or the proposals can be taken in the maintenance of the service level agreements. In both cases the proposals could lead to activities in the action plan. The actual changes will be carried by the Change Management process. For more information about the Change Management Process consult the Change Management Wiki.  The activities that take place in the maintain sub-process are summed up in the following table (Table 2.5.1). The table contains the name of the (sub) activity and a short definition of the activity. Activities \tSub-Activities \tDescriptions Maintain \tMaintenance of Service level agreements \tThis is a process to keep the service level agreements in proper condition. The process ends with MAINTAINED SERVICE LEVEL AGREEMENTS. Maintenance of operational level agreements \tThis is a process to keep the operational level agreements in proper condition. The process ends with MAINTAINED OPERATIONAL LEVEL AGREEMENTS. Request for change to SLA and/or OLA \tRequest for a change to the SLA and/or OLA is formulated. This process ends with a REQUEST FOR CHANGE. Reporting \tIn this process the whole maintain implemented security policies process is documented in a specific way. This process ends with REPORTS.  Table 2.5.1: (Sub) activities and descriptions Maintenance sub-process ITIL Security Management  Figure 2.5.1 is the process-data diagram of the implementation sub-process. This picture shows the integration of the meta-process model (left) and the meta-data model (right). The dotted arrows indicate which concepts are created or adjusted in the activities of the implementation phase.  Maintenance process data model.jpg  Figure 2.5.1: Process-data model Maintenance sub-process  The maintenance sub-process starts with the maintenance of the service level agreements and the maintenance of the operational level agreements. After these activities take place (in no particular order) and there is a request for a change the request for change activity will take place and after the request for change activity is concluded the reporting activity starts. If there is no request for a change then the reporting activity will start directly after the first two activities. The concepts in the meta-data model are created/ adjusted during the maintenance phase. For a list of the concepts and their definition take a look at table 2.5.2. Concept \tDescription MAINTENANCE DOCUMENTS \tAgreements kept in proper condition. MAINTAINED SERVICE LEVEL AGREEMENTS \tService Level Agreements(security paragraph) kept in proper condition. MAINTAINED OPERATIONAL LEVEL AGREEMENTS \tOperational Level Agreements kept in proper condition. REQUEST FOR CHANGE \tForm, or screen, used to record details of a request for a change to the SLA/OLA.  Table 2.5.2: Concept and definition Plan sub-process Security management Complete process-data model  The following picture shows the complete process-data model of the Security Management process. This means that the complete meta-process model and the complete meta-data model and the integrations of the two models of the Security Management process are shown.  Process data model security management.jpg  Figure 2.6.1: Process-data model Security Management process Relations with other ITIL processes  The security Management Process, as stated in the introduction, has relations with almost all other ITIL-processes. These processes are:      IT Customer Relationship Management     Service Level Management     Availability Management     Capacity Management     IT Service Continuity Management     Configuration Management     Release Management     Incident Management & Service Desk     Problem Management     Change Management (ITSM)  Within these processes there are a couple of activities concerning security that have to take place. These activities are done as required. The concerning process and its process manager are responsible for these activities. However, the Security Management will give indications to the concerning process on how these (security specific) activities should be structured. Example  Internal E-mail Policies.  The use of internal e-mail in an organization has a lot of security risks. So if an organization chooses to use e-mail as a means of communication, it is highly recommended that the organization implements a well thought out e-mail security plan/policies. In this example the ITIL security Management approach is used to implement e-mail policies in an organization.  First of all, the Security management team is formed and the guidelines of how the process should be carried out are formulated and made clear to all employees and providers concerned. These actions are carried out in the Control phase of the Security Management process.  The next step in the process to implement e-mail policies is the Planning phase. In the Planning phase, the policies are formulated. Besides the policies that are already written in the Service Level Agreements, the policies that are specific to e-mail security are formulated and added to the service level agreements. At the end of this phase the entire plan is formulated and is ready to be implemented.  The following phase in the process is the actual implementation of the e-mail policies, which is done according to the plan which was formulated in the preceding phase (Plan phase).  After the actual implementation the e-mail policies will be evaluated. In order to evaluate the implemented policies the organization will perform self-assessments, or have internal or external auditors review the policies for design and operating effectiveness.  The last phase is the maintenance phase. In the maintenance phase the implemented e-mail policies are maintained. The organization now knows which policies are properly implemented and are properly followed, which policies need more work in order to help the security plan of the organization, and if there are new policies that have to be implemented. At the end of this process the Request for Change are formulated (if needed) and the e-mail policies are properly maintained.  In order for the organization to keep its security plan up-to-date the organization will have to perform the security management process continuously. There is no end to this process; an organization can always better its security. See also      Infrastructure Management Services     ITIL v3     Microsoft Operations Framework     Information security management system     COBIT     Capability Maturity Model     ISPL  References      Bon van, J. (2004). IT-Service management: een introductie op basis van ITIL. Van Haren Publishing     Cazemier, Jacques A.; Overbeek, Paul L.; Peters, Louk M. (2000). Security Management, Stationery Office.     Security management. (February 1, 2005). Retrieved from Microsoft Technet Web site: http://www.microsoft.com/technet/itsolutions/cits/mo/smf/mofsmsmf.mspx      Tse, D. (2005). Security in Modern Business: security assessment model for information security Practices. Hong Kong: University of Hong Kong.  See also      Information security  In computing, managed security services (MSS) are network security services that have been outsourced to a service provider. A company providing such a service is a managed security service provider (MSSP)[1] The roots of MSSPs are in the Internet Service Providers (ISPs) in the mid to late 1990’s. Initially ISPs would sell customers a firewall appliance, as customer premises equipment (CPE), and for an additional fee would manage the customer-owned firewall over a dial-up connection.[2]  According to recent industry research, most organizations (74%) manage IT security in-house, but 82% of IT professionals said they have either already partnered with, or plan to partner with, a managed security service provider.[3]  Businesses turn to managed security services providers to alleviate the pressures they face daily related to information security such as targeted malware, customer data theft, skills shortages and resource constraints.[4]  Managed security services (MSS) are also considered the systematic approach to managing an organization's security needs. The services may be conducted in-house or outsourced to a service provider that oversees other companies' network and information system security. Functions of a managed security service include round-the-clock monitoring and management of intrusion detection systems and firewalls, overseeing patch management and upgrades, performing security assessments and security audits, and responding to emergencies. There are products available from a number of vendors to help organize and guide the procedures involved. This diverts the burden of performing the chores manually, which can be considerable, away from administrators.  Industry research firm Forrester Research in late 2014 identified the 13 most significant vendors in the North American market with its 26-criteria evaluation of managed security service providers (MSSPs)--identifying IBM, Dell SecureWorks, Trustwave, AT&T, Verizon and others as the leaders in the MSSP market.[5]  Contents      1 Early History of Managed Security Services     2 Industry terms     3 Six categories of managed security services         3.1 On-site consulting         3.2 Perimeter management of the client's network         3.3 Product resale         3.4 Managed security monitoring         3.5 Penetration testing and vulnerability assessments         3.6 Compliance monitoring         3.7 Engaging an MSSP     4 Managed security services for mid-sized and smaller businesses     5 See also     6 References     7 Further reading  Early History of Managed Security Services  An early example of a cloud-based MSSP service is US West !NTERACT Internet Security. The security service didn’t require the customer to purchase any equipment and no security equipment was installed at the customers premises.[6] The service is considered a MSSP offering in that US West retained ownership of the firewall equipment and the firewalls were operated from their own Internet Point of Presence (PoP)[7] The service was based on Check Point Firewall-1 equipment.[8] Following over a year long beta introduction period, the service was generally available by early 1997.[6][7] The service also offered managed Virtual Private Networking (VPN) encryption security at launch.[7] Industry terms      Asset: A resource valuable to a company worthy of protection.     Incident: An assessed occurrence that actually or potentially jeopardizes the confidentiality, integrity, or availability of an asset.     Alert: Identified information, i.e. fact, used to correlate an incident.  Six categories of managed security services On-site consulting  This is customized assistance in the assessment of business risks, key business requirements for security and the development of security policies and processes. It may include comprehensive security architecture assessments and design (include technology, business risks, technical risks and procedures). Consulting may also include security product integration and On-site mitigation support after an intrusion has occurred, including emergency incident response and forensic analysis[1][9] Perimeter management of the client's network  This service involves installing, upgrading, and managing the firewall, Virtual Private Network (VPN) and/or intrusion detection hardware and software, electronic mail, and commonly performing configuration changes on behalf of the customer. Management includes monitoring, maintaining the firewall's traffic routing rules, and generating regular traffic and management reports to the customer.[1] Intrusion detection management, either at the network level or at the individual host level, involves providing intrusion alerts to a customer, keeping up to date with new defenses against intrusion, and regularly reporting on intrusion attempts and activity. Content filtering services may be provided by; such as, email filtering) and other data traffic filtering.[9] Product resale  Clearly not a managed service by itself, product resale is a major revenue generator for many MSS providers. This category provides value-added hardware and software for a variety of security-related tasks. One such service that may be provided is archival of customer data.[9] Managed security monitoring  This is the day-to-day monitoring and interpretation of important system events throughout the network—including unauthorized behavior, malicious hacks, denial of service (DoS), anomalies, and trend analysis. It is the first step in an incident response process. Penetration testing and vulnerability assessments  This includes one-time or periodic software scans or hacking attempts in order to find vulnerabilities in a technical and logical perimeter. It generally does not assess security throughout the network, nor does it accurately reflect personnel-related exposures due to disgruntled employees, social engineering, etc. Regularly, reports are given to the client.[1][9] Compliance monitoring  This includes monitoring event logs not for intrusions, but change management. This service will identify changes to a system that violate a formal security policy for example, if a rogue administrator grants himself or herself too much access to a system. In short, it measures compliance to a technical risk model. Engaging an MSSP  The decision criteria for engaging the services of an MSSP are much the same as those for any other form of outsourcing: cost-effectiveness compared to in-house solutions, focus upon core competencies, need for round-the-clock service, and ease of remaining up-to-date. An important factor, specific to MSS, is that outsourcing network security hands over critical control of the company's infrastructure to an outside party, the MSSP, whilst not relieving the ultimate responsibility for errors. The client of an MSSP still has the ultimate responsibility for its own security, and as such must be prepared to manage and monitor the MSSP, and hold it accountable for the services for which it is contracted. The relationship between MSSP and client is not a turnkey one.[1]  Although the organization remains responsible for defending its network against information security and related business risks, working with an MSSP allows the organization to focus on its core activities while remaining protected against network vulnerabilities.  Business risks can result when information assets upon which the business depends are not securely configured and managed (resulting in asset compromise due to violations of confidentiality, availability, and integrity). Compliance with specific government-defined security requirements can be achieved by using managed security services.[10] Managed security services for mid-sized and smaller businesses  The business model behind managed security services is commonplace among large enterprise companies with their IT security experts. The model was later adapted to fit medium-sized and smaller companies (SMBs - organizations up to 500 employees, or with no more than 100 employee at any one site) by the value-added reseller (VAR) community, either specializing in managed security or offering it as an extension to their managed IT service solutions. SMBs are increasingly turning to managed security services for a number of reasons. Chief among these are the specialized, complex and highly dynamic nature of IT security and the growing number of regulatory requirements obliging businesses to secure the digital safety and integrity of personal information and financial data held or transferred via their computer networks.  Whereas larger organizations typically employ an IT specialist or department, organizations at a smaller scale such as distributed location businesses, medical or dental offices, attorneys, professional services providers or retailers do not typically employ full-time security specialists, although they frequently employ IT staff or external IT consultants. Of these organizations, many are constrained by budget limitations. To address the combined issues of lack of expertise, lack of time and limited financial resources, an emerging category of managed security service provider for the SMB has arisen.  The organizations across sectors are now shifting to Managed Security services from the traditional in-house IT security practices. A trend of outsourcing the IT security jobs to the Managed Security Services vendors is picking up at an appreciable pace. This also helps the enterprises to focus more on their core business activities as a strategic approach. Effective management, cost-effectiveness and seamless monitoring are the major drivers fueling the demand of these services. Further, with the increase in the participation of leading IT companies worldwide, the end user enterprises are gaining confidence in outsourcing the IT security.[11]  Services providers in this category tend to offer comprehensive IT security services delivered on remotely managed appliances or devices that are simple to install and run for the most part in the background. Fees are normally highly affordable to reflect financial constraints, and are charged on a monthly basis at a flat rate to ensure predictability of costs. Service providers deliver daily, weekly, monthly or exception-based reporting depending on the client’s requirements.[12] Security Tuning(Firewall tuning/ IDS tuning/ SIEM tuning) [13]  Today IT security has become a power weapon as cyberattacks have become highly sophisticated. As enterprises toil to keep at par with the new malware deviant or e-mail spoofing fraud gambit. Among different prominent players, Managed Security Service provider observe the growing need to combat increasingly complicated and intended attacks. In response, these vendors are busy enhancing the sophistication of their solution, in many cases winning over other security expert to expand their portfolio. Besides this increasing regulatory compliance associated with the protection of citizen’s data worldwide, is likely to stimulate enterprises the enterprises to ensure a high data-security level.  Some of the frontrunners in engaging managed security services are Financial Services, telecom, information technology etc. To maintain a competitive edge, MSS vendors are focusing more and more on refining their product offering of technologies deployed at clients. Another crucial factor of profitability remains the capability to lower the cost yet generate more revenue by avoiding the deployment of additional tools. Simplifying both service creation and integration the products ensures unprecedented visibility as well as integration. Besides this, the MSS market would witness a tremendous growth in regions such as North America, Europe, Asia –Pacific and Latin America, Middle East and Africa.[14] See also      Information security operations center  Security threat landscape is emerging every day, not only becoming complex but also rendering traditional security solutions inadequate. Technology solutions are necessary but are incapable of proactive detection and reliable prevention. The frontiers of attack range from perimeter to end point. Business risks are continuously mounting as threats evolve on 24×7 basis. Enterprises need to meet the compliance requirements and this needs significant business attention. The upkeep of security of business critical Information assets is a challenge which is becoming complex and harder.  Round the clock security monitoring and management enables you to secure your IT infrastructure on a proactive basis. Entrust your IT infrastructure with our 24×7 world class security operations. Our remote managed security services enable you with the necessary safeguards to protect information assets in a no-compromise way. Our holistic approach to security services considers security as a continuous process and not as discrete incidents, thus delivering a robust and secure infrastructure.  Security Device Management Services cover Incident Management, Problem Management, Change Management, Configuration Management, Service Level Management and Vendor/3rd Party Coordination for:      Firewalls     VPN     Proxy     Anti-Virus, Anti-Spyware     IDS/IPS     Unified Threat Management     Messaging Security     Web Security     End Point Security – NAC, Personal firewall, WSUS, IPC, AV, AS  Benefits      Brings expertise to sensitive security operations     Proactive round-the-clock support     Ensures compliance     Reduced investment in tools and technologies for security infrastructure management     Lower cost of security operations  The ITIL security management process describes the structured fitting of security in the management organization. ITIL security management is based on the ISO 27001 standard. According to ISO.ORG  \"ISO/IEC 27001:2005 covers all types of organizations (e.g. commercial enterprises, government agencies, not-for profit organizations). ISO/IEC 27001:2005 specifies the requirements for establishing, implementing, operating, monitoring, reviewing, maintaining and improving a documented Information Security Management System within the context of the organization's overall business risks. It specifies requirements for the implementation of security controls customized to the needs of individual organizations or parts thereof. ISO/IEC 27001:2005 is designed to ensure the selection of adequate and proportionate security controls that protect information assets and give confidence to interested parties.\"  A basic concept of security management is the information security. The primary goal of information security is to guarantee safety of information. When protecting information it is the value of the information that must be protected. These values are stipulated by the confidentiality, integrity and availability. Inferred aspects are privacy, anonymity and verifiability.  The goal of the Security Management is split up in two parts:      The realization of the security requirements defined in the service level agreement (SLA) and other external requirements which are specified in underpinning contracts, legislation and possible internal or external imposed policies.     The realization of a basic level of security. This is necessary to guarantee the continuity of the management organization. This is also necessary in order to reach a simplified service-level management for the information security, as it happens to be easier to manage a limited number of SLAs than it is to manage a large number of SLAs.  The input of the security management process is formed by the SLAs with the specified security requirements, legislation documents (if applicable) and other (external) underpinning contracts. These requirements can also act as key performance indicators (KPIs) which can be used for the process management and for the justification of the results of the security management process.  The output gives justification information to the realization of the SLAs and a report with deviations from the requirements.", "category": "Edison", "id": 141}
